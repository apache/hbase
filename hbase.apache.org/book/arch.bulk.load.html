<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>9.8.&nbsp;Bulk Loading</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="architecture.html" title="Chapter&nbsp;9.&nbsp;Architecture"><link rel="prev" href="regions.arch.html" title="9.7.&nbsp;Regions"><link rel="next" href="arch.hdfs.html" title="9.9.&nbsp;HDFS"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">9.8.&nbsp;Bulk Loading</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="regions.arch.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;9.&nbsp;Architecture</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="arch.hdfs.html">Next</a></td></tr></table><hr></div><div class="section" title="9.8.&nbsp;Bulk Loading"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.bulk.load"></a>9.8.&nbsp;Bulk Loading</h2></div></div></div><div class="section" title="9.8.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.overview"></a>9.8.1.&nbsp;Overview</h3></div></div></div><p>
        HBase includes several methods of loading data into tables.
        The most straightforward method is to either use the <code class="code">TableOutputFormat</code>
        class from a MapReduce job, or use the normal client APIs; however,
        these are not always the most efficient methods.
      </p><p>
        The bulk load feature uses a MapReduce job to output table data in HBase's internal
        data format, and then directly loads the generated StoreFiles into a running
        cluster. Using bulk load will use less CPU and network resources than
        simply using the HBase API.
      </p></div><div class="section" title="9.8.2.&nbsp;Bulk Load Architecture"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.arch"></a>9.8.2.&nbsp;Bulk Load Architecture</h3></div></div></div><p>
        The HBase bulk load process consists of two main steps.
      </p><div class="section" title="9.8.2.1.&nbsp;Preparing data via a MapReduce job"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.prep"></a>9.8.2.1.&nbsp;Preparing data via a MapReduce job</h4></div></div></div><p>
          The first step of a bulk load is to generate HBase data files (StoreFiles) from
          a MapReduce job using <code class="code">HFileOutputFormat</code>. This output format writes
          out data in HBase's internal storage format so that they can be
          later loaded very efficiently into the cluster.
        </p><p>
          In order to function efficiently, <code class="code">HFileOutputFormat</code> must be
          configured such that each output HFile fits within a single region.
          In order to do this, jobs whose output will be bulk loaded into HBase
          use Hadoop's <code class="code">TotalOrderPartitioner</code> class to partition the map output
          into disjoint ranges of the key space, corresponding to the key
          ranges of the regions in the table.
        </p><p>
          <code class="code">HFileOutputFormat</code> includes a convenience function,
          <code class="code">configureIncrementalLoad()</code>, which automatically sets up
          a <code class="code">TotalOrderPartitioner</code> based on the current region boundaries of a
          table.
        </p></div><div class="section" title="9.8.2.2.&nbsp;Completing the data load"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.complete"></a>9.8.2.2.&nbsp;Completing the data load</h4></div></div></div><p>
          After the data has been prepared using
          <code class="code">HFileOutputFormat</code>, it is loaded into the cluster using
          <code class="code">completebulkload</code>. This command line tool iterates
          through the prepared data files, and for each one determines the
          region the file belongs to. It then contacts the appropriate Region
          Server which adopts the HFile, moving it into its storage directory
          and making the data available to clients.
        </p><p>
          If the region boundaries have changed during the course of bulk load
          preparation, or between the preparation and completion steps, the
          <code class="code">completebulkloads</code> utility will automatically split the
          data files into pieces corresponding to the new boundaries. This
          process is not optimally efficient, so users should take care to
          minimize the delay between preparing a bulk load and importing it
          into the cluster, especially if other clients are simultaneously
          loading data through other means.
        </p></div></div><div class="section" title="9.8.3.&nbsp;Importing the prepared data using the completebulkload tool"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.import"></a>9.8.3.&nbsp;Importing the prepared data using the completebulkload tool</h3></div></div></div><p>
        After a data import has been prepared, either by using the
        <code class="code">importtsv</code> tool with the
        "<code class="code">importtsv.bulk.output</code>" option or by some other MapReduce
        job using the <code class="code">HFileOutputFormat</code>, the
        <code class="code">completebulkload</code> tool is used to import the data into the
        running cluster.
      </p><p>
        The <code class="code">completebulkload</code> tool simply takes the output path
        where <code class="code">importtsv</code> or your MapReduce job put its results, and
        the table name to import into. For example:
      </p><code class="code">$ hadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /user/todd/myoutput mytable</code><p>
        The <code class="code">-c config-file</code> option can be used to specify a file
        containing the appropriate hbase parameters (e.g., hbase-site.xml) if
        not supplied already on the CLASSPATH (In addition, the CLASSPATH must
        contain the directory that has the zookeeper configuration file if
        zookeeper is NOT managed by HBase).
      </p><p>
        Note: If the target table does not already exist in HBase, this
        tool will create the table automatically.</p><p>
        This tool will run quickly, after which point the new data will be visible in
        the cluster.
      </p></div><div class="section" title="9.8.4.&nbsp;See Also"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.also"></a>9.8.4.&nbsp;See Also</h3></div></div></div><p>For more information about the referenced utilities, see <a class="xref" href="ops_mgt.html#importtsv" title="14.1.9.&nbsp;ImportTsv">Section&nbsp;14.1.9, &#8220;ImportTsv&#8221;</a> and  <a class="xref" href="ops_mgt.html#completebulkload" title="14.1.10.&nbsp;CompleteBulkLoad">Section&nbsp;14.1.10, &#8220;CompleteBulkLoad&#8221;</a>.
      </p></div><div class="section" title="9.8.5.&nbsp;Advanced Usage"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.adv"></a>9.8.5.&nbsp;Advanced Usage</h3></div></div></div><p>
        Although the <code class="code">importtsv</code> tool is useful in many cases, advanced users may
        want to generate data programatically, or import data from other formats. To get
        started doing so, dig into <code class="code">ImportTsv.java</code> and check the JavaDoc for
        HFileOutputFormat.
      </p><p>
        The import step of the bulk load can also be done programatically. See the
        <code class="code">LoadIncrementalHFiles</code> class for more information.
      </p></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'arch.bulk.load';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="regions.arch.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="architecture.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="arch.hdfs.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">9.7.&nbsp;Regions&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;9.9.&nbsp;HDFS</td></tr></table></div></body></html>
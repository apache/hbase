<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Chapter&nbsp;1.&nbsp;ZooKeeper</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter" title="Chapter&nbsp;1.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title"><a name="zookeeper"></a>Chapter&nbsp;1.&nbsp;ZooKeeper<a class="indexterm" name="d0e5"></a></h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d0e116">1.1. Using existing ZooKeeper ensemble</a></span></dt><dt><span class="section"><a href="#zk.sasl.auth">1.2. SASL Authentication with ZooKeeper</a></span></dt><dd><dl><dt><span class="section"><a href="#d0e181">1.2.1. Operating System Prerequisites</a></span></dt><dt><span class="section"><a href="#d0e262">1.2.2. HBase-managed Zookeeper Configuration</a></span></dt><dt><span class="section"><a href="#d0e337">1.2.3. External Zookeeper Configuration</a></span></dt><dt><span class="section"><a href="#d0e396">1.2.4. Zookeeper Server Authentication Log Output</a></span></dt><dt><span class="section"><a href="#d0e404">1.2.5. Zookeeper Client Authentication Log Output</a></span></dt><dt><span class="section"><a href="#d0e412">1.2.6. Configuration from Scratch</a></span></dt><dt><span class="section"><a href="#d0e421">1.2.7. Future improvements</a></span></dt></dl></dd></dl></div><p>A distributed Apache HBase (TM) installation depends on a running ZooKeeper cluster.
            All participating nodes and clients need to be able to access the
            running ZooKeeper ensemble. Apache HBase by default manages a ZooKeeper
            "cluster" for you. It will start and stop the ZooKeeper ensemble
            as part of the HBase start/stop process. You can also manage the
            ZooKeeper ensemble independent of HBase and just point HBase at
            the cluster it should use. To toggle HBase management of
            ZooKeeper, use the <code class="varname">HBASE_MANAGES_ZK</code> variable in
            <code class="filename">conf/hbase-env.sh</code>. This variable, which
            defaults to <code class="varname">true</code>, tells HBase whether to
            start/stop the ZooKeeper ensemble servers as part of HBase
            start/stop.</p><p>When HBase manages the ZooKeeper ensemble, you can specify
            ZooKeeper configuration using its native
            <code class="filename">zoo.cfg</code> file, or, the easier option is to
            just specify ZooKeeper options directly in
            <code class="filename">conf/hbase-site.xml</code>. A ZooKeeper
            configuration option can be set as a property in the HBase
            <code class="filename">hbase-site.xml</code> XML configuration file by
            prefacing the ZooKeeper option name with
            <code class="varname">hbase.zookeeper.property</code>. For example, the
            <code class="varname">clientPort</code> setting in ZooKeeper can be changed
            by setting the
            <code class="varname">hbase.zookeeper.property.clientPort</code> property.
            For all default values used by HBase, including ZooKeeper
            configuration, see <a class="xref" href="#">???</a>. Look for the
            <code class="varname">hbase.zookeeper.property</code> prefix <sup>[<a name="d0e44" href="#ftn.d0e44" class="footnote">1</a>]</sup></p><p>You must at least list the ensemble servers in
            <code class="filename">hbase-site.xml</code> using the
            <code class="varname">hbase.zookeeper.quorum</code> property. This property
            defaults to a single ensemble member at
            <code class="varname">localhost</code> which is not suitable for a fully
            distributed HBase. (It binds to the local machine only and remote
            clients will not be able to connect). </p><div class="note" title="How many ZooKeepers should I run?" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="how_many_zks"></a>How many ZooKeepers should I run?</h3><p>You can run a ZooKeeper ensemble that comprises 1 node
                only but in production it is recommended that you run a
                ZooKeeper ensemble of 3, 5 or 7 machines; the more members an
                ensemble has, the more tolerant the ensemble is of host
                failures. Also, run an odd number of machines. In ZooKeeper,
                an even number of peers is supported, but it is normally not used
                because an even sized ensemble requires, proportionally, more peers
                to form a quorum than an odd sized ensemble requires. For example, an
                ensemble with 4 peers requires 3 to form a quorum, while an ensemble with
                5 also requires 3 to form a quorum. Thus, an ensemble of 5 allows 2 peers to
                fail, and thus is more fault tolerant than the ensemble of 4, which allows
                only 1 down peer.
                </p><p>Give each ZooKeeper server around 1GB of RAM, and if possible, its own
                dedicated disk (A dedicated disk is the best thing you can do
                to ensure a performant ZooKeeper ensemble). For very heavily
                loaded clusters, run ZooKeeper servers on separate machines
                from RegionServers (DataNodes and TaskTrackers).</p></div><p>For example, to have HBase manage a ZooKeeper quorum on
            nodes <span class="emphasis"><em>rs{1,2,3,4,5}.example.com</em></span>, bound to
            port 2222 (the default is 2181) ensure
            <code class="varname">HBASE_MANAGE_ZK</code> is commented out or set to
            <code class="varname">true</code> in <code class="filename">conf/hbase-env.sh</code>
            and then edit <code class="filename">conf/hbase-site.xml</code> and set
            <code class="varname">hbase.zookeeper.property.clientPort</code> and
            <code class="varname">hbase.zookeeper.quorum</code>. You should also set
            <code class="varname">hbase.zookeeper.property.dataDir</code> to other than
            the default as the default has ZooKeeper persist data under
            <code class="filename">/tmp</code> which is often cleared on system
            restart. In the example below we have ZooKeeper persist to
            <code class="filename">/user/local/zookeeper</code>. </p><pre class="programlisting">
  &lt;configuration&gt;
    ...
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
      &lt;value&gt;2222&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The port at which the clients will connect.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
      &lt;value&gt;rs1.example.com,rs2.example.com,rs3.example.com,rs4.example.com,rs5.example.com&lt;/value&gt;
      &lt;description&gt;Comma separated list of servers in the ZooKeeper Quorum.
      For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
      By default this is set to localhost for local and pseudo-distributed modes
      of operation. For a fully-distributed setup, this should be set to a full
      list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
      this is the list of servers which we will start/stop ZooKeeper on.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
      &lt;value&gt;/usr/local/zookeeper&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The directory where the snapshot is stored.
      &lt;/description&gt;
    &lt;/property&gt;
    ...
  &lt;/configuration&gt;</pre><div class="caution" title="ZooKeeper Maintenance" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">ZooKeeper Maintenance</h3><p>Be sure to set up the data dir cleaner described under
          <a class="link" href="http://zookeeper.apache.org/doc/r3.1.2/zookeeperAdmin.html#sc_maintenance" target="_top">Zookeeper Maintenance</a> else you could
          have 'interesting' problems a couple of months in; i.e. zookeeper could start
          dropping sessions if it has to run through a directory of hundreds of thousands of
          logs which is wont to do around leader reelection time -- a process rare but run on
      occasion whether because a machine is dropped or happens to hiccup.</p></div><div class="section" title="1.1.&nbsp;Using existing ZooKeeper ensemble"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e116"></a>1.1.&nbsp;Using existing ZooKeeper ensemble</h2></div></div></div><p>To point HBase at an existing ZooKeeper cluster, one that
              is not managed by HBase, set <code class="varname">HBASE_MANAGES_ZK</code>
              in <code class="filename">conf/hbase-env.sh</code> to false
              </p><pre class="programlisting">
  ...
  # Tell HBase whether it should manage its own instance of Zookeeper or not.
  export HBASE_MANAGES_ZK=false</pre><p> Next set ensemble locations
              and client port, if non-standard, in
              <code class="filename">hbase-site.xml</code>, or add a suitably
              configured <code class="filename">zoo.cfg</code> to HBase's
              <code class="filename">CLASSPATH</code>. HBase will prefer the
              configuration found in <code class="filename">zoo.cfg</code> over any
              settings in <code class="filename">hbase-site.xml</code>.</p><p>When HBase manages ZooKeeper, it will start/stop the
              ZooKeeper servers as a part of the regular start/stop scripts.
              If you would like to run ZooKeeper yourself, independent of
              HBase start/stop, you would do the following</p><pre class="programlisting">
${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper
</pre><p>Note that you can use HBase in this manner to spin up a
              ZooKeeper cluster, unrelated to HBase. Just make sure to set
              <code class="varname">HBASE_MANAGES_ZK</code> to <code class="varname">false</code>
              if you want it to stay up across HBase restarts so that when
              HBase shuts down, it doesn't take ZooKeeper down with it.</p><p>For more information about running a distinct ZooKeeper
              cluster, see the ZooKeeper <a class="link" href="http://hadoop.apache.org/zookeeper/docs/current/zookeeperStarted.html" target="_top">Getting
              Started Guide</a>.  Additionally, see the <a class="link" href="http://wiki.apache.org/hadoop/ZooKeeper/FAQ#A7" target="_top">ZooKeeper Wiki</a> or the
          <a class="link" href="http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_zkMulitServerSetup" target="_top">ZooKeeper documentation</a>
          for more information on ZooKeeper sizing.
            </p></div><div class="section" title="1.2.&nbsp;SASL Authentication with ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="zk.sasl.auth"></a>1.2.&nbsp;SASL Authentication with ZooKeeper</h2></div></div></div><p>Newer releases of Apache HBase (&gt;= 0.92) will
              support connecting to a ZooKeeper Quorum that supports
              SASL authentication (which is available in Zookeeper
              versions 3.4.0 or later).</p><p>This describes how to set up HBase to mutually
              authenticate with a ZooKeeper Quorum. ZooKeeper/HBase
              mutual authentication (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-2418" target="_top">HBASE-2418</a>)
              is required as part of a complete secure HBase configuration
              (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-3025" target="_top">HBASE-3025</a>).

              For simplicity of explication, this section ignores
              additional configuration required (Secure HDFS and Coprocessor
              configuration).  It's recommended to begin with an
              HBase-managed Zookeeper configuration (as opposed to a
              standalone Zookeeper quorum) for ease of learning.
              </p><div class="section" title="1.2.1.&nbsp;Operating System Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d0e181"></a>1.2.1.&nbsp;Operating System Prerequisites</h3></div></div></div></div><p>
                  You need to have a working Kerberos KDC setup. For
                  each <code class="code">$HOST</code> that will run a ZooKeeper
                  server, you should have a principle
                  <code class="code">zookeeper/$HOST</code>.  For each such host,
                  add a service key (using the <code class="code">kadmin</code> or
                  <code class="code">kadmin.local</code> tool's <code class="code">ktadd</code>
                  command) for <code class="code">zookeeper/$HOST</code> and copy
                  this file to <code class="code">$HOST</code>, and make it
                  readable only to the user that will run zookeeper on
                  <code class="code">$HOST</code>. Note the location of this file,
                  which we will use below as
                  <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code>.
              </p><p>
                Similarly, for each <code class="code">$HOST</code> that will run
                an HBase server (master or regionserver), you should
                have a principle: <code class="code">hbase/$HOST</code>. For each
                host, add a keytab file called
                <code class="filename">hbase.keytab</code> containing a service
                key for <code class="code">hbase/$HOST</code>, copy this file to
                <code class="code">$HOST</code>, and make it readable only to the
                user that will run an HBase service on
                <code class="code">$HOST</code>. Note the location of this file,
                which we will use below as
                <code class="filename">$PATH_TO_HBASE_KEYTAB</code>.
              </p><p>
                Each user who will be an HBase client should also be
                given a Kerberos principal. This principal should
                usually have a password assigned to it (as opposed to,
                as with the HBase servers, a keytab file) which only
                this user knows. The client's principal's
                <code class="code">maxrenewlife</code> should be set so that it can
                be renewed enough so that the user can complete their
                HBase client processes. For example, if a user runs a
                long-running HBase client process that takes at most 3
                days, we might create this user's principal within
                <code class="code">kadmin</code> with: <code class="code">addprinc -maxrenewlife
                3days</code>. The Zookeeper client and server
                libraries manage their own ticket refreshment by
                running threads that wake up periodically to do the
                refreshment.
              </p><p>On each host that will run an HBase client
                (e.g. <code class="code">hbase shell</code>), add the following
                file to the HBase home directory's <code class="filename">conf</code>
                directory:</p><pre class="programlisting">
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=false
                    useTicketCache=true;
                  };
                </pre><p>We'll refer to this JAAS configuration file as
                <code class="filename">$CLIENT_CONF</code> below.</p><div class="section" title="1.2.2.&nbsp;HBase-managed Zookeeper Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="d0e262"></a>1.2.2.&nbsp;HBase-managed Zookeeper Configuration</h3></div></div></div><p>On each node that will run a zookeeper, a
                master, or a regionserver, create a <a class="link" href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jgss/tutorials/LoginConfigFile.html" target="_top">JAAS</a>
                configuration file in the conf directory of the node's
                <code class="filename">HBASE_HOME</code> directory that looks like the
                following:</p><pre class="programlisting">
                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre>

                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> and
                <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code> files are what
                you created above, and <code class="code">$HOST</code> is the hostname for that
                node.

                <p>The <code class="code">Server</code> section will be used by
                the Zookeeper quorum server, while the
                <code class="code">Client</code> section will be used by the HBase
                master and regionservers. The path to this file should
                be substituted for the text <code class="filename">$HBASE_SERVER_CONF</code>
                in the <code class="filename">hbase-env.sh</code>
                listing below.</p><p>
                  The path to this file should be substituted for the
                  text <code class="filename">$CLIENT_CONF</code> in the
                  <code class="filename">hbase-env.sh</code> listing below.
                </p><p>Modify your <code class="filename">hbase-env.sh</code> to include the
                following:</p><pre class="programlisting">
                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=true
                  export HBASE_ZOOKEEPER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre>

                where <code class="filename">$HBASE_SERVER_CONF</code> and
                <code class="filename">$CLIENT_CONF</code> are the full paths to the
                JAAS configuration files created above.

                <p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run zookeeper, master or regionserver to contain:</p><pre class="programlisting">
                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.authProvider.1&lt;/name&gt;
                      &lt;value&gt;org.apache.zookeeper.server.auth.SASLAuthenticationProvider&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeHostFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeRealmFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>Start your hbase cluster by running one or more
                of the following set of commands on the appropriate
                hosts:
                </p><pre class="programlisting">
                  bin/hbase zookeeper start
                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="1.2.3.&nbsp;External Zookeeper Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="d0e337"></a>1.2.3.&nbsp;External Zookeeper Configuration</h3></div></div></div><p>Add a JAAS configuration file that looks like:

                </p><pre class="programlisting">
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre><p>

                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> is the keytab
                created above for HBase services to run on this host, and <code class="code">$HOST</code> is the
                hostname for that node. Put this in the HBase home's
                configuration directory. We'll refer to this file's
                full pathname as <code class="filename">$HBASE_SERVER_CONF</code> below.</p><p>Modify your hbase-env.sh to include the following:</p><pre class="programlisting">
                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=false
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre><p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run a master or regionserver to contain:</p><pre class="programlisting">
                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  
                </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>
                  Add a <code class="filename">zoo.cfg</code> for each Zookeeper Quorum host containing:
                  </p><pre class="programlisting">
                      authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
                      kerberos.removeHostFromPrincipal=true
                      kerberos.removeRealmFromPrincipal=true
                  </pre><p>

                  Also on each of these hosts, create a JAAS configuration file containing:

                  </p><pre class="programlisting">
                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  </pre><p>

                  where <code class="code">$HOST</code> is the hostname of each
                  Quorum host. We will refer to the full pathname of
                  this file as <code class="filename">$ZK_SERVER_CONF</code> below.

                </p><p>
                  Start your Zookeepers on each Zookeeper Quorum host with:

                  </p><pre class="programlisting">
                    SERVER_JVMFLAGS="-Djava.security.auth.login.config=$ZK_SERVER_CONF" bin/zkServer start
                  </pre><p>

                </p><p>
                  Start your HBase cluster by running one or more of the following set of commands on the appropriate nodes:
                </p><pre class="programlisting">
                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="1.2.4.&nbsp;Zookeeper Server Authentication Log Output"><div class="titlepage"><div><div><h3 class="title"><a name="d0e396"></a>1.2.4.&nbsp;Zookeeper Server Authentication Log Output</h3></div></div></div><p>If the configuration above is successful,
                you should see something similar to the following in
                your Zookeeper server logs:
                </p><pre class="programlisting">
11/12/05 22:43:39 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:39 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:39 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:36:42 UTC 2011
..
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler:
  Successfully authenticated client: authenticationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN;
  authorizationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN.
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler: Setting authorizedID: hbase
11/12/05 22:43:59 INFO server.ZooKeeperServer: adding SASL authorization for authorizationID: hbase
                </pre><p>

                </p></div><div class="section" title="1.2.5.&nbsp;Zookeeper Client Authentication Log Output"><div class="titlepage"><div><div><h3 class="title"><a name="d0e404"></a>1.2.5.&nbsp;Zookeeper Client Authentication Log Output</h3></div></div></div><p>On the Zookeeper client side (HBase master or regionserver),
                you should see something similar to the following:

                </p><pre class="programlisting">
11/12/05 22:43:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=ip-10-166-175-249.us-west-1.compute.internal:2181 sessionTimeout=180000 watcher=master:60000
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Opening socket connection to server /10.166.175.249:2181
11/12/05 22:43:59 INFO zookeeper.RecoverableZooKeeper: The identifier of this process is 14851@ip-10-166-175-249
11/12/05 22:43:59 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:59 INFO client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Socket connection established to ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, initiating session
11/12/05 22:43:59 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:30:37 UTC 2011
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Session establishment complete on server ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, sessionid = 0x134106594320000, negotiated timeout = 180000
                </pre><p>
                </p></div><div class="section" title="1.2.6.&nbsp;Configuration from Scratch"><div class="titlepage"><div><div><h3 class="title"><a name="d0e412"></a>1.2.6.&nbsp;Configuration from Scratch</h3></div></div></div>

                This has been tested on the current standard Amazon
                Linux AMI.  First setup KDC and principals as
                described above. Next checkout code and run a sanity
                check.

                <pre class="programlisting">
                git clone git://git.apache.org/hbase.git
                cd hbase
                mvn clean test -Dtest=TestZooKeeperACL
                </pre>

                Then configure HBase as described above.
                Manually edit target/cached_classpath.txt (see below)..

                <pre class="programlisting">
                bin/hbase zookeeper &amp;
                bin/hbase master &amp;
                bin/hbase regionserver &amp;
                </pre></div><div class="section" title="1.2.7.&nbsp;Future improvements"><div class="titlepage"><div><div><h3 class="title"><a name="d0e421"></a>1.2.7.&nbsp;Future improvements</h3></div></div></div><div class="section" title="1.2.7.1.&nbsp;Fix target/cached_classpath.txt"><div class="titlepage"><div><div><h4 class="title"><a name="d0e424"></a>1.2.7.1.&nbsp;Fix target/cached_classpath.txt</h4></div></div></div><p>
                You must override the standard hadoop-core jar file from the
                <code class="code">target/cached_classpath.txt</code>
                file with the version containing the HADOOP-7070 fix. You can use the following script to do this:

                </p><pre class="programlisting">
                  echo `find ~/.m2 -name "*hadoop-core*7070*SNAPSHOT.jar"` ':' `cat target/cached_classpath.txt` | sed 's/ //g' &gt; target/tmp.txt
                  mv target/tmp.txt target/cached_classpath.txt
                </pre><p>

                </p></div><div class="section" title="1.2.7.2.&nbsp;Set JAAS configuration programmatically"><div class="titlepage"><div><div><h4 class="title"><a name="d0e435"></a>1.2.7.2.&nbsp;Set JAAS configuration
                  programmatically</h4></div></div></div>


                  This would avoid the need for a separate Hadoop jar
                  that fixes <a class="link" href="https://issues.apache.org/jira/browse/HADOOP-7070" target="_top">HADOOP-7070</a>.
                </div><div class="section" title="1.2.7.3.&nbsp;Elimination of kerberos.removeHostFromPrincipal and kerberos.removeRealmFromPrincipal"><div class="titlepage"><div><div><h4 class="title"><a name="d0e442"></a>1.2.7.3.&nbsp;Elimination of
                  <code class="code">kerberos.removeHostFromPrincipal</code> and
                  <code class="code">kerberos.removeRealmFromPrincipal</code></h4></div></div></div></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d0e44" href="#d0e44" class="para">1</a>] </sup>For the full list of ZooKeeper configurations, see
                ZooKeeper's <code class="filename">zoo.cfg</code>. HBase does not ship
                with a <code class="filename">zoo.cfg</code> so you will need to browse
                the <code class="filename">conf</code> directory in an appropriate
                ZooKeeper download.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'zookeeper';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>
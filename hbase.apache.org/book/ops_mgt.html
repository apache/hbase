<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Chapter&nbsp;14.&nbsp;Apache HBase (TM) Operational Management</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="prev" href="casestudies.perftroub.html" title="13.3.&nbsp;Performance/Troubleshooting"><link rel="next" href="ops.regionmgt.html" title="14.2.&nbsp;Region Management"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter&nbsp;14.&nbsp;Apache HBase (TM) Operational Management</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="casestudies.perftroub.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="ops.regionmgt.html">Next</a></td></tr></table><hr></div><div class="chapter" title="Chapter&nbsp;14.&nbsp;Apache HBase (TM) Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;14.&nbsp;Apache HBase (TM) Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="ops_mgt.html#tools">14.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="ops_mgt.html#driver">14.1.1. Driver</a></span></dt><dt><span class="section"><a href="ops_mgt.html#hbck">14.1.2. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="ops_mgt.html#hfile_tool2">14.1.3. HFile Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#wal_tools">14.1.4. WAL Tools</a></span></dt><dt><span class="section"><a href="ops_mgt.html#compression.tool">14.1.5. Compression Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#copytable">14.1.6. CopyTable</a></span></dt><dt><span class="section"><a href="ops_mgt.html#export">14.1.7. Export</a></span></dt><dt><span class="section"><a href="ops_mgt.html#import">14.1.8. Import</a></span></dt><dt><span class="section"><a href="ops_mgt.html#importtsv">14.1.9. ImportTsv</a></span></dt><dt><span class="section"><a href="ops_mgt.html#completebulkload">14.1.10. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="ops_mgt.html#walplayer">14.1.11. WALPlayer</a></span></dt><dt><span class="section"><a href="ops_mgt.html#rowcounter">14.1.12. RowCounter</a></span></dt></dl></dd><dt><span class="section"><a href="ops.regionmgt.html">14.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.majorcompact">14.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.merge">14.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="node.management.html">14.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="node.management.html#decommission">14.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="node.management.html#rolling">14.3.2. Rolling Restart</a></span></dt></dl></dd><dt><span class="section"><a href="hbase_metrics.html">14.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="hbase_metrics.html#metric_setup">14.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="hbase_metrics.html#rs_metrics">14.4.2. RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="ops.monitoring.html">14.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="ops.monitoring.html#ops.monitoring.overview">14.5.1. Overview</a></span></dt><dt><span class="section"><a href="ops.monitoring.html#ops.slow.query">14.5.2. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="cluster_replication.html">14.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html">14.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="ops.backup.html#ops.backup.fullshutdown">14.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.replication">14.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.copytable">14.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.export">14.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="ops.capacity.html">14.8. Capacity Planning</a></span></dt><dd><dl><dt><span class="section"><a href="ops.capacity.html#ops.capacity.storage">14.8.1. Storage</a></span></dt><dt><span class="section"><a href="ops.capacity.html#ops.capacity.regions">14.8.2. Regions</a></span></dt></dl></dd></dl></div>
  This chapter will cover operational tools and practices required of a running Apache HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="trouble.html" title="Chapter&nbsp;12.&nbsp;Troubleshooting and Debugging Apache HBase (TM)">Chapter&nbsp;12, <i>Troubleshooting and Debugging Apache HBase (TM)</i></a>, <a class="xref" href="performance.html" title="Chapter&nbsp;11.&nbsp;Apache HBase (TM) Performance Tuning">Chapter&nbsp;11, <i>Apache HBase (TM) Performance Tuning</i></a>,
  and <a class="xref" href="configuration.html" title="Chapter&nbsp;2.&nbsp;Apache HBase (TM) Configuration">Chapter&nbsp;2, <i>Apache HBase (TM) Configuration</i></a> but is a distinct topic in itself.

  <div class="section" title="14.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>14.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</p><div class="section" title="14.1.1.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>14.1.1.&nbsp;Driver</h3></div></div></div><p>There is a <code class="code">Driver</code> class that is executed by the HBase jar can be used to invoke frequently accessed utilities.  For example,
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar
</pre><p>
... will return...
</p><pre class="programlisting">
An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is chan
</pre><p>
... for allowable program names.
      </p></div><div class="section" title="14.1.2.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>14.1.2.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p><p>For more information, see <a class="xref" href="hbck.in.depth.html" title="Appendix&nbsp;B.&nbsp;hbck In Depth">Appendix&nbsp;B, <i>hbck In Depth</i></a>.
        </p></div><div class="section" title="14.1.3.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>14.1.3.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="regions.arch.html#hfile_tool" title="9.7.5.2.2.&nbsp;HFile Tool">Section&nbsp;9.7.5.2.2, &#8220;HFile Tool&#8221;</a>.</p></div><div class="section" title="14.1.4.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>14.1.4.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="14.1.4.1.&nbsp;HLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>14.1.4.1.&nbsp;<code class="classname">HLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">HLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre><div class="section" title="14.1.4.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>14.1.4.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to print the contents of an HLog.
          </p></div></div></div><div class="section" title="14.1.5.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>14.1.5.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="compression.html#compression.test" title="C.1.&nbsp;CompressionTest Tool">Section&nbsp;C.1, &#8220;CompressionTest Tool&#8221;</a>.</p></div><div class="section" title="14.1.6.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>14.1.6.&nbsp;CopyTable</h3></div></div></div><p>
            CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><div class="note" title="Scanner Caching" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scanner Caching</h3><p>Caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><p>
        See Jonathan Hsieh's <a class="link" href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_top">Online HBase Backups with CopyTable</a> blog post for more on <span class="command"><strong>CopyTable</strong></span>.
        </p></div><div class="section" title="14.1.7.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>14.1.7.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="14.1.8.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>14.1.8.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="14.1.9.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>14.1.9.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase.  It has two distinct usages:  loading data from TSV format in HDFS
       into HBase via Puts, and preparing StoreFiles to be loaded via the <code class="code">completebulkload</code>.
       </p><p>To load data via Puts (i.e., non-bulk loading):
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>
       </p><p>To generate StoreFiles for bulk-loading:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>
       </p><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="ops_mgt.html#completebulkload" title="14.1.10.&nbsp;CompleteBulkLoad">Section&nbsp;14.1.10, &#8220;CompleteBulkLoad&#8221;</a>.
       </p><div class="section" title="14.1.9.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>14.1.9.1.&nbsp;ImportTsv Options</h4></div></div></div>
       Running ImportTsv with no arguments prints brief usage information:
<pre class="programlisting">
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: if you do not use this option, then the target table must already exist in HBase

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
</pre></div><div class="section" title="14.1.9.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>14.1.9.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a ColumnFamily called 'd' with two columns "c1" and "c2".
         </p><p>Assume that an input file exists as follows:
</p><pre class="programlisting">
row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
</pre><p>
         </p><p>For ImportTsv to use this imput file, the command line needs to look like this:
 </p><pre class="programlisting">
 HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput  datatsv hdfs://inputfile
 </pre><p>
         ... and in this example the first column is the rowkey, which is why the HBASE_ROW_KEY is used.  The second and third columns in the file will be imported as "d:c1" and "d:c2", respectively.
         </p></div><div class="section" title="14.1.9.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>14.1.9.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table is pre-split appropriately.
         </p></div><div class="section" title="14.1.9.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>14.1.9.4.&nbsp;See Also</h4></div></div></div>
       For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a></div></div><div class="section" title="14.1.10.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>14.1.10.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase table.  This utility is often used
	   in conjunction with output from <a class="xref" href="ops_mgt.html#importtsv" title="14.1.9.&nbsp;ImportTsv">Section&nbsp;14.1.9, &#8220;ImportTsv&#8221;</a>.
	   </p><p>There are two ways to invoke this utility, with explicit classname and via the driver:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
.. and via the Driver..
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
	  </p><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a>.
       </p></div><div class="section" title="14.1.11.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>14.1.11.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a timerange can be provided (in milliseconds). The WAL is filtered to this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p></div><div class="section" title="14.1.12.&nbsp;RowCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>14.1.12.&nbsp;RowCounter</h3></div></div></div><p>RowCounter is a mapreduce job to count all the rows of a table.  This is a good utility to use
           as a sanity check to ensure that HBase can read all the blocks of a table if there are any concerns of metadata inconsistency.
           It will run the mapreduce all in a single process but it will run faster if you have a MapReduce cluster in place for it to
           exploit.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'ops_mgt';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="casestudies.perftroub.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="ops.regionmgt.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">13.3.&nbsp;Performance/Troubleshooting&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;14.2.&nbsp;Region Management</td></tr></table></div></body></html>
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: Procedure.proto

package org.apache.hadoop.hbase.shaded.protobuf.generated;

public final class ProcedureProtos {
  private ProcedureProtos() {}
  public static void registerAllExtensions(
      org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * Protobuf enum {@code hbase.pb.ProcedureState}
   */
  public enum ProcedureState
      implements org.apache.hadoop.hbase.shaded.com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     * Procedure in construction, not yet added to the executor
     * </pre>
     *
     * <code>INITIALIZING = 1;</code>
     */
    INITIALIZING(1),
    /**
     * <pre>
     * Procedure added to the executor, and ready to be executed
     * </pre>
     *
     * <code>RUNNABLE = 2;</code>
     */
    RUNNABLE(2),
    /**
     * <pre>
     * The procedure is waiting on children to be completed
     * </pre>
     *
     * <code>WAITING = 3;</code>
     */
    WAITING(3),
    /**
     * <pre>
     * The procedure is waiting a timout or an external event
     * </pre>
     *
     * <code>WAITING_TIMEOUT = 4;</code>
     */
    WAITING_TIMEOUT(4),
    /**
     * <pre>
     * The procedure failed and was rolledback
     * </pre>
     *
     * <code>ROLLEDBACK = 5;</code>
     */
    ROLLEDBACK(5),
    /**
     * <pre>
     * The procedure execution is completed successfully.
     * </pre>
     *
     * <code>SUCCESS = 6;</code>
     */
    SUCCESS(6),
    /**
     * <pre>
     * The procedure execution is failed, may need to rollback
     * </pre>
     *
     * <code>FAILED = 7;</code>
     */
    FAILED(7),
    ;

    /**
     * <pre>
     * Procedure in construction, not yet added to the executor
     * </pre>
     *
     * <code>INITIALIZING = 1;</code>
     */
    public static final int INITIALIZING_VALUE = 1;
    /**
     * <pre>
     * Procedure added to the executor, and ready to be executed
     * </pre>
     *
     * <code>RUNNABLE = 2;</code>
     */
    public static final int RUNNABLE_VALUE = 2;
    /**
     * <pre>
     * The procedure is waiting on children to be completed
     * </pre>
     *
     * <code>WAITING = 3;</code>
     */
    public static final int WAITING_VALUE = 3;
    /**
     * <pre>
     * The procedure is waiting a timout or an external event
     * </pre>
     *
     * <code>WAITING_TIMEOUT = 4;</code>
     */
    public static final int WAITING_TIMEOUT_VALUE = 4;
    /**
     * <pre>
     * The procedure failed and was rolledback
     * </pre>
     *
     * <code>ROLLEDBACK = 5;</code>
     */
    public static final int ROLLEDBACK_VALUE = 5;
    /**
     * <pre>
     * The procedure execution is completed successfully.
     * </pre>
     *
     * <code>SUCCESS = 6;</code>
     */
    public static final int SUCCESS_VALUE = 6;
    /**
     * <pre>
     * The procedure execution is failed, may need to rollback
     * </pre>
     *
     * <code>FAILED = 7;</code>
     */
    public static final int FAILED_VALUE = 7;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static ProcedureState valueOf(int value) {
      return forNumber(value);
    }

    public static ProcedureState forNumber(int value) {
      switch (value) {
        case 1: return INITIALIZING;
        case 2: return RUNNABLE;
        case 3: return WAITING;
        case 4: return WAITING_TIMEOUT;
        case 5: return ROLLEDBACK;
        case 6: return SUCCESS;
        case 7: return FAILED;
        default: return null;
      }
    }

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<ProcedureState>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<
        ProcedureState> internalValueMap =
          new org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<ProcedureState>() {
            public ProcedureState findValueByNumber(int number) {
              return ProcedureState.forNumber(number);
            }
          };

    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final ProcedureState[] VALUES = values();

    public static ProcedureState valueOf(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private ProcedureState(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hbase.pb.ProcedureState)
  }

  public interface ProcedureOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.Procedure)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    boolean hasClassName();
    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    java.lang.String getClassName();
    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
        getClassNameBytes();

    /**
     * <pre>
     * parent if not a root-procedure otherwise not set
     * </pre>
     *
     * <code>optional uint64 parent_id = 2;</code>
     */
    boolean hasParentId();
    /**
     * <pre>
     * parent if not a root-procedure otherwise not set
     * </pre>
     *
     * <code>optional uint64 parent_id = 2;</code>
     */
    long getParentId();

    /**
     * <code>required uint64 proc_id = 3;</code>
     */
    boolean hasProcId();
    /**
     * <code>required uint64 proc_id = 3;</code>
     */
    long getProcId();

    /**
     * <code>required uint64 submitted_time = 4;</code>
     */
    boolean hasSubmittedTime();
    /**
     * <code>required uint64 submitted_time = 4;</code>
     */
    long getSubmittedTime();

    /**
     * <code>optional string owner = 5;</code>
     */
    boolean hasOwner();
    /**
     * <code>optional string owner = 5;</code>
     */
    java.lang.String getOwner();
    /**
     * <code>optional string owner = 5;</code>
     */
    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
        getOwnerBytes();

    /**
     * <pre>
     * internal "runtime" state
     * </pre>
     *
     * <code>required .hbase.pb.ProcedureState state = 6;</code>
     */
    boolean hasState();
    /**
     * <pre>
     * internal "runtime" state
     * </pre>
     *
     * <code>required .hbase.pb.ProcedureState state = 6;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState getState();

    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    java.util.List<java.lang.Integer> getStackIdList();
    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    int getStackIdCount();
    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    int getStackId(int index);

    /**
     * <code>required uint64 last_update = 8;</code>
     */
    boolean hasLastUpdate();
    /**
     * <code>required uint64 last_update = 8;</code>
     */
    long getLastUpdate();

    /**
     * <code>optional uint32 timeout = 9;</code>
     */
    boolean hasTimeout();
    /**
     * <code>optional uint32 timeout = 9;</code>
     */
    int getTimeout();

    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    boolean hasException();
    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage getException();
    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder getExceptionOrBuilder();

    /**
     * <pre>
     * opaque (user) result structure
     * </pre>
     *
     * <code>optional bytes result = 11;</code>
     */
    boolean hasResult();
    /**
     * <pre>
     * opaque (user) result structure
     * </pre>
     *
     * <code>optional bytes result = 11;</code>
     */
    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getResult();

    /**
     * <pre>
     * opaque (user) procedure internal-state
     * </pre>
     *
     * <code>optional bytes state_data = 12;</code>
     */
    boolean hasStateData();
    /**
     * <pre>
     * opaque (user) procedure internal-state
     * </pre>
     *
     * <code>optional bytes state_data = 12;</code>
     */
    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getStateData();

    /**
     * <pre>
     * Nonce to prevent same procedure submit by multiple times
     * </pre>
     *
     * <code>optional uint64 nonce_group = 13 [default = 0];</code>
     */
    boolean hasNonceGroup();
    /**
     * <pre>
     * Nonce to prevent same procedure submit by multiple times
     * </pre>
     *
     * <code>optional uint64 nonce_group = 13 [default = 0];</code>
     */
    long getNonceGroup();

    /**
     * <code>optional uint64 nonce = 14 [default = 0];</code>
     */
    boolean hasNonce();
    /**
     * <code>optional uint64 nonce = 14 [default = 0];</code>
     */
    long getNonce();
  }
  /**
   * <pre>
   **
   * Procedure metadata, serialized by the ProcedureStore to be able to recover the old state.
   * </pre>
   *
   * Protobuf type {@code hbase.pb.Procedure}
   */
  public  static final class Procedure extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.Procedure)
      ProcedureOrBuilder {
    // Use Procedure.newBuilder() to construct.
    private Procedure(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Procedure() {
      className_ = "";
      parentId_ = 0L;
      procId_ = 0L;
      submittedTime_ = 0L;
      owner_ = "";
      state_ = 1;
      stackId_ = java.util.Collections.emptyList();
      lastUpdate_ = 0L;
      timeout_ = 0;
      result_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
      stateData_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
      nonceGroup_ = 0L;
      nonce_ = 0L;
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Procedure(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              className_ = bs;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              parentId_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              procId_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              submittedTime_ = input.readUInt64();
              break;
            }
            case 42: {
              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              owner_ = bs;
              break;
            }
            case 48: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState value = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(6, rawValue);
              } else {
                bitField0_ |= 0x00000020;
                state_ = rawValue;
              }
              break;
            }
            case 56: {
              if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
                stackId_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000040;
              }
              stackId_.add(input.readUInt32());
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000040) == 0x00000040) && input.getBytesUntilLimit() > 0) {
                stackId_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000040;
              }
              while (input.getBytesUntilLimit() > 0) {
                stackId_.add(input.readUInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 64: {
              bitField0_ |= 0x00000040;
              lastUpdate_ = input.readUInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000080;
              timeout_ = input.readUInt32();
              break;
            }
            case 82: {
              org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) == 0x00000100)) {
                subBuilder = exception_.toBuilder();
              }
              exception_ = input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(exception_);
                exception_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            case 90: {
              bitField0_ |= 0x00000200;
              result_ = input.readBytes();
              break;
            }
            case 98: {
              bitField0_ |= 0x00000400;
              stateData_ = input.readBytes();
              break;
            }
            case 104: {
              bitField0_ |= 0x00000800;
              nonceGroup_ = input.readUInt64();
              break;
            }
            case 112: {
              bitField0_ |= 0x00001000;
              nonce_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
          stackId_ = java.util.Collections.unmodifiableList(stackId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_Procedure_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_Procedure_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder.class);
    }

    private int bitField0_;
    public static final int CLASS_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object className_;
    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    public boolean hasClassName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    public java.lang.String getClassName() {
      java.lang.Object ref = className_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = 
            (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          className_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * internal "static" state
     * </pre>
     *
     * <code>required string class_name = 1;</code>
     */
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
        getClassNameBytes() {
      java.lang.Object ref = className_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        className_ = b;
        return b;
      } else {
        return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PARENT_ID_FIELD_NUMBER = 2;
    private long parentId_;
    /**
     * <pre>
     * parent if not a root-procedure otherwise not set
     * </pre>
     *
     * <code>optional uint64 parent_id = 2;</code>
     */
    public boolean hasParentId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <pre>
     * parent if not a root-procedure otherwise not set
     * </pre>
     *
     * <code>optional uint64 parent_id = 2;</code>
     */
    public long getParentId() {
      return parentId_;
    }

    public static final int PROC_ID_FIELD_NUMBER = 3;
    private long procId_;
    /**
     * <code>required uint64 proc_id = 3;</code>
     */
    public boolean hasProcId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint64 proc_id = 3;</code>
     */
    public long getProcId() {
      return procId_;
    }

    public static final int SUBMITTED_TIME_FIELD_NUMBER = 4;
    private long submittedTime_;
    /**
     * <code>required uint64 submitted_time = 4;</code>
     */
    public boolean hasSubmittedTime() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 submitted_time = 4;</code>
     */
    public long getSubmittedTime() {
      return submittedTime_;
    }

    public static final int OWNER_FIELD_NUMBER = 5;
    private volatile java.lang.Object owner_;
    /**
     * <code>optional string owner = 5;</code>
     */
    public boolean hasOwner() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional string owner = 5;</code>
     */
    public java.lang.String getOwner() {
      java.lang.Object ref = owner_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = 
            (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          owner_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string owner = 5;</code>
     */
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
        getOwnerBytes() {
      java.lang.Object ref = owner_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        owner_ = b;
        return b;
      } else {
        return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STATE_FIELD_NUMBER = 6;
    private int state_;
    /**
     * <pre>
     * internal "runtime" state
     * </pre>
     *
     * <code>required .hbase.pb.ProcedureState state = 6;</code>
     */
    public boolean hasState() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <pre>
     * internal "runtime" state
     * </pre>
     *
     * <code>required .hbase.pb.ProcedureState state = 6;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState getState() {
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState result = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState.valueOf(state_);
      return result == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState.INITIALIZING : result;
    }

    public static final int STACK_ID_FIELD_NUMBER = 7;
    private java.util.List<java.lang.Integer> stackId_;
    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    public java.util.List<java.lang.Integer>
        getStackIdList() {
      return stackId_;
    }
    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    public int getStackIdCount() {
      return stackId_.size();
    }
    /**
     * <pre>
     * stack indices in case the procedure was running
     * </pre>
     *
     * <code>repeated uint32 stack_id = 7;</code>
     */
    public int getStackId(int index) {
      return stackId_.get(index);
    }

    public static final int LAST_UPDATE_FIELD_NUMBER = 8;
    private long lastUpdate_;
    /**
     * <code>required uint64 last_update = 8;</code>
     */
    public boolean hasLastUpdate() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>required uint64 last_update = 8;</code>
     */
    public long getLastUpdate() {
      return lastUpdate_;
    }

    public static final int TIMEOUT_FIELD_NUMBER = 9;
    private int timeout_;
    /**
     * <code>optional uint32 timeout = 9;</code>
     */
    public boolean hasTimeout() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint32 timeout = 9;</code>
     */
    public int getTimeout() {
      return timeout_;
    }

    public static final int EXCEPTION_FIELD_NUMBER = 10;
    private org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage exception_;
    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    public boolean hasException() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage getException() {
      return exception_ == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.getDefaultInstance() : exception_;
    }
    /**
     * <pre>
     * user state/results
     * </pre>
     *
     * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder getExceptionOrBuilder() {
      return exception_ == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.getDefaultInstance() : exception_;
    }

    public static final int RESULT_FIELD_NUMBER = 11;
    private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString result_;
    /**
     * <pre>
     * opaque (user) result structure
     * </pre>
     *
     * <code>optional bytes result = 11;</code>
     */
    public boolean hasResult() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <pre>
     * opaque (user) result structure
     * </pre>
     *
     * <code>optional bytes result = 11;</code>
     */
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getResult() {
      return result_;
    }

    public static final int STATE_DATA_FIELD_NUMBER = 12;
    private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString stateData_;
    /**
     * <pre>
     * opaque (user) procedure internal-state
     * </pre>
     *
     * <code>optional bytes state_data = 12;</code>
     */
    public boolean hasStateData() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <pre>
     * opaque (user) procedure internal-state
     * </pre>
     *
     * <code>optional bytes state_data = 12;</code>
     */
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getStateData() {
      return stateData_;
    }

    public static final int NONCE_GROUP_FIELD_NUMBER = 13;
    private long nonceGroup_;
    /**
     * <pre>
     * Nonce to prevent same procedure submit by multiple times
     * </pre>
     *
     * <code>optional uint64 nonce_group = 13 [default = 0];</code>
     */
    public boolean hasNonceGroup() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <pre>
     * Nonce to prevent same procedure submit by multiple times
     * </pre>
     *
     * <code>optional uint64 nonce_group = 13 [default = 0];</code>
     */
    public long getNonceGroup() {
      return nonceGroup_;
    }

    public static final int NONCE_FIELD_NUMBER = 14;
    private long nonce_;
    /**
     * <code>optional uint64 nonce = 14 [default = 0];</code>
     */
    public boolean hasNonce() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional uint64 nonce = 14 [default = 0];</code>
     */
    public long getNonce() {
      return nonce_;
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasClassName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasProcId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSubmittedTime()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasState()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLastUpdate()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.writeString(output, 1, className_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, parentId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, procId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, submittedTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.writeString(output, 5, owner_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeEnum(6, state_);
      }
      for (int i = 0; i < stackId_.size(); i++) {
        output.writeUInt32(7, stackId_.get(i));
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(8, lastUpdate_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt32(9, timeout_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeMessage(10, getException());
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeBytes(11, result_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeBytes(12, stateData_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeUInt64(13, nonceGroup_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeUInt64(14, nonce_);
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.computeStringSize(1, className_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, parentId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, procId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, submittedTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.computeStringSize(5, owner_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeEnumSize(6, state_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < stackId_.size(); i++) {
          dataSize += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
            .computeUInt32SizeNoTag(stackId_.get(i));
        }
        size += dataSize;
        size += 1 * getStackIdList().size();
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(8, lastUpdate_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt32Size(9, timeout_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, getException());
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeBytesSize(11, result_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeBytesSize(12, stateData_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(13, nonceGroup_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(14, nonce_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure) obj;

      boolean result = true;
      result = result && (hasClassName() == other.hasClassName());
      if (hasClassName()) {
        result = result && getClassName()
            .equals(other.getClassName());
      }
      result = result && (hasParentId() == other.hasParentId());
      if (hasParentId()) {
        result = result && (getParentId()
            == other.getParentId());
      }
      result = result && (hasProcId() == other.hasProcId());
      if (hasProcId()) {
        result = result && (getProcId()
            == other.getProcId());
      }
      result = result && (hasSubmittedTime() == other.hasSubmittedTime());
      if (hasSubmittedTime()) {
        result = result && (getSubmittedTime()
            == other.getSubmittedTime());
      }
      result = result && (hasOwner() == other.hasOwner());
      if (hasOwner()) {
        result = result && getOwner()
            .equals(other.getOwner());
      }
      result = result && (hasState() == other.hasState());
      if (hasState()) {
        result = result && state_ == other.state_;
      }
      result = result && getStackIdList()
          .equals(other.getStackIdList());
      result = result && (hasLastUpdate() == other.hasLastUpdate());
      if (hasLastUpdate()) {
        result = result && (getLastUpdate()
            == other.getLastUpdate());
      }
      result = result && (hasTimeout() == other.hasTimeout());
      if (hasTimeout()) {
        result = result && (getTimeout()
            == other.getTimeout());
      }
      result = result && (hasException() == other.hasException());
      if (hasException()) {
        result = result && getException()
            .equals(other.getException());
      }
      result = result && (hasResult() == other.hasResult());
      if (hasResult()) {
        result = result && getResult()
            .equals(other.getResult());
      }
      result = result && (hasStateData() == other.hasStateData());
      if (hasStateData()) {
        result = result && getStateData()
            .equals(other.getStateData());
      }
      result = result && (hasNonceGroup() == other.hasNonceGroup());
      if (hasNonceGroup()) {
        result = result && (getNonceGroup()
            == other.getNonceGroup());
      }
      result = result && (hasNonce() == other.hasNonce());
      if (hasNonce()) {
        result = result && (getNonce()
            == other.getNonce());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasClassName()) {
        hash = (37 * hash) + CLASS_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getClassName().hashCode();
      }
      if (hasParentId()) {
        hash = (37 * hash) + PARENT_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getParentId());
      }
      if (hasProcId()) {
        hash = (37 * hash) + PROC_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getProcId());
      }
      if (hasSubmittedTime()) {
        hash = (37 * hash) + SUBMITTED_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getSubmittedTime());
      }
      if (hasOwner()) {
        hash = (37 * hash) + OWNER_FIELD_NUMBER;
        hash = (53 * hash) + getOwner().hashCode();
      }
      if (hasState()) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + state_;
      }
      if (getStackIdCount() > 0) {
        hash = (37 * hash) + STACK_ID_FIELD_NUMBER;
        hash = (53 * hash) + getStackIdList().hashCode();
      }
      if (hasLastUpdate()) {
        hash = (37 * hash) + LAST_UPDATE_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getLastUpdate());
      }
      if (hasTimeout()) {
        hash = (37 * hash) + TIMEOUT_FIELD_NUMBER;
        hash = (53 * hash) + getTimeout();
      }
      if (hasException()) {
        hash = (37 * hash) + EXCEPTION_FIELD_NUMBER;
        hash = (53 * hash) + getException().hashCode();
      }
      if (hasResult()) {
        hash = (37 * hash) + RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getResult().hashCode();
      }
      if (hasStateData()) {
        hash = (37 * hash) + STATE_DATA_FIELD_NUMBER;
        hash = (53 * hash) + getStateData().hashCode();
      }
      if (hasNonceGroup()) {
        hash = (37 * hash) + NONCE_GROUP_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getNonceGroup());
      }
      if (hasNonce()) {
        hash = (37 * hash) + NONCE_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getNonce());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Procedure metadata, serialized by the ProcedureStore to be able to recover the old state.
     * </pre>
     *
     * Protobuf type {@code hbase.pb.Procedure}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.Procedure)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_Procedure_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_Procedure_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getExceptionFieldBuilder();
        }
      }
      public Builder clear() {
        super.clear();
        className_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        parentId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        procId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        submittedTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        owner_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        state_ = 1;
        bitField0_ = (bitField0_ & ~0x00000020);
        stackId_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000040);
        lastUpdate_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        timeout_ = 0;
        bitField0_ = (bitField0_ & ~0x00000100);
        if (exceptionBuilder_ == null) {
          exception_ = null;
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        result_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000400);
        stateData_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000800);
        nonceGroup_ = 0L;
        bitField0_ = (bitField0_ & ~0x00001000);
        nonce_ = 0L;
        bitField0_ = (bitField0_ & ~0x00002000);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_Procedure_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.className_ = className_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.parentId_ = parentId_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.procId_ = procId_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.submittedTime_ = submittedTime_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.owner_ = owner_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.state_ = state_;
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          stackId_ = java.util.Collections.unmodifiableList(stackId_);
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.stackId_ = stackId_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000040;
        }
        result.lastUpdate_ = lastUpdate_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000080;
        }
        result.timeout_ = timeout_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000100;
        }
        if (exceptionBuilder_ == null) {
          result.exception_ = exception_;
        } else {
          result.exception_ = exceptionBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000200;
        }
        result.result_ = result_;
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000400;
        }
        result.stateData_ = stateData_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00000800;
        }
        result.nonceGroup_ = nonceGroup_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.nonce_ = nonce_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.getDefaultInstance()) return this;
        if (other.hasClassName()) {
          bitField0_ |= 0x00000001;
          className_ = other.className_;
          onChanged();
        }
        if (other.hasParentId()) {
          setParentId(other.getParentId());
        }
        if (other.hasProcId()) {
          setProcId(other.getProcId());
        }
        if (other.hasSubmittedTime()) {
          setSubmittedTime(other.getSubmittedTime());
        }
        if (other.hasOwner()) {
          bitField0_ |= 0x00000010;
          owner_ = other.owner_;
          onChanged();
        }
        if (other.hasState()) {
          setState(other.getState());
        }
        if (!other.stackId_.isEmpty()) {
          if (stackId_.isEmpty()) {
            stackId_ = other.stackId_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureStackIdIsMutable();
            stackId_.addAll(other.stackId_);
          }
          onChanged();
        }
        if (other.hasLastUpdate()) {
          setLastUpdate(other.getLastUpdate());
        }
        if (other.hasTimeout()) {
          setTimeout(other.getTimeout());
        }
        if (other.hasException()) {
          mergeException(other.getException());
        }
        if (other.hasResult()) {
          setResult(other.getResult());
        }
        if (other.hasStateData()) {
          setStateData(other.getStateData());
        }
        if (other.hasNonceGroup()) {
          setNonceGroup(other.getNonceGroup());
        }
        if (other.hasNonce()) {
          setNonce(other.getNonce());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        if (!hasClassName()) {
          return false;
        }
        if (!hasProcId()) {
          return false;
        }
        if (!hasSubmittedTime()) {
          return false;
        }
        if (!hasState()) {
          return false;
        }
        if (!hasLastUpdate()) {
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object className_ = "";
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public boolean hasClassName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public java.lang.String getClassName() {
        java.lang.Object ref = className_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs =
              (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            className_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
          getClassNameBytes() {
        java.lang.Object ref = className_;
        if (ref instanceof String) {
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          className_ = b;
          return b;
        } else {
          return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public Builder setClassName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        className_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public Builder clearClassName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        className_ = getDefaultInstance().getClassName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * internal "static" state
       * </pre>
       *
       * <code>required string class_name = 1;</code>
       */
      public Builder setClassNameBytes(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        className_ = value;
        onChanged();
        return this;
      }

      private long parentId_ ;
      /**
       * <pre>
       * parent if not a root-procedure otherwise not set
       * </pre>
       *
       * <code>optional uint64 parent_id = 2;</code>
       */
      public boolean hasParentId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <pre>
       * parent if not a root-procedure otherwise not set
       * </pre>
       *
       * <code>optional uint64 parent_id = 2;</code>
       */
      public long getParentId() {
        return parentId_;
      }
      /**
       * <pre>
       * parent if not a root-procedure otherwise not set
       * </pre>
       *
       * <code>optional uint64 parent_id = 2;</code>
       */
      public Builder setParentId(long value) {
        bitField0_ |= 0x00000002;
        parentId_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * parent if not a root-procedure otherwise not set
       * </pre>
       *
       * <code>optional uint64 parent_id = 2;</code>
       */
      public Builder clearParentId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        parentId_ = 0L;
        onChanged();
        return this;
      }

      private long procId_ ;
      /**
       * <code>required uint64 proc_id = 3;</code>
       */
      public boolean hasProcId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint64 proc_id = 3;</code>
       */
      public long getProcId() {
        return procId_;
      }
      /**
       * <code>required uint64 proc_id = 3;</code>
       */
      public Builder setProcId(long value) {
        bitField0_ |= 0x00000004;
        procId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 proc_id = 3;</code>
       */
      public Builder clearProcId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        procId_ = 0L;
        onChanged();
        return this;
      }

      private long submittedTime_ ;
      /**
       * <code>required uint64 submitted_time = 4;</code>
       */
      public boolean hasSubmittedTime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 submitted_time = 4;</code>
       */
      public long getSubmittedTime() {
        return submittedTime_;
      }
      /**
       * <code>required uint64 submitted_time = 4;</code>
       */
      public Builder setSubmittedTime(long value) {
        bitField0_ |= 0x00000008;
        submittedTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 submitted_time = 4;</code>
       */
      public Builder clearSubmittedTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        submittedTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object owner_ = "";
      /**
       * <code>optional string owner = 5;</code>
       */
      public boolean hasOwner() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional string owner = 5;</code>
       */
      public java.lang.String getOwner() {
        java.lang.Object ref = owner_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs =
              (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            owner_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string owner = 5;</code>
       */
      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
          getOwnerBytes() {
        java.lang.Object ref = owner_;
        if (ref instanceof String) {
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          owner_ = b;
          return b;
        } else {
          return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string owner = 5;</code>
       */
      public Builder setOwner(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        owner_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string owner = 5;</code>
       */
      public Builder clearOwner() {
        bitField0_ = (bitField0_ & ~0x00000010);
        owner_ = getDefaultInstance().getOwner();
        onChanged();
        return this;
      }
      /**
       * <code>optional string owner = 5;</code>
       */
      public Builder setOwnerBytes(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        owner_ = value;
        onChanged();
        return this;
      }

      private int state_ = 1;
      /**
       * <pre>
       * internal "runtime" state
       * </pre>
       *
       * <code>required .hbase.pb.ProcedureState state = 6;</code>
       */
      public boolean hasState() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <pre>
       * internal "runtime" state
       * </pre>
       *
       * <code>required .hbase.pb.ProcedureState state = 6;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState getState() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState result = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState.valueOf(state_);
        return result == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState.INITIALIZING : result;
      }
      /**
       * <pre>
       * internal "runtime" state
       * </pre>
       *
       * <code>required .hbase.pb.ProcedureState state = 6;</code>
       */
      public Builder setState(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000020;
        state_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * internal "runtime" state
       * </pre>
       *
       * <code>required .hbase.pb.ProcedureState state = 6;</code>
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000020);
        state_ = 1;
        onChanged();
        return this;
      }

      private java.util.List<java.lang.Integer> stackId_ = java.util.Collections.emptyList();
      private void ensureStackIdIsMutable() {
        if (!((bitField0_ & 0x00000040) == 0x00000040)) {
          stackId_ = new java.util.ArrayList<java.lang.Integer>(stackId_);
          bitField0_ |= 0x00000040;
         }
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public java.util.List<java.lang.Integer>
          getStackIdList() {
        return java.util.Collections.unmodifiableList(stackId_);
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public int getStackIdCount() {
        return stackId_.size();
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public int getStackId(int index) {
        return stackId_.get(index);
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public Builder setStackId(
          int index, int value) {
        ensureStackIdIsMutable();
        stackId_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public Builder addStackId(int value) {
        ensureStackIdIsMutable();
        stackId_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public Builder addAllStackId(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureStackIdIsMutable();
        org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, stackId_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       * stack indices in case the procedure was running
       * </pre>
       *
       * <code>repeated uint32 stack_id = 7;</code>
       */
      public Builder clearStackId() {
        stackId_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      private long lastUpdate_ ;
      /**
       * <code>required uint64 last_update = 8;</code>
       */
      public boolean hasLastUpdate() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>required uint64 last_update = 8;</code>
       */
      public long getLastUpdate() {
        return lastUpdate_;
      }
      /**
       * <code>required uint64 last_update = 8;</code>
       */
      public Builder setLastUpdate(long value) {
        bitField0_ |= 0x00000080;
        lastUpdate_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 last_update = 8;</code>
       */
      public Builder clearLastUpdate() {
        bitField0_ = (bitField0_ & ~0x00000080);
        lastUpdate_ = 0L;
        onChanged();
        return this;
      }

      private int timeout_ ;
      /**
       * <code>optional uint32 timeout = 9;</code>
       */
      public boolean hasTimeout() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint32 timeout = 9;</code>
       */
      public int getTimeout() {
        return timeout_;
      }
      /**
       * <code>optional uint32 timeout = 9;</code>
       */
      public Builder setTimeout(int value) {
        bitField0_ |= 0x00000100;
        timeout_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 timeout = 9;</code>
       */
      public Builder clearTimeout() {
        bitField0_ = (bitField0_ & ~0x00000100);
        timeout_ = 0;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage exception_ = null;
      private org.apache.hadoop.hbase.shaded.com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder> exceptionBuilder_;
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public boolean hasException() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage getException() {
        if (exceptionBuilder_ == null) {
          return exception_ == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.getDefaultInstance() : exception_;
        } else {
          return exceptionBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public Builder setException(org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage value) {
        if (exceptionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          exception_ = value;
          onChanged();
        } else {
          exceptionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public Builder setException(
          org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder builderForValue) {
        if (exceptionBuilder_ == null) {
          exception_ = builderForValue.build();
          onChanged();
        } else {
          exceptionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public Builder mergeException(org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage value) {
        if (exceptionBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200) &&
              exception_ != null &&
              exception_ != org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.getDefaultInstance()) {
            exception_ =
              org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.newBuilder(exception_).mergeFrom(value).buildPartial();
          } else {
            exception_ = value;
          }
          onChanged();
        } else {
          exceptionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public Builder clearException() {
        if (exceptionBuilder_ == null) {
          exception_ = null;
          onChanged();
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder getExceptionBuilder() {
        bitField0_ |= 0x00000200;
        onChanged();
        return getExceptionFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder getExceptionOrBuilder() {
        if (exceptionBuilder_ != null) {
          return exceptionBuilder_.getMessageOrBuilder();
        } else {
          return exception_ == null ?
              org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.getDefaultInstance() : exception_;
        }
      }
      /**
       * <pre>
       * user state/results
       * </pre>
       *
       * <code>optional .hbase.pb.ForeignExceptionMessage exception = 10;</code>
       */
      private org.apache.hadoop.hbase.shaded.com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder> 
          getExceptionFieldBuilder() {
        if (exceptionBuilder_ == null) {
          exceptionBuilder_ = new org.apache.hadoop.hbase.shaded.com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessage.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.ForeignExceptionMessageOrBuilder>(
                  getException(),
                  getParentForChildren(),
                  isClean());
          exception_ = null;
        }
        return exceptionBuilder_;
      }

      private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString result_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * opaque (user) result structure
       * </pre>
       *
       * <code>optional bytes result = 11;</code>
       */
      public boolean hasResult() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <pre>
       * opaque (user) result structure
       * </pre>
       *
       * <code>optional bytes result = 11;</code>
       */
      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getResult() {
        return result_;
      }
      /**
       * <pre>
       * opaque (user) result structure
       * </pre>
       *
       * <code>optional bytes result = 11;</code>
       */
      public Builder setResult(org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        result_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * opaque (user) result structure
       * </pre>
       *
       * <code>optional bytes result = 11;</code>
       */
      public Builder clearResult() {
        bitField0_ = (bitField0_ & ~0x00000400);
        result_ = getDefaultInstance().getResult();
        onChanged();
        return this;
      }

      private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString stateData_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * opaque (user) procedure internal-state
       * </pre>
       *
       * <code>optional bytes state_data = 12;</code>
       */
      public boolean hasStateData() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <pre>
       * opaque (user) procedure internal-state
       * </pre>
       *
       * <code>optional bytes state_data = 12;</code>
       */
      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getStateData() {
        return stateData_;
      }
      /**
       * <pre>
       * opaque (user) procedure internal-state
       * </pre>
       *
       * <code>optional bytes state_data = 12;</code>
       */
      public Builder setStateData(org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        stateData_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * opaque (user) procedure internal-state
       * </pre>
       *
       * <code>optional bytes state_data = 12;</code>
       */
      public Builder clearStateData() {
        bitField0_ = (bitField0_ & ~0x00000800);
        stateData_ = getDefaultInstance().getStateData();
        onChanged();
        return this;
      }

      private long nonceGroup_ ;
      /**
       * <pre>
       * Nonce to prevent same procedure submit by multiple times
       * </pre>
       *
       * <code>optional uint64 nonce_group = 13 [default = 0];</code>
       */
      public boolean hasNonceGroup() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <pre>
       * Nonce to prevent same procedure submit by multiple times
       * </pre>
       *
       * <code>optional uint64 nonce_group = 13 [default = 0];</code>
       */
      public long getNonceGroup() {
        return nonceGroup_;
      }
      /**
       * <pre>
       * Nonce to prevent same procedure submit by multiple times
       * </pre>
       *
       * <code>optional uint64 nonce_group = 13 [default = 0];</code>
       */
      public Builder setNonceGroup(long value) {
        bitField0_ |= 0x00001000;
        nonceGroup_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Nonce to prevent same procedure submit by multiple times
       * </pre>
       *
       * <code>optional uint64 nonce_group = 13 [default = 0];</code>
       */
      public Builder clearNonceGroup() {
        bitField0_ = (bitField0_ & ~0x00001000);
        nonceGroup_ = 0L;
        onChanged();
        return this;
      }

      private long nonce_ ;
      /**
       * <code>optional uint64 nonce = 14 [default = 0];</code>
       */
      public boolean hasNonce() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional uint64 nonce = 14 [default = 0];</code>
       */
      public long getNonce() {
        return nonce_;
      }
      /**
       * <code>optional uint64 nonce = 14 [default = 0];</code>
       */
      public Builder setNonce(long value) {
        bitField0_ |= 0x00002000;
        nonce_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 nonce = 14 [default = 0];</code>
       */
      public Builder clearNonce() {
        bitField0_ = (bitField0_ & ~0x00002000);
        nonce_ = 0L;
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.Procedure)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.Procedure)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<Procedure>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<Procedure>() {
      public Procedure parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new Procedure(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<Procedure> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<Procedure> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SequentialProcedureDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.SequentialProcedureData)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required bool executed = 1;</code>
     */
    boolean hasExecuted();
    /**
     * <code>required bool executed = 1;</code>
     */
    boolean getExecuted();
  }
  /**
   * <pre>
   **
   * SequentialProcedure data
   * </pre>
   *
   * Protobuf type {@code hbase.pb.SequentialProcedureData}
   */
  public  static final class SequentialProcedureData extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.SequentialProcedureData)
      SequentialProcedureDataOrBuilder {
    // Use SequentialProcedureData.newBuilder() to construct.
    private SequentialProcedureData(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SequentialProcedureData() {
      executed_ = false;
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SequentialProcedureData(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              executed_ = input.readBool();
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_SequentialProcedureData_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_SequentialProcedureData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.Builder.class);
    }

    private int bitField0_;
    public static final int EXECUTED_FIELD_NUMBER = 1;
    private boolean executed_;
    /**
     * <code>required bool executed = 1;</code>
     */
    public boolean hasExecuted() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bool executed = 1;</code>
     */
    public boolean getExecuted() {
      return executed_;
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasExecuted()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, executed_);
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, executed_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData) obj;

      boolean result = true;
      result = result && (hasExecuted() == other.hasExecuted());
      if (hasExecuted()) {
        result = result && (getExecuted()
            == other.getExecuted());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasExecuted()) {
        hash = (37 * hash) + EXECUTED_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashBoolean(
            getExecuted());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * SequentialProcedure data
     * </pre>
     *
     * Protobuf type {@code hbase.pb.SequentialProcedureData}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.SequentialProcedureData)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureDataOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_SequentialProcedureData_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_SequentialProcedureData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      public Builder clear() {
        super.clear();
        executed_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_SequentialProcedureData_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.executed_ = executed_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData.getDefaultInstance()) return this;
        if (other.hasExecuted()) {
          setExecuted(other.getExecuted());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        if (!hasExecuted()) {
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private boolean executed_ ;
      /**
       * <code>required bool executed = 1;</code>
       */
      public boolean hasExecuted() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bool executed = 1;</code>
       */
      public boolean getExecuted() {
        return executed_;
      }
      /**
       * <code>required bool executed = 1;</code>
       */
      public Builder setExecuted(boolean value) {
        bitField0_ |= 0x00000001;
        executed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bool executed = 1;</code>
       */
      public Builder clearExecuted() {
        bitField0_ = (bitField0_ & ~0x00000001);
        executed_ = false;
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.SequentialProcedureData)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.SequentialProcedureData)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<SequentialProcedureData>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<SequentialProcedureData>() {
      public SequentialProcedureData parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new SequentialProcedureData(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<SequentialProcedureData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<SequentialProcedureData> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StateMachineProcedureDataOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.StateMachineProcedureData)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated uint32 state = 1;</code>
     */
    java.util.List<java.lang.Integer> getStateList();
    /**
     * <code>repeated uint32 state = 1;</code>
     */
    int getStateCount();
    /**
     * <code>repeated uint32 state = 1;</code>
     */
    int getState(int index);
  }
  /**
   * <pre>
   **
   * StateMachineProcedure data
   * </pre>
   *
   * Protobuf type {@code hbase.pb.StateMachineProcedureData}
   */
  public  static final class StateMachineProcedureData extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.StateMachineProcedureData)
      StateMachineProcedureDataOrBuilder {
    // Use StateMachineProcedureData.newBuilder() to construct.
    private StateMachineProcedureData(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StateMachineProcedureData() {
      state_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StateMachineProcedureData(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                state_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000001;
              }
              state_.add(input.readUInt32());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
                state_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                state_.add(input.readUInt32());
              }
              input.popLimit(limit);
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          state_ = java.util.Collections.unmodifiableList(state_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_StateMachineProcedureData_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_StateMachineProcedureData_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.Builder.class);
    }

    public static final int STATE_FIELD_NUMBER = 1;
    private java.util.List<java.lang.Integer> state_;
    /**
     * <code>repeated uint32 state = 1;</code>
     */
    public java.util.List<java.lang.Integer>
        getStateList() {
      return state_;
    }
    /**
     * <code>repeated uint32 state = 1;</code>
     */
    public int getStateCount() {
      return state_.size();
    }
    /**
     * <code>repeated uint32 state = 1;</code>
     */
    public int getState(int index) {
      return state_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < state_.size(); i++) {
        output.writeUInt32(1, state_.get(i));
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < state_.size(); i++) {
          dataSize += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
            .computeUInt32SizeNoTag(state_.get(i));
        }
        size += dataSize;
        size += 1 * getStateList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData) obj;

      boolean result = true;
      result = result && getStateList()
          .equals(other.getStateList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getStateCount() > 0) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + getStateList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * StateMachineProcedure data
     * </pre>
     *
     * Protobuf type {@code hbase.pb.StateMachineProcedureData}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.StateMachineProcedureData)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureDataOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_StateMachineProcedureData_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_StateMachineProcedureData_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      public Builder clear() {
        super.clear();
        state_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_StateMachineProcedureData_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          state_ = java.util.Collections.unmodifiableList(state_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.state_ = state_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData.getDefaultInstance()) return this;
        if (!other.state_.isEmpty()) {
          if (state_.isEmpty()) {
            state_ = other.state_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureStateIsMutable();
            state_.addAll(other.state_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<java.lang.Integer> state_ = java.util.Collections.emptyList();
      private void ensureStateIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          state_ = new java.util.ArrayList<java.lang.Integer>(state_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public java.util.List<java.lang.Integer>
          getStateList() {
        return java.util.Collections.unmodifiableList(state_);
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public int getStateCount() {
        return state_.size();
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public int getState(int index) {
        return state_.get(index);
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public Builder setState(
          int index, int value) {
        ensureStateIsMutable();
        state_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public Builder addState(int value) {
        ensureStateIsMutable();
        state_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public Builder addAllState(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureStateIsMutable();
        org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, state_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint32 state = 1;</code>
       */
      public Builder clearState() {
        state_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.StateMachineProcedureData)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.StateMachineProcedureData)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<StateMachineProcedureData>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<StateMachineProcedureData>() {
      public StateMachineProcedureData parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new StateMachineProcedureData(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<StateMachineProcedureData> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<StateMachineProcedureData> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.StateMachineProcedureData getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcedureWALHeaderOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.ProcedureWALHeader)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required uint32 version = 1;</code>
     */
    boolean hasVersion();
    /**
     * <code>required uint32 version = 1;</code>
     */
    int getVersion();

    /**
     * <code>required uint32 type = 2;</code>
     */
    boolean hasType();
    /**
     * <code>required uint32 type = 2;</code>
     */
    int getType();

    /**
     * <code>required uint64 log_id = 3;</code>
     */
    boolean hasLogId();
    /**
     * <code>required uint64 log_id = 3;</code>
     */
    long getLogId();

    /**
     * <code>required uint64 min_proc_id = 4;</code>
     */
    boolean hasMinProcId();
    /**
     * <code>required uint64 min_proc_id = 4;</code>
     */
    long getMinProcId();
  }
  /**
   * <pre>
   **
   * Procedure WAL header
   * </pre>
   *
   * Protobuf type {@code hbase.pb.ProcedureWALHeader}
   */
  public  static final class ProcedureWALHeader extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.ProcedureWALHeader)
      ProcedureWALHeaderOrBuilder {
    // Use ProcedureWALHeader.newBuilder() to construct.
    private ProcedureWALHeader(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcedureWALHeader() {
      version_ = 0;
      type_ = 0;
      logId_ = 0L;
      minProcId_ = 0L;
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcedureWALHeader(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              version_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              type_ = input.readUInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              logId_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              minProcId_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALHeader_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALHeader_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.Builder.class);
    }

    private int bitField0_;
    public static final int VERSION_FIELD_NUMBER = 1;
    private int version_;
    /**
     * <code>required uint32 version = 1;</code>
     */
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required uint32 version = 1;</code>
     */
    public int getVersion() {
      return version_;
    }

    public static final int TYPE_FIELD_NUMBER = 2;
    private int type_;
    /**
     * <code>required uint32 type = 2;</code>
     */
    public boolean hasType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint32 type = 2;</code>
     */
    public int getType() {
      return type_;
    }

    public static final int LOG_ID_FIELD_NUMBER = 3;
    private long logId_;
    /**
     * <code>required uint64 log_id = 3;</code>
     */
    public boolean hasLogId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint64 log_id = 3;</code>
     */
    public long getLogId() {
      return logId_;
    }

    public static final int MIN_PROC_ID_FIELD_NUMBER = 4;
    private long minProcId_;
    /**
     * <code>required uint64 min_proc_id = 4;</code>
     */
    public boolean hasMinProcId() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 min_proc_id = 4;</code>
     */
    public long getMinProcId() {
      return minProcId_;
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLogId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasMinProcId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, version_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, type_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, logId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, minProcId_);
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, version_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, type_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, logId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, minProcId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader) obj;

      boolean result = true;
      result = result && (hasVersion() == other.hasVersion());
      if (hasVersion()) {
        result = result && (getVersion()
            == other.getVersion());
      }
      result = result && (hasType() == other.hasType());
      if (hasType()) {
        result = result && (getType()
            == other.getType());
      }
      result = result && (hasLogId() == other.hasLogId());
      if (hasLogId()) {
        result = result && (getLogId()
            == other.getLogId());
      }
      result = result && (hasMinProcId() == other.hasMinProcId());
      if (hasMinProcId()) {
        result = result && (getMinProcId()
            == other.getMinProcId());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getVersion();
      }
      if (hasType()) {
        hash = (37 * hash) + TYPE_FIELD_NUMBER;
        hash = (53 * hash) + getType();
      }
      if (hasLogId()) {
        hash = (37 * hash) + LOG_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getLogId());
      }
      if (hasMinProcId()) {
        hash = (37 * hash) + MIN_PROC_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getMinProcId());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Procedure WAL header
     * </pre>
     *
     * Protobuf type {@code hbase.pb.ProcedureWALHeader}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.ProcedureWALHeader)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeaderOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALHeader_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALHeader_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      public Builder clear() {
        super.clear();
        version_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        logId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        minProcId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALHeader_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.version_ = version_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.type_ = type_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.logId_ = logId_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.minProcId_ = minProcId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader.getDefaultInstance()) return this;
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        if (other.hasType()) {
          setType(other.getType());
        }
        if (other.hasLogId()) {
          setLogId(other.getLogId());
        }
        if (other.hasMinProcId()) {
          setMinProcId(other.getMinProcId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        if (!hasVersion()) {
          return false;
        }
        if (!hasType()) {
          return false;
        }
        if (!hasLogId()) {
          return false;
        }
        if (!hasMinProcId()) {
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int version_ ;
      /**
       * <code>required uint32 version = 1;</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public int getVersion() {
        return version_;
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public Builder setVersion(int value) {
        bitField0_ |= 0x00000001;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        version_ = 0;
        onChanged();
        return this;
      }

      private int type_ ;
      /**
       * <code>required uint32 type = 2;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint32 type = 2;</code>
       */
      public int getType() {
        return type_;
      }
      /**
       * <code>required uint32 type = 2;</code>
       */
      public Builder setType(int value) {
        bitField0_ |= 0x00000002;
        type_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 type = 2;</code>
       */
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        type_ = 0;
        onChanged();
        return this;
      }

      private long logId_ ;
      /**
       * <code>required uint64 log_id = 3;</code>
       */
      public boolean hasLogId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint64 log_id = 3;</code>
       */
      public long getLogId() {
        return logId_;
      }
      /**
       * <code>required uint64 log_id = 3;</code>
       */
      public Builder setLogId(long value) {
        bitField0_ |= 0x00000004;
        logId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 log_id = 3;</code>
       */
      public Builder clearLogId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        logId_ = 0L;
        onChanged();
        return this;
      }

      private long minProcId_ ;
      /**
       * <code>required uint64 min_proc_id = 4;</code>
       */
      public boolean hasMinProcId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 min_proc_id = 4;</code>
       */
      public long getMinProcId() {
        return minProcId_;
      }
      /**
       * <code>required uint64 min_proc_id = 4;</code>
       */
      public Builder setMinProcId(long value) {
        bitField0_ |= 0x00000008;
        minProcId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 min_proc_id = 4;</code>
       */
      public Builder clearMinProcId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        minProcId_ = 0L;
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.ProcedureWALHeader)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.ProcedureWALHeader)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALHeader>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<ProcedureWALHeader>() {
      public ProcedureWALHeader parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new ProcedureWALHeader(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALHeader> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALHeader> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALHeader getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcedureWALTrailerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.ProcedureWALTrailer)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required uint32 version = 1;</code>
     */
    boolean hasVersion();
    /**
     * <code>required uint32 version = 1;</code>
     */
    int getVersion();

    /**
     * <code>required uint64 tracker_pos = 2;</code>
     */
    boolean hasTrackerPos();
    /**
     * <code>required uint64 tracker_pos = 2;</code>
     */
    long getTrackerPos();
  }
  /**
   * <pre>
   **
   * Procedure WAL trailer
   * </pre>
   *
   * Protobuf type {@code hbase.pb.ProcedureWALTrailer}
   */
  public  static final class ProcedureWALTrailer extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.ProcedureWALTrailer)
      ProcedureWALTrailerOrBuilder {
    // Use ProcedureWALTrailer.newBuilder() to construct.
    private ProcedureWALTrailer(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcedureWALTrailer() {
      version_ = 0;
      trackerPos_ = 0L;
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcedureWALTrailer(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              version_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              trackerPos_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALTrailer_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALTrailer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.Builder.class);
    }

    private int bitField0_;
    public static final int VERSION_FIELD_NUMBER = 1;
    private int version_;
    /**
     * <code>required uint32 version = 1;</code>
     */
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required uint32 version = 1;</code>
     */
    public int getVersion() {
      return version_;
    }

    public static final int TRACKER_POS_FIELD_NUMBER = 2;
    private long trackerPos_;
    /**
     * <code>required uint64 tracker_pos = 2;</code>
     */
    public boolean hasTrackerPos() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint64 tracker_pos = 2;</code>
     */
    public long getTrackerPos() {
      return trackerPos_;
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTrackerPos()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, version_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, trackerPos_);
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, version_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, trackerPos_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer) obj;

      boolean result = true;
      result = result && (hasVersion() == other.hasVersion());
      if (hasVersion()) {
        result = result && (getVersion()
            == other.getVersion());
      }
      result = result && (hasTrackerPos() == other.hasTrackerPos());
      if (hasTrackerPos()) {
        result = result && (getTrackerPos()
            == other.getTrackerPos());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getVersion();
      }
      if (hasTrackerPos()) {
        hash = (37 * hash) + TRACKER_POS_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getTrackerPos());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Procedure WAL trailer
     * </pre>
     *
     * Protobuf type {@code hbase.pb.ProcedureWALTrailer}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.ProcedureWALTrailer)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailerOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALTrailer_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALTrailer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      public Builder clear() {
        super.clear();
        version_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        trackerPos_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALTrailer_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.version_ = version_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.trackerPos_ = trackerPos_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer.getDefaultInstance()) return this;
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        if (other.hasTrackerPos()) {
          setTrackerPos(other.getTrackerPos());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        if (!hasVersion()) {
          return false;
        }
        if (!hasTrackerPos()) {
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int version_ ;
      /**
       * <code>required uint32 version = 1;</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public int getVersion() {
        return version_;
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public Builder setVersion(int value) {
        bitField0_ |= 0x00000001;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 version = 1;</code>
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        version_ = 0;
        onChanged();
        return this;
      }

      private long trackerPos_ ;
      /**
       * <code>required uint64 tracker_pos = 2;</code>
       */
      public boolean hasTrackerPos() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint64 tracker_pos = 2;</code>
       */
      public long getTrackerPos() {
        return trackerPos_;
      }
      /**
       * <code>required uint64 tracker_pos = 2;</code>
       */
      public Builder setTrackerPos(long value) {
        bitField0_ |= 0x00000002;
        trackerPos_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 tracker_pos = 2;</code>
       */
      public Builder clearTrackerPos() {
        bitField0_ = (bitField0_ & ~0x00000002);
        trackerPos_ = 0L;
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.ProcedureWALTrailer)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.ProcedureWALTrailer)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALTrailer>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<ProcedureWALTrailer>() {
      public ProcedureWALTrailer parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new ProcedureWALTrailer(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALTrailer> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALTrailer> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALTrailer getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcedureStoreTrackerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.ProcedureStoreTracker)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> 
        getNodeList();
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getNode(int index);
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    int getNodeCount();
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder> 
        getNodeOrBuilderList();
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder getNodeOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hbase.pb.ProcedureStoreTracker}
   */
  public  static final class ProcedureStoreTracker extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.ProcedureStoreTracker)
      ProcedureStoreTrackerOrBuilder {
    // Use ProcedureStoreTracker.newBuilder() to construct.
    private ProcedureStoreTracker(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcedureStoreTracker() {
      node_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcedureStoreTracker(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                node_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode>();
                mutable_bitField0_ |= 0x00000001;
              }
              node_.add(
                  input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          node_ = java.util.Collections.unmodifiableList(node_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.Builder.class);
    }

    public interface TrackerNodeOrBuilder extends
        // @@protoc_insertion_point(interface_extends:hbase.pb.ProcedureStoreTracker.TrackerNode)
        org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

      /**
       * <code>required uint64 start_id = 1;</code>
       */
      boolean hasStartId();
      /**
       * <code>required uint64 start_id = 1;</code>
       */
      long getStartId();

      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      java.util.List<java.lang.Long> getUpdatedList();
      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      int getUpdatedCount();
      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      long getUpdated(int index);

      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      java.util.List<java.lang.Long> getDeletedList();
      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      int getDeletedCount();
      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      long getDeleted(int index);
    }
    /**
     * Protobuf type {@code hbase.pb.ProcedureStoreTracker.TrackerNode}
     */
    public  static final class TrackerNode extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:hbase.pb.ProcedureStoreTracker.TrackerNode)
        TrackerNodeOrBuilder {
      // Use TrackerNode.newBuilder() to construct.
      private TrackerNode(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private TrackerNode() {
        startId_ = 0L;
        updated_ = java.util.Collections.emptyList();
        deleted_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private TrackerNode(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
        this();
        int mutable_bitField0_ = 0;
        org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                startId_ = input.readUInt64();
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  updated_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                updated_.add(input.readUInt64());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                  updated_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  updated_.add(input.readUInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                  deleted_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000004;
                }
                deleted_.add(input.readUInt64());
                break;
              }
              case 26: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004) && input.getBytesUntilLimit() > 0) {
                  deleted_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000004;
                }
                while (input.getBytesUntilLimit() > 0) {
                  deleted_.add(input.readUInt64());
                }
                input.popLimit(limit);
                break;
              }
            }
          }
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            updated_ = java.util.Collections.unmodifiableList(updated_);
          }
          if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
            deleted_ = java.util.Collections.unmodifiableList(deleted_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder.class);
      }

      private int bitField0_;
      public static final int START_ID_FIELD_NUMBER = 1;
      private long startId_;
      /**
       * <code>required uint64 start_id = 1;</code>
       */
      public boolean hasStartId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint64 start_id = 1;</code>
       */
      public long getStartId() {
        return startId_;
      }

      public static final int UPDATED_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Long> updated_;
      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      public java.util.List<java.lang.Long>
          getUpdatedList() {
        return updated_;
      }
      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      public int getUpdatedCount() {
        return updated_.size();
      }
      /**
       * <code>repeated uint64 updated = 2;</code>
       */
      public long getUpdated(int index) {
        return updated_.get(index);
      }

      public static final int DELETED_FIELD_NUMBER = 3;
      private java.util.List<java.lang.Long> deleted_;
      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      public java.util.List<java.lang.Long>
          getDeletedList() {
        return deleted_;
      }
      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      public int getDeletedCount() {
        return deleted_.size();
      }
      /**
       * <code>repeated uint64 deleted = 3;</code>
       */
      public long getDeleted(int index) {
        return deleted_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        if (!hasStartId()) {
          memoizedIsInitialized = 0;
          return false;
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, startId_);
        }
        for (int i = 0; i < updated_.size(); i++) {
          output.writeUInt64(2, updated_.get(i));
        }
        for (int i = 0; i < deleted_.size(); i++) {
          output.writeUInt64(3, deleted_.get(i));
        }
        unknownFields.writeTo(output);
      }

      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, startId_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < updated_.size(); i++) {
            dataSize += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
              .computeUInt64SizeNoTag(updated_.get(i));
          }
          size += dataSize;
          size += 1 * getUpdatedList().size();
        }
        {
          int dataSize = 0;
          for (int i = 0; i < deleted_.size(); i++) {
            dataSize += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
              .computeUInt64SizeNoTag(deleted_.get(i));
          }
          size += dataSize;
          size += 1 * getDeletedList().size();
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode)) {
          return super.equals(obj);
        }
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode) obj;

        boolean result = true;
        result = result && (hasStartId() == other.hasStartId());
        if (hasStartId()) {
          result = result && (getStartId()
              == other.getStartId());
        }
        result = result && getUpdatedList()
            .equals(other.getUpdatedList());
        result = result && getDeletedList()
            .equals(other.getDeletedList());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (hasStartId()) {
          hash = (37 * hash) + START_ID_FIELD_NUMBER;
          hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
              getStartId());
        }
        if (getUpdatedCount() > 0) {
          hash = (37 * hash) + UPDATED_FIELD_NUMBER;
          hash = (53 * hash) + getUpdatedList().hashCode();
        }
        if (getDeletedCount() > 0) {
          hash = (37 * hash) + DELETED_FIELD_NUMBER;
          hash = (53 * hash) + getDeletedList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(byte[] data)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          byte[] data,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          java.io.InputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseDelimitedFrom(
          java.io.InputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parseFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hbase.pb.ProcedureStoreTracker.TrackerNode}
       */
      public static final class Builder extends
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:hbase.pb.ProcedureStoreTracker.TrackerNode)
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder {
        public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor;
        }

        protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder.class);
        }

        // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        public Builder clear() {
          super.clear();
          startId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          updated_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          deleted_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor;
        }

        public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getDefaultInstanceForType() {
          return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.getDefaultInstance();
        }

        public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode build() {
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode buildPartial() {
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.startId_ = startId_;
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            updated_ = java.util.Collections.unmodifiableList(updated_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.updated_ = updated_;
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            deleted_ = java.util.Collections.unmodifiableList(deleted_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.deleted_ = deleted_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder clone() {
          return (Builder) super.clone();
        }
        public Builder setField(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return (Builder) super.setField(field, value);
        }
        public Builder clearField(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        public Builder clearOneof(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        public Builder setRepeatedField(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        public Builder addRepeatedField(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode) {
            return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode other) {
          if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.getDefaultInstance()) return this;
          if (other.hasStartId()) {
            setStartId(other.getStartId());
          }
          if (!other.updated_.isEmpty()) {
            if (updated_.isEmpty()) {
              updated_ = other.updated_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureUpdatedIsMutable();
              updated_.addAll(other.updated_);
            }
            onChanged();
          }
          if (!other.deleted_.isEmpty()) {
            if (deleted_.isEmpty()) {
              deleted_ = other.deleted_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureDeletedIsMutable();
              deleted_.addAll(other.deleted_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        public final boolean isInitialized() {
          if (!hasStartId()) {
            return false;
          }
          return true;
        }

        public Builder mergeFrom(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
            org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private long startId_ ;
        /**
         * <code>required uint64 start_id = 1;</code>
         */
        public boolean hasStartId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>required uint64 start_id = 1;</code>
         */
        public long getStartId() {
          return startId_;
        }
        /**
         * <code>required uint64 start_id = 1;</code>
         */
        public Builder setStartId(long value) {
          bitField0_ |= 0x00000001;
          startId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required uint64 start_id = 1;</code>
         */
        public Builder clearStartId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          startId_ = 0L;
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Long> updated_ = java.util.Collections.emptyList();
        private void ensureUpdatedIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            updated_ = new java.util.ArrayList<java.lang.Long>(updated_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public java.util.List<java.lang.Long>
            getUpdatedList() {
          return java.util.Collections.unmodifiableList(updated_);
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public int getUpdatedCount() {
          return updated_.size();
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public long getUpdated(int index) {
          return updated_.get(index);
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public Builder setUpdated(
            int index, long value) {
          ensureUpdatedIsMutable();
          updated_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public Builder addUpdated(long value) {
          ensureUpdatedIsMutable();
          updated_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public Builder addAllUpdated(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureUpdatedIsMutable();
          org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, updated_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 updated = 2;</code>
         */
        public Builder clearUpdated() {
          updated_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Long> deleted_ = java.util.Collections.emptyList();
        private void ensureDeletedIsMutable() {
          if (!((bitField0_ & 0x00000004) == 0x00000004)) {
            deleted_ = new java.util.ArrayList<java.lang.Long>(deleted_);
            bitField0_ |= 0x00000004;
           }
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public java.util.List<java.lang.Long>
            getDeletedList() {
          return java.util.Collections.unmodifiableList(deleted_);
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public int getDeletedCount() {
          return deleted_.size();
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public long getDeleted(int index) {
          return deleted_.get(index);
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public Builder setDeleted(
            int index, long value) {
          ensureDeletedIsMutable();
          deleted_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public Builder addDeleted(long value) {
          ensureDeletedIsMutable();
          deleted_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public Builder addAllDeleted(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureDeletedIsMutable();
          org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, deleted_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deleted = 3;</code>
         */
        public Builder clearDeleted() {
          deleted_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }
        public final Builder setUnknownFields(
            final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        public final Builder mergeUnknownFields(
            final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:hbase.pb.ProcedureStoreTracker.TrackerNode)
      }

      // @@protoc_insertion_point(class_scope:hbase.pb.ProcedureStoreTracker.TrackerNode)
      private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode();
      }

      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<TrackerNode>
          PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<TrackerNode>() {
        public TrackerNode parsePartialFrom(
            org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
            org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
            return new TrackerNode(input, extensionRegistry);
        }
      };

      public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<TrackerNode> parser() {
        return PARSER;
      }

      @java.lang.Override
      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<TrackerNode> getParserForType() {
        return PARSER;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int NODE_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> node_;
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> getNodeList() {
      return node_;
    }
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder> 
        getNodeOrBuilderList() {
      return node_;
    }
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    public int getNodeCount() {
      return node_.size();
    }
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getNode(int index) {
      return node_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder getNodeOrBuilder(
        int index) {
      return node_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getNodeCount(); i++) {
        if (!getNode(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < node_.size(); i++) {
        output.writeMessage(1, node_.get(i));
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < node_.size(); i++) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, node_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker) obj;

      boolean result = true;
      result = result && getNodeList()
          .equals(other.getNodeList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getNodeCount() > 0) {
        hash = (37 * hash) + NODE_FIELD_NUMBER;
        hash = (53 * hash) + getNodeList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.ProcedureStoreTracker}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.ProcedureStoreTracker)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTrackerOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeFieldBuilder();
        }
      }
      public Builder clear() {
        super.clear();
        if (nodeBuilder_ == null) {
          node_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeBuilder_.clear();
        }
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureStoreTracker_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker(this);
        int from_bitField0_ = bitField0_;
        if (nodeBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            node_ = java.util.Collections.unmodifiableList(node_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.node_ = node_;
        } else {
          result.node_ = nodeBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.getDefaultInstance()) return this;
        if (nodeBuilder_ == null) {
          if (!other.node_.isEmpty()) {
            if (node_.isEmpty()) {
              node_ = other.node_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeIsMutable();
              node_.addAll(other.node_);
            }
            onChanged();
          }
        } else {
          if (!other.node_.isEmpty()) {
            if (nodeBuilder_.isEmpty()) {
              nodeBuilder_.dispose();
              nodeBuilder_ = null;
              node_ = other.node_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeBuilder_ = 
                org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodeFieldBuilder() : null;
            } else {
              nodeBuilder_.addAllMessages(other.node_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getNodeCount(); i++) {
          if (!getNode(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> node_ =
        java.util.Collections.emptyList();
      private void ensureNodeIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          node_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode>(node_);
          bitField0_ |= 0x00000001;
         }
      }

      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder> nodeBuilder_;

      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> getNodeList() {
        if (nodeBuilder_ == null) {
          return java.util.Collections.unmodifiableList(node_);
        } else {
          return nodeBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public int getNodeCount() {
        if (nodeBuilder_ == null) {
          return node_.size();
        } else {
          return nodeBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode getNode(int index) {
        if (nodeBuilder_ == null) {
          return node_.get(index);
        } else {
          return nodeBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder setNode(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode value) {
        if (nodeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeIsMutable();
          node_.set(index, value);
          onChanged();
        } else {
          nodeBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder setNode(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder builderForValue) {
        if (nodeBuilder_ == null) {
          ensureNodeIsMutable();
          node_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder addNode(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode value) {
        if (nodeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeIsMutable();
          node_.add(value);
          onChanged();
        } else {
          nodeBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder addNode(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode value) {
        if (nodeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeIsMutable();
          node_.add(index, value);
          onChanged();
        } else {
          nodeBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder addNode(
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder builderForValue) {
        if (nodeBuilder_ == null) {
          ensureNodeIsMutable();
          node_.add(builderForValue.build());
          onChanged();
        } else {
          nodeBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder addNode(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder builderForValue) {
        if (nodeBuilder_ == null) {
          ensureNodeIsMutable();
          node_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder addAllNode(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode> values) {
        if (nodeBuilder_ == null) {
          ensureNodeIsMutable();
          org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, node_);
          onChanged();
        } else {
          nodeBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder clearNode() {
        if (nodeBuilder_ == null) {
          node_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public Builder removeNode(int index) {
        if (nodeBuilder_ == null) {
          ensureNodeIsMutable();
          node_.remove(index);
          onChanged();
        } else {
          nodeBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder getNodeBuilder(
          int index) {
        return getNodeFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder getNodeOrBuilder(
          int index) {
        if (nodeBuilder_ == null) {
          return node_.get(index);  } else {
          return nodeBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder> 
           getNodeOrBuilderList() {
        if (nodeBuilder_ != null) {
          return nodeBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(node_);
        }
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder addNodeBuilder() {
        return getNodeFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder addNodeBuilder(
          int index) {
        return getNodeFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.ProcedureStoreTracker.TrackerNode node = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder> 
           getNodeBuilderList() {
        return getNodeFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder> 
          getNodeFieldBuilder() {
        if (nodeBuilder_ == null) {
          nodeBuilder_ = new org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNode.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker.TrackerNodeOrBuilder>(
                  node_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          node_ = null;
        }
        return nodeBuilder_;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.ProcedureStoreTracker)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.ProcedureStoreTracker)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureStoreTracker>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<ProcedureStoreTracker>() {
      public ProcedureStoreTracker parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new ProcedureStoreTracker(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureStoreTracker> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureStoreTracker> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureStoreTracker getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProcedureWALEntryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hbase.pb.ProcedureWALEntry)
      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
     */
    boolean hasType();
    /**
     * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type getType();

    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> 
        getProcedureList();
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getProcedure(int index);
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    int getProcedureCount();
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder> 
        getProcedureOrBuilderList();
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder getProcedureOrBuilder(
        int index);

    /**
     * <code>optional uint64 proc_id = 3;</code>
     */
    boolean hasProcId();
    /**
     * <code>optional uint64 proc_id = 3;</code>
     */
    long getProcId();

    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    java.util.List<java.lang.Long> getChildIdList();
    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    int getChildIdCount();
    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    long getChildId(int index);
  }
  /**
   * Protobuf type {@code hbase.pb.ProcedureWALEntry}
   */
  public  static final class ProcedureWALEntry extends
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hbase.pb.ProcedureWALEntry)
      ProcedureWALEntryOrBuilder {
    // Use ProcedureWALEntry.newBuilder() to construct.
    private ProcedureWALEntry(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProcedureWALEntry() {
      type_ = 1;
      procedure_ = java.util.Collections.emptyList();
      procId_ = 0L;
      childId_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProcedureWALEntry(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      this();
      int mutable_bitField0_ = 0;
      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type value = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                type_ = rawValue;
              }
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                procedure_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure>();
                mutable_bitField0_ |= 0x00000002;
              }
              procedure_.add(
                  input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.PARSER, extensionRegistry));
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              procId_ = input.readUInt64();
              break;
            }
            case 32: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                childId_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000008;
              }
              childId_.add(input.readUInt64());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008) && input.getBytesUntilLimit() > 0) {
                childId_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                childId_.add(input.readUInt64());
              }
              input.popLimit(limit);
              break;
            }
          }
        }
      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          procedure_ = java.util.Collections.unmodifiableList(procedure_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          childId_ = java.util.Collections.unmodifiableList(childId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALEntry_descriptor;
    }

    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALEntry_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Builder.class);
    }

    /**
     * Protobuf enum {@code hbase.pb.ProcedureWALEntry.Type}
     */
    public enum Type
        implements org.apache.hadoop.hbase.shaded.com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>PROCEDURE_WAL_EOF = 1;</code>
       */
      PROCEDURE_WAL_EOF(1),
      /**
       * <code>PROCEDURE_WAL_INIT = 2;</code>
       */
      PROCEDURE_WAL_INIT(2),
      /**
       * <code>PROCEDURE_WAL_INSERT = 3;</code>
       */
      PROCEDURE_WAL_INSERT(3),
      /**
       * <code>PROCEDURE_WAL_UPDATE = 4;</code>
       */
      PROCEDURE_WAL_UPDATE(4),
      /**
       * <code>PROCEDURE_WAL_DELETE = 5;</code>
       */
      PROCEDURE_WAL_DELETE(5),
      /**
       * <code>PROCEDURE_WAL_COMPACT = 6;</code>
       */
      PROCEDURE_WAL_COMPACT(6),
      ;

      /**
       * <code>PROCEDURE_WAL_EOF = 1;</code>
       */
      public static final int PROCEDURE_WAL_EOF_VALUE = 1;
      /**
       * <code>PROCEDURE_WAL_INIT = 2;</code>
       */
      public static final int PROCEDURE_WAL_INIT_VALUE = 2;
      /**
       * <code>PROCEDURE_WAL_INSERT = 3;</code>
       */
      public static final int PROCEDURE_WAL_INSERT_VALUE = 3;
      /**
       * <code>PROCEDURE_WAL_UPDATE = 4;</code>
       */
      public static final int PROCEDURE_WAL_UPDATE_VALUE = 4;
      /**
       * <code>PROCEDURE_WAL_DELETE = 5;</code>
       */
      public static final int PROCEDURE_WAL_DELETE_VALUE = 5;
      /**
       * <code>PROCEDURE_WAL_COMPACT = 6;</code>
       */
      public static final int PROCEDURE_WAL_COMPACT_VALUE = 6;


      public final int getNumber() {
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Type valueOf(int value) {
        return forNumber(value);
      }

      public static Type forNumber(int value) {
        switch (value) {
          case 1: return PROCEDURE_WAL_EOF;
          case 2: return PROCEDURE_WAL_INIT;
          case 3: return PROCEDURE_WAL_INSERT;
          case 4: return PROCEDURE_WAL_UPDATE;
          case 5: return PROCEDURE_WAL_DELETE;
          case 6: return PROCEDURE_WAL_COMPACT;
          default: return null;
        }
      }

      public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<Type>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<
          Type> internalValueMap =
            new org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.EnumLiteMap<Type>() {
              public Type findValueByNumber(int number) {
                return Type.forNumber(number);
              }
            };

      public final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.getDescriptor().getEnumTypes().get(0);
      }

      private static final Type[] VALUES = values();

      public static Type valueOf(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Type(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:hbase.pb.ProcedureWALEntry.Type)
    }

    private int bitField0_;
    public static final int TYPE_FIELD_NUMBER = 1;
    private int type_;
    /**
     * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
     */
    public boolean hasType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type getType() {
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type result = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type.valueOf(type_);
      return result == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type.PROCEDURE_WAL_EOF : result;
    }

    public static final int PROCEDURE_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> procedure_;
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> getProcedureList() {
      return procedure_;
    }
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder> 
        getProcedureOrBuilderList() {
      return procedure_;
    }
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    public int getProcedureCount() {
      return procedure_.size();
    }
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getProcedure(int index) {
      return procedure_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
     */
    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder getProcedureOrBuilder(
        int index) {
      return procedure_.get(index);
    }

    public static final int PROC_ID_FIELD_NUMBER = 3;
    private long procId_;
    /**
     * <code>optional uint64 proc_id = 3;</code>
     */
    public boolean hasProcId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 proc_id = 3;</code>
     */
    public long getProcId() {
      return procId_;
    }

    public static final int CHILD_ID_FIELD_NUMBER = 4;
    private java.util.List<java.lang.Long> childId_;
    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    public java.util.List<java.lang.Long>
        getChildIdList() {
      return childId_;
    }
    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    public int getChildIdCount() {
      return childId_.size();
    }
    /**
     * <code>repeated uint64 child_id = 4;</code>
     */
    public long getChildId(int index) {
      return childId_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getProcedureCount(); i++) {
        if (!getProcedure(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, type_);
      }
      for (int i = 0; i < procedure_.size(); i++) {
        output.writeMessage(2, procedure_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(3, procId_);
      }
      for (int i = 0; i < childId_.size(); i++) {
        output.writeUInt64(4, childId_.get(i));
      }
      unknownFields.writeTo(output);
    }

    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, type_);
      }
      for (int i = 0; i < procedure_.size(); i++) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, procedure_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, procId_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < childId_.size(); i++) {
          dataSize += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
            .computeUInt64SizeNoTag(childId_.get(i));
        }
        size += dataSize;
        size += 1 * getChildIdList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry) obj;

      boolean result = true;
      result = result && (hasType() == other.hasType());
      if (hasType()) {
        result = result && type_ == other.type_;
      }
      result = result && getProcedureList()
          .equals(other.getProcedureList());
      result = result && (hasProcId() == other.hasProcId());
      if (hasProcId()) {
        result = result && (getProcId()
            == other.getProcId());
      }
      result = result && getChildIdList()
          .equals(other.getChildIdList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasType()) {
        hash = (37 * hash) + TYPE_FIELD_NUMBER;
        hash = (53 * hash) + type_;
      }
      if (getProcedureCount() > 0) {
        hash = (37 * hash) + PROCEDURE_FIELD_NUMBER;
        hash = (53 * hash) + getProcedureList().hashCode();
      }
      if (hasProcId()) {
        hash = (37 * hash) + PROC_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashLong(
            getProcId());
      }
      if (getChildIdCount() > 0) {
        hash = (37 * hash) + CHILD_ID_FIELD_NUMBER;
        hash = (53 * hash) + getChildIdList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(byte[] data)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        byte[] data,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parseFrom(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.ProcedureWALEntry}
     */
    public static final class Builder extends
        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hbase.pb.ProcedureWALEntry)
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntryOrBuilder {
      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALEntry_descriptor;
      }

      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getProcedureFieldBuilder();
        }
      }
      public Builder clear() {
        super.clear();
        type_ = 1;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (procedureBuilder_ == null) {
          procedure_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          procedureBuilder_.clear();
        }
        procId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        childId_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.internal_static_hbase_pb_ProcedureWALEntry_descriptor;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry build() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry buildPartial() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.type_ = type_;
        if (procedureBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            procedure_ = java.util.Collections.unmodifiableList(procedure_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.procedure_ = procedure_;
        } else {
          result.procedure_ = procedureBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000002;
        }
        result.procId_ = procId_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          childId_ = java.util.Collections.unmodifiableList(childId_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.childId_ = childId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder clone() {
        return (Builder) super.clone();
      }
      public Builder setField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.setField(field, value);
      }
      public Builder clearField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      public Builder clearOneof(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      public Builder setRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      public Builder addRepeatedField(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry) {
          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry other) {
        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.getDefaultInstance()) return this;
        if (other.hasType()) {
          setType(other.getType());
        }
        if (procedureBuilder_ == null) {
          if (!other.procedure_.isEmpty()) {
            if (procedure_.isEmpty()) {
              procedure_ = other.procedure_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureProcedureIsMutable();
              procedure_.addAll(other.procedure_);
            }
            onChanged();
          }
        } else {
          if (!other.procedure_.isEmpty()) {
            if (procedureBuilder_.isEmpty()) {
              procedureBuilder_.dispose();
              procedureBuilder_ = null;
              procedure_ = other.procedure_;
              bitField0_ = (bitField0_ & ~0x00000002);
              procedureBuilder_ = 
                org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getProcedureFieldBuilder() : null;
            } else {
              procedureBuilder_.addAllMessages(other.procedure_);
            }
          }
        }
        if (other.hasProcId()) {
          setProcId(other.getProcId());
        }
        if (!other.childId_.isEmpty()) {
          if (childId_.isEmpty()) {
            childId_ = other.childId_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureChildIdIsMutable();
            childId_.addAll(other.childId_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      public final boolean isInitialized() {
        if (!hasType()) {
          return false;
        }
        for (int i = 0; i < getProcedureCount(); i++) {
          if (!getProcedure(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int type_ = 1;
      /**
       * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type getType() {
        org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type result = org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type.valueOf(type_);
        return result == null ? org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type.PROCEDURE_WAL_EOF : result;
      }
      /**
       * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
       */
      public Builder setType(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry.Type value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        type_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>required .hbase.pb.ProcedureWALEntry.Type type = 1;</code>
       */
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = 1;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> procedure_ =
        java.util.Collections.emptyList();
      private void ensureProcedureIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          procedure_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure>(procedure_);
          bitField0_ |= 0x00000002;
         }
      }

      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder> procedureBuilder_;

      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> getProcedureList() {
        if (procedureBuilder_ == null) {
          return java.util.Collections.unmodifiableList(procedure_);
        } else {
          return procedureBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public int getProcedureCount() {
        if (procedureBuilder_ == null) {
          return procedure_.size();
        } else {
          return procedureBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure getProcedure(int index) {
        if (procedureBuilder_ == null) {
          return procedure_.get(index);
        } else {
          return procedureBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder setProcedure(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure value) {
        if (procedureBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureProcedureIsMutable();
          procedure_.set(index, value);
          onChanged();
        } else {
          procedureBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder setProcedure(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder builderForValue) {
        if (procedureBuilder_ == null) {
          ensureProcedureIsMutable();
          procedure_.set(index, builderForValue.build());
          onChanged();
        } else {
          procedureBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder addProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure value) {
        if (procedureBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureProcedureIsMutable();
          procedure_.add(value);
          onChanged();
        } else {
          procedureBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder addProcedure(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure value) {
        if (procedureBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureProcedureIsMutable();
          procedure_.add(index, value);
          onChanged();
        } else {
          procedureBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder addProcedure(
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder builderForValue) {
        if (procedureBuilder_ == null) {
          ensureProcedureIsMutable();
          procedure_.add(builderForValue.build());
          onChanged();
        } else {
          procedureBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder addProcedure(
          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder builderForValue) {
        if (procedureBuilder_ == null) {
          ensureProcedureIsMutable();
          procedure_.add(index, builderForValue.build());
          onChanged();
        } else {
          procedureBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder addAllProcedure(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure> values) {
        if (procedureBuilder_ == null) {
          ensureProcedureIsMutable();
          org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, procedure_);
          onChanged();
        } else {
          procedureBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder clearProcedure() {
        if (procedureBuilder_ == null) {
          procedure_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          procedureBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public Builder removeProcedure(int index) {
        if (procedureBuilder_ == null) {
          ensureProcedureIsMutable();
          procedure_.remove(index);
          onChanged();
        } else {
          procedureBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder getProcedureBuilder(
          int index) {
        return getProcedureFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder getProcedureOrBuilder(
          int index) {
        if (procedureBuilder_ == null) {
          return procedure_.get(index);  } else {
          return procedureBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder> 
           getProcedureOrBuilderList() {
        if (procedureBuilder_ != null) {
          return procedureBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(procedure_);
        }
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder addProcedureBuilder() {
        return getProcedureFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder addProcedureBuilder(
          int index) {
        return getProcedureFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.Procedure procedure = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder> 
           getProcedureBuilderList() {
        return getProcedureFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder> 
          getProcedureFieldBuilder() {
        if (procedureBuilder_ == null) {
          procedureBuilder_ = new org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureOrBuilder>(
                  procedure_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          procedure_ = null;
        }
        return procedureBuilder_;
      }

      private long procId_ ;
      /**
       * <code>optional uint64 proc_id = 3;</code>
       */
      public boolean hasProcId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 proc_id = 3;</code>
       */
      public long getProcId() {
        return procId_;
      }
      /**
       * <code>optional uint64 proc_id = 3;</code>
       */
      public Builder setProcId(long value) {
        bitField0_ |= 0x00000004;
        procId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 proc_id = 3;</code>
       */
      public Builder clearProcId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        procId_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<java.lang.Long> childId_ = java.util.Collections.emptyList();
      private void ensureChildIdIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          childId_ = new java.util.ArrayList<java.lang.Long>(childId_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public java.util.List<java.lang.Long>
          getChildIdList() {
        return java.util.Collections.unmodifiableList(childId_);
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public int getChildIdCount() {
        return childId_.size();
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public long getChildId(int index) {
        return childId_.get(index);
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public Builder setChildId(
          int index, long value) {
        ensureChildIdIsMutable();
        childId_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public Builder addChildId(long value) {
        ensureChildIdIsMutable();
        childId_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public Builder addAllChildId(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureChildIdIsMutable();
        org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, childId_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 child_id = 4;</code>
       */
      public Builder clearChildId() {
        childId_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }
      public final Builder setUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      public final Builder mergeUnknownFields(
          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hbase.pb.ProcedureWALEntry)
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.ProcedureWALEntry)
    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry();
    }

    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALEntry>
        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<ProcedureWALEntry>() {
      public ProcedureWALEntry parsePartialFrom(
          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
          return new ProcedureWALEntry(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALEntry> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<ProcedureWALEntry> getParserForType() {
      return PARSER;
    }

    public org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureWALEntry getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_Procedure_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_Procedure_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_SequentialProcedureData_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_SequentialProcedureData_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_StateMachineProcedureData_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_StateMachineProcedureData_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_ProcedureWALHeader_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_ProcedureWALHeader_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_ProcedureWALTrailer_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_ProcedureWALTrailer_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_ProcedureStoreTracker_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_ProcedureStoreTracker_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_fieldAccessorTable;
  private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_ProcedureWALEntry_descriptor;
  private static final 
    org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hbase_pb_ProcedureWALEntry_fieldAccessorTable;

  public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\017Procedure.proto\022\010hbase.pb\032\023ErrorHandli" +
      "ng.proto\"\317\002\n\tProcedure\022\022\n\nclass_name\030\001 \002" +
      "(\t\022\021\n\tparent_id\030\002 \001(\004\022\017\n\007proc_id\030\003 \002(\004\022\026" +
      "\n\016submitted_time\030\004 \002(\004\022\r\n\005owner\030\005 \001(\t\022\'\n" +
      "\005state\030\006 \002(\0162\030.hbase.pb.ProcedureState\022\020" +
      "\n\010stack_id\030\007 \003(\r\022\023\n\013last_update\030\010 \002(\004\022\017\n" +
      "\007timeout\030\t \001(\r\0224\n\texception\030\n \001(\0132!.hbas" +
      "e.pb.ForeignExceptionMessage\022\016\n\006result\030\013" +
      " \001(\014\022\022\n\nstate_data\030\014 \001(\014\022\026\n\013nonce_group\030" +
      "\r \001(\004:\0010\022\020\n\005nonce\030\016 \001(\004:\0010\"+\n\027Sequential",
      "ProcedureData\022\020\n\010executed\030\001 \002(\010\"*\n\031State" +
      "MachineProcedureData\022\r\n\005state\030\001 \003(\r\"X\n\022P" +
      "rocedureWALHeader\022\017\n\007version\030\001 \002(\r\022\014\n\004ty" +
      "pe\030\002 \002(\r\022\016\n\006log_id\030\003 \002(\004\022\023\n\013min_proc_id\030" +
      "\004 \002(\004\";\n\023ProcedureWALTrailer\022\017\n\007version\030" +
      "\001 \002(\r\022\023\n\013tracker_pos\030\002 \002(\004\"\225\001\n\025Procedure" +
      "StoreTracker\0229\n\004node\030\001 \003(\0132+.hbase.pb.Pr" +
      "ocedureStoreTracker.TrackerNode\032A\n\013Track" +
      "erNode\022\020\n\010start_id\030\001 \002(\004\022\017\n\007updated\030\002 \003(" +
      "\004\022\017\n\007deleted\030\003 \003(\004\"\257\002\n\021ProcedureWALEntry",
      "\022.\n\004type\030\001 \002(\0162 .hbase.pb.ProcedureWALEn" +
      "try.Type\022&\n\tprocedure\030\002 \003(\0132\023.hbase.pb.P" +
      "rocedure\022\017\n\007proc_id\030\003 \001(\004\022\020\n\010child_id\030\004 " +
      "\003(\004\"\236\001\n\004Type\022\025\n\021PROCEDURE_WAL_EOF\020\001\022\026\n\022P" +
      "ROCEDURE_WAL_INIT\020\002\022\030\n\024PROCEDURE_WAL_INS" +
      "ERT\020\003\022\030\n\024PROCEDURE_WAL_UPDATE\020\004\022\030\n\024PROCE" +
      "DURE_WAL_DELETE\020\005\022\031\n\025PROCEDURE_WAL_COMPA" +
      "CT\020\006*{\n\016ProcedureState\022\020\n\014INITIALIZING\020\001" +
      "\022\014\n\010RUNNABLE\020\002\022\013\n\007WAITING\020\003\022\023\n\017WAITING_T" +
      "IMEOUT\020\004\022\016\n\nROLLEDBACK\020\005\022\013\n\007SUCCESS\020\006\022\n\n",
      "\006FAILED\020\007BL\n1org.apache.hadoop.hbase.sha" +
      "ded.protobuf.generatedB\017ProcedureProtosH" +
      "\001\210\001\001\240\001\001"
    };
    org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistry assignDescriptors(
              org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.getDescriptor(),
        }, assigner);
    internal_static_hbase_pb_Procedure_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_hbase_pb_Procedure_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_Procedure_descriptor,
        new java.lang.String[] { "ClassName", "ParentId", "ProcId", "SubmittedTime", "Owner", "State", "StackId", "LastUpdate", "Timeout", "Exception", "Result", "StateData", "NonceGroup", "Nonce", });
    internal_static_hbase_pb_SequentialProcedureData_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_hbase_pb_SequentialProcedureData_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_SequentialProcedureData_descriptor,
        new java.lang.String[] { "Executed", });
    internal_static_hbase_pb_StateMachineProcedureData_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_hbase_pb_StateMachineProcedureData_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_StateMachineProcedureData_descriptor,
        new java.lang.String[] { "State", });
    internal_static_hbase_pb_ProcedureWALHeader_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_hbase_pb_ProcedureWALHeader_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_ProcedureWALHeader_descriptor,
        new java.lang.String[] { "Version", "Type", "LogId", "MinProcId", });
    internal_static_hbase_pb_ProcedureWALTrailer_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_hbase_pb_ProcedureWALTrailer_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_ProcedureWALTrailer_descriptor,
        new java.lang.String[] { "Version", "TrackerPos", });
    internal_static_hbase_pb_ProcedureStoreTracker_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_hbase_pb_ProcedureStoreTracker_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_ProcedureStoreTracker_descriptor,
        new java.lang.String[] { "Node", });
    internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor =
      internal_static_hbase_pb_ProcedureStoreTracker_descriptor.getNestedTypes().get(0);
    internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_ProcedureStoreTracker_TrackerNode_descriptor,
        new java.lang.String[] { "StartId", "Updated", "Deleted", });
    internal_static_hbase_pb_ProcedureWALEntry_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_hbase_pb_ProcedureWALEntry_fieldAccessorTable = new
      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hbase_pb_ProcedureWALEntry_descriptor,
        new java.lang.String[] { "Type", "Procedure", "ProcId", "ChildId", });
    org.apache.hadoop.hbase.shaded.protobuf.generated.ErrorHandlingProtos.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}

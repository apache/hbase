// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: WAL.proto

package org.apache.hadoop.hbase.protobuf.generated;

public final class WALProtos {
  private WALProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code hbase.pb.ScopeType}
   */
  public enum ScopeType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>REPLICATION_SCOPE_LOCAL = 0;</code>
     */
    REPLICATION_SCOPE_LOCAL(0, 0),
    /**
     * <code>REPLICATION_SCOPE_GLOBAL = 1;</code>
     */
    REPLICATION_SCOPE_GLOBAL(1, 1),
    ;

    /**
     * <code>REPLICATION_SCOPE_LOCAL = 0;</code>
     */
    public static final int REPLICATION_SCOPE_LOCAL_VALUE = 0;
    /**
     * <code>REPLICATION_SCOPE_GLOBAL = 1;</code>
     */
    public static final int REPLICATION_SCOPE_GLOBAL_VALUE = 1;


    public final int getNumber() { return value; }

    public static ScopeType valueOf(int value) {
      switch (value) {
        case 0: return REPLICATION_SCOPE_LOCAL;
        case 1: return REPLICATION_SCOPE_GLOBAL;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ScopeType>() {
            public ScopeType findValueByNumber(int number) {
              return ScopeType.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final ScopeType[] VALUES = values();

    public static ScopeType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private ScopeType(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hbase.pb.ScopeType)
  }

  public interface WALHeaderOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool has_compression = 1;
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    boolean hasHasCompression();
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    boolean getHasCompression();

    // optional bytes encryption_key = 2;
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    boolean hasEncryptionKey();
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    com.google.protobuf.ByteString getEncryptionKey();

    // optional bool has_tag_compression = 3;
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    boolean hasHasTagCompression();
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    boolean getHasTagCompression();

    // optional string writer_cls_name = 4;
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    boolean hasWriterClsName();
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    java.lang.String getWriterClsName();
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    com.google.protobuf.ByteString
        getWriterClsNameBytes();

    // optional string cell_codec_cls_name = 5;
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    boolean hasCellCodecClsName();
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    java.lang.String getCellCodecClsName();
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    com.google.protobuf.ByteString
        getCellCodecClsNameBytes();
  }
  /**
   * Protobuf type {@code hbase.pb.WALHeader}
   */
  public static final class WALHeader extends
      com.google.protobuf.GeneratedMessage
      implements WALHeaderOrBuilder {
    // Use WALHeader.newBuilder() to construct.
    private WALHeader(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALHeader(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALHeader defaultInstance;
    public static WALHeader getDefaultInstance() {
      return defaultInstance;
    }

    public WALHeader getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALHeader(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              hasCompression_ = input.readBool();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              encryptionKey_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              hasTagCompression_ = input.readBool();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              writerClsName_ = input.readBytes();
              break;
            }
            case 42: {
              bitField0_ |= 0x00000010;
              cellCodecClsName_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALHeader_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALHeader_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.Builder.class);
    }

    public static com.google.protobuf.Parser<WALHeader> PARSER =
        new com.google.protobuf.AbstractParser<WALHeader>() {
      public WALHeader parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALHeader(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALHeader> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool has_compression = 1;
    public static final int HAS_COMPRESSION_FIELD_NUMBER = 1;
    private boolean hasCompression_;
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    public boolean hasHasCompression() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    public boolean getHasCompression() {
      return hasCompression_;
    }

    // optional bytes encryption_key = 2;
    public static final int ENCRYPTION_KEY_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString encryptionKey_;
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    public boolean hasEncryptionKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    public com.google.protobuf.ByteString getEncryptionKey() {
      return encryptionKey_;
    }

    // optional bool has_tag_compression = 3;
    public static final int HAS_TAG_COMPRESSION_FIELD_NUMBER = 3;
    private boolean hasTagCompression_;
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    public boolean hasHasTagCompression() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    public boolean getHasTagCompression() {
      return hasTagCompression_;
    }

    // optional string writer_cls_name = 4;
    public static final int WRITER_CLS_NAME_FIELD_NUMBER = 4;
    private java.lang.Object writerClsName_;
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    public boolean hasWriterClsName() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    public java.lang.String getWriterClsName() {
      java.lang.Object ref = writerClsName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          writerClsName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string writer_cls_name = 4;</code>
     */
    public com.google.protobuf.ByteString
        getWriterClsNameBytes() {
      java.lang.Object ref = writerClsName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        writerClsName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string cell_codec_cls_name = 5;
    public static final int CELL_CODEC_CLS_NAME_FIELD_NUMBER = 5;
    private java.lang.Object cellCodecClsName_;
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    public boolean hasCellCodecClsName() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    public java.lang.String getCellCodecClsName() {
      java.lang.Object ref = cellCodecClsName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          cellCodecClsName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string cell_codec_cls_name = 5;</code>
     */
    public com.google.protobuf.ByteString
        getCellCodecClsNameBytes() {
      java.lang.Object ref = cellCodecClsName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        cellCodecClsName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      hasCompression_ = false;
      encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
      hasTagCompression_ = false;
      writerClsName_ = "";
      cellCodecClsName_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, hasCompression_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, encryptionKey_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBool(3, hasTagCompression_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getWriterClsNameBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(5, getCellCodecClsNameBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, hasCompression_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, encryptionKey_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, hasTagCompression_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getWriterClsNameBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getCellCodecClsNameBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) obj;

      boolean result = true;
      result = result && (hasHasCompression() == other.hasHasCompression());
      if (hasHasCompression()) {
        result = result && (getHasCompression()
            == other.getHasCompression());
      }
      result = result && (hasEncryptionKey() == other.hasEncryptionKey());
      if (hasEncryptionKey()) {
        result = result && getEncryptionKey()
            .equals(other.getEncryptionKey());
      }
      result = result && (hasHasTagCompression() == other.hasHasTagCompression());
      if (hasHasTagCompression()) {
        result = result && (getHasTagCompression()
            == other.getHasTagCompression());
      }
      result = result && (hasWriterClsName() == other.hasWriterClsName());
      if (hasWriterClsName()) {
        result = result && getWriterClsName()
            .equals(other.getWriterClsName());
      }
      result = result && (hasCellCodecClsName() == other.hasCellCodecClsName());
      if (hasCellCodecClsName()) {
        result = result && getCellCodecClsName()
            .equals(other.getCellCodecClsName());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHasCompression()) {
        hash = (37 * hash) + HAS_COMPRESSION_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getHasCompression());
      }
      if (hasEncryptionKey()) {
        hash = (37 * hash) + ENCRYPTION_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getEncryptionKey().hashCode();
      }
      if (hasHasTagCompression()) {
        hash = (37 * hash) + HAS_TAG_COMPRESSION_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getHasTagCompression());
      }
      if (hasWriterClsName()) {
        hash = (37 * hash) + WRITER_CLS_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getWriterClsName().hashCode();
      }
      if (hasCellCodecClsName()) {
        hash = (37 * hash) + CELL_CODEC_CLS_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getCellCodecClsName().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.WALHeader}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeaderOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALHeader_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALHeader_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        hasCompression_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        hasTagCompression_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        writerClsName_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        cellCodecClsName_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALHeader_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.hasCompression_ = hasCompression_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.encryptionKey_ = encryptionKey_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.hasTagCompression_ = hasTagCompression_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.writerClsName_ = writerClsName_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.cellCodecClsName_ = cellCodecClsName_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.getDefaultInstance()) return this;
        if (other.hasHasCompression()) {
          setHasCompression(other.getHasCompression());
        }
        if (other.hasEncryptionKey()) {
          setEncryptionKey(other.getEncryptionKey());
        }
        if (other.hasHasTagCompression()) {
          setHasTagCompression(other.getHasTagCompression());
        }
        if (other.hasWriterClsName()) {
          bitField0_ |= 0x00000008;
          writerClsName_ = other.writerClsName_;
          onChanged();
        }
        if (other.hasCellCodecClsName()) {
          bitField0_ |= 0x00000010;
          cellCodecClsName_ = other.cellCodecClsName_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool has_compression = 1;
      private boolean hasCompression_ ;
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public boolean hasHasCompression() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public boolean getHasCompression() {
        return hasCompression_;
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public Builder setHasCompression(boolean value) {
        bitField0_ |= 0x00000001;
        hasCompression_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public Builder clearHasCompression() {
        bitField0_ = (bitField0_ & ~0x00000001);
        hasCompression_ = false;
        onChanged();
        return this;
      }

      // optional bytes encryption_key = 2;
      private com.google.protobuf.ByteString encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public boolean hasEncryptionKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public com.google.protobuf.ByteString getEncryptionKey() {
        return encryptionKey_;
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public Builder setEncryptionKey(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        encryptionKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public Builder clearEncryptionKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        encryptionKey_ = getDefaultInstance().getEncryptionKey();
        onChanged();
        return this;
      }

      // optional bool has_tag_compression = 3;
      private boolean hasTagCompression_ ;
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public boolean hasHasTagCompression() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public boolean getHasTagCompression() {
        return hasTagCompression_;
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public Builder setHasTagCompression(boolean value) {
        bitField0_ |= 0x00000004;
        hasTagCompression_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public Builder clearHasTagCompression() {
        bitField0_ = (bitField0_ & ~0x00000004);
        hasTagCompression_ = false;
        onChanged();
        return this;
      }

      // optional string writer_cls_name = 4;
      private java.lang.Object writerClsName_ = "";
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public boolean hasWriterClsName() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public java.lang.String getWriterClsName() {
        java.lang.Object ref = writerClsName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          writerClsName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public com.google.protobuf.ByteString
          getWriterClsNameBytes() {
        java.lang.Object ref = writerClsName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          writerClsName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public Builder setWriterClsName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        writerClsName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public Builder clearWriterClsName() {
        bitField0_ = (bitField0_ & ~0x00000008);
        writerClsName_ = getDefaultInstance().getWriterClsName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string writer_cls_name = 4;</code>
       */
      public Builder setWriterClsNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        writerClsName_ = value;
        onChanged();
        return this;
      }

      // optional string cell_codec_cls_name = 5;
      private java.lang.Object cellCodecClsName_ = "";
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public boolean hasCellCodecClsName() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public java.lang.String getCellCodecClsName() {
        java.lang.Object ref = cellCodecClsName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          cellCodecClsName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public com.google.protobuf.ByteString
          getCellCodecClsNameBytes() {
        java.lang.Object ref = cellCodecClsName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          cellCodecClsName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public Builder setCellCodecClsName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        cellCodecClsName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public Builder clearCellCodecClsName() {
        bitField0_ = (bitField0_ & ~0x00000010);
        cellCodecClsName_ = getDefaultInstance().getCellCodecClsName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string cell_codec_cls_name = 5;</code>
       */
      public Builder setCellCodecClsNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        cellCodecClsName_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.WALHeader)
    }

    static {
      defaultInstance = new WALHeader(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.WALHeader)
  }

  public interface WALKeyOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes encoded_region_name = 1;
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // required bytes table_name = 2;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 2;</code>
     */
    com.google.protobuf.ByteString getTableName();

    // required uint64 log_sequence_number = 3;
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    boolean hasLogSequenceNumber();
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    long getLogSequenceNumber();

    // required uint64 write_time = 4;
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    boolean hasWriteTime();
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    long getWriteTime();

    // optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated boolean hasClusterId();
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId();
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder();

    // repeated .hbase.pb.FamilyScope scopes = 6;
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> 
        getScopesList();
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index);
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    int getScopesCount();
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
        getScopesOrBuilderList();
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
        int index);

    // optional uint32 following_kv_count = 7;
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    boolean hasFollowingKvCount();
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    int getFollowingKvCount();

    // repeated .hbase.pb.UUID cluster_ids = 8;
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> 
        getClusterIdsList();
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index);
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    int getClusterIdsCount();
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
        getClusterIdsOrBuilderList();
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
        int index);

    // optional uint64 nonceGroup = 9;
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    boolean hasNonceGroup();
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    long getNonceGroup();

    // optional uint64 nonce = 10;
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    boolean hasNonce();
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    long getNonce();

    // optional uint64 orig_sequence_number = 11;
    /**
     * <code>optional uint64 orig_sequence_number = 11;</code>
     */
    boolean hasOrigSequenceNumber();
    /**
     * <code>optional uint64 orig_sequence_number = 11;</code>
     */
    long getOrigSequenceNumber();

    // repeated .hbase.pb.Attribute extended_attributes = 12;
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> 
        getExtendedAttributesList();
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute getExtendedAttributes(int index);
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    int getExtendedAttributesCount();
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder> 
        getExtendedAttributesOrBuilderList();
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder getExtendedAttributesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hbase.pb.WALKey}
   *
   * <pre>
   *
   * Protocol buffer version of WALKey; see WALKey comment, not really a key but WALEdit header
   * for some KVs
   * </pre>
   */
  public static final class WALKey extends
      com.google.protobuf.GeneratedMessage
      implements WALKeyOrBuilder {
    // Use WALKey.newBuilder() to construct.
    private WALKey(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALKey(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALKey defaultInstance;
    public static WALKey getDefaultInstance() {
      return defaultInstance;
    }

    public WALKey getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALKey(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              tableName_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              logSequenceNumber_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              writeTime_ = input.readUInt64();
              break;
            }
            case 42: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = clusterId_.toBuilder();
              }
              clusterId_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(clusterId_);
                clusterId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                scopes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope>();
                mutable_bitField0_ |= 0x00000020;
              }
              scopes_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000020;
              followingKvCount_ = input.readUInt32();
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
                clusterIds_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID>();
                mutable_bitField0_ |= 0x00000080;
              }
              clusterIds_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.PARSER, extensionRegistry));
              break;
            }
            case 72: {
              bitField0_ |= 0x00000040;
              nonceGroup_ = input.readUInt64();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000080;
              nonce_ = input.readUInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000100;
              origSequenceNumber_ = input.readUInt64();
              break;
            }
            case 98: {
              if (!((mutable_bitField0_ & 0x00000800) == 0x00000800)) {
                extendedAttributes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute>();
                mutable_bitField0_ |= 0x00000800;
              }
              extendedAttributes_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          scopes_ = java.util.Collections.unmodifiableList(scopes_);
        }
        if (((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
          clusterIds_ = java.util.Collections.unmodifiableList(clusterIds_);
        }
        if (((mutable_bitField0_ & 0x00000800) == 0x00000800)) {
          extendedAttributes_ = java.util.Collections.unmodifiableList(extendedAttributes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALKey_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALKey_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.Builder.class);
    }

    public static com.google.protobuf.Parser<WALKey> PARSER =
        new com.google.protobuf.AbstractParser<WALKey>() {
      public WALKey parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALKey(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALKey> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes encoded_region_name = 1;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // required bytes table_name = 2;
    public static final int TABLE_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required uint64 log_sequence_number = 3;
    public static final int LOG_SEQUENCE_NUMBER_FIELD_NUMBER = 3;
    private long logSequenceNumber_;
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    public boolean hasLogSequenceNumber() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    public long getLogSequenceNumber() {
      return logSequenceNumber_;
    }

    // required uint64 write_time = 4;
    public static final int WRITE_TIME_FIELD_NUMBER = 4;
    private long writeTime_;
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    public boolean hasWriteTime() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    public long getWriteTime() {
      return writeTime_;
    }

    // optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];
    public static final int CLUSTER_ID_FIELD_NUMBER = 5;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_;
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public boolean hasClusterId() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
      return clusterId_;
    }
    /**
     * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94)
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
      return clusterId_;
    }

    // repeated .hbase.pb.FamilyScope scopes = 6;
    public static final int SCOPES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> scopes_;
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> getScopesList() {
      return scopes_;
    }
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
        getScopesOrBuilderList() {
      return scopes_;
    }
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    public int getScopesCount() {
      return scopes_.size();
    }
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index) {
      return scopes_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
        int index) {
      return scopes_.get(index);
    }

    // optional uint32 following_kv_count = 7;
    public static final int FOLLOWING_KV_COUNT_FIELD_NUMBER = 7;
    private int followingKvCount_;
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    public boolean hasFollowingKvCount() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    public int getFollowingKvCount() {
      return followingKvCount_;
    }

    // repeated .hbase.pb.UUID cluster_ids = 8;
    public static final int CLUSTER_IDS_FIELD_NUMBER = 8;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> clusterIds_;
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> getClusterIdsList() {
      return clusterIds_;
    }
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
        getClusterIdsOrBuilderList() {
      return clusterIds_;
    }
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public int getClusterIdsCount() {
      return clusterIds_.size();
    }
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index) {
      return clusterIds_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
        int index) {
      return clusterIds_.get(index);
    }

    // optional uint64 nonceGroup = 9;
    public static final int NONCEGROUP_FIELD_NUMBER = 9;
    private long nonceGroup_;
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    public boolean hasNonceGroup() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    public long getNonceGroup() {
      return nonceGroup_;
    }

    // optional uint64 nonce = 10;
    public static final int NONCE_FIELD_NUMBER = 10;
    private long nonce_;
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    public boolean hasNonce() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    public long getNonce() {
      return nonce_;
    }

    // optional uint64 orig_sequence_number = 11;
    public static final int ORIG_SEQUENCE_NUMBER_FIELD_NUMBER = 11;
    private long origSequenceNumber_;
    /**
     * <code>optional uint64 orig_sequence_number = 11;</code>
     */
    public boolean hasOrigSequenceNumber() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional uint64 orig_sequence_number = 11;</code>
     */
    public long getOrigSequenceNumber() {
      return origSequenceNumber_;
    }

    // repeated .hbase.pb.Attribute extended_attributes = 12;
    public static final int EXTENDED_ATTRIBUTES_FIELD_NUMBER = 12;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> extendedAttributes_;
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> getExtendedAttributesList() {
      return extendedAttributes_;
    }
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder> 
        getExtendedAttributesOrBuilderList() {
      return extendedAttributes_;
    }
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    public int getExtendedAttributesCount() {
      return extendedAttributes_.size();
    }
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute getExtendedAttributes(int index) {
      return extendedAttributes_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder getExtendedAttributesOrBuilder(
        int index) {
      return extendedAttributes_.get(index);
    }

    private void initFields() {
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      logSequenceNumber_ = 0L;
      writeTime_ = 0L;
      clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
      scopes_ = java.util.Collections.emptyList();
      followingKvCount_ = 0;
      clusterIds_ = java.util.Collections.emptyList();
      nonceGroup_ = 0L;
      nonce_ = 0L;
      origSequenceNumber_ = 0L;
      extendedAttributes_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLogSequenceNumber()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasWriteTime()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasClusterId()) {
        if (!getClusterId().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getScopesCount(); i++) {
        if (!getScopes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getClusterIdsCount(); i++) {
        if (!getClusterIds(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getExtendedAttributesCount(); i++) {
        if (!getExtendedAttributes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, logSequenceNumber_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, writeTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(5, clusterId_);
      }
      for (int i = 0; i < scopes_.size(); i++) {
        output.writeMessage(6, scopes_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt32(7, followingKvCount_);
      }
      for (int i = 0; i < clusterIds_.size(); i++) {
        output.writeMessage(8, clusterIds_.get(i));
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(9, nonceGroup_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(10, nonce_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeUInt64(11, origSequenceNumber_);
      }
      for (int i = 0; i < extendedAttributes_.size(); i++) {
        output.writeMessage(12, extendedAttributes_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, logSequenceNumber_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, writeTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, clusterId_);
      }
      for (int i = 0; i < scopes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, scopes_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(7, followingKvCount_);
      }
      for (int i = 0; i < clusterIds_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, clusterIds_.get(i));
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(9, nonceGroup_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(10, nonce_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(11, origSequenceNumber_);
      }
      for (int i = 0; i < extendedAttributes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, extendedAttributes_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) obj;

      boolean result = true;
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasLogSequenceNumber() == other.hasLogSequenceNumber());
      if (hasLogSequenceNumber()) {
        result = result && (getLogSequenceNumber()
            == other.getLogSequenceNumber());
      }
      result = result && (hasWriteTime() == other.hasWriteTime());
      if (hasWriteTime()) {
        result = result && (getWriteTime()
            == other.getWriteTime());
      }
      result = result && (hasClusterId() == other.hasClusterId());
      if (hasClusterId()) {
        result = result && getClusterId()
            .equals(other.getClusterId());
      }
      result = result && getScopesList()
          .equals(other.getScopesList());
      result = result && (hasFollowingKvCount() == other.hasFollowingKvCount());
      if (hasFollowingKvCount()) {
        result = result && (getFollowingKvCount()
            == other.getFollowingKvCount());
      }
      result = result && getClusterIdsList()
          .equals(other.getClusterIdsList());
      result = result && (hasNonceGroup() == other.hasNonceGroup());
      if (hasNonceGroup()) {
        result = result && (getNonceGroup()
            == other.getNonceGroup());
      }
      result = result && (hasNonce() == other.hasNonce());
      if (hasNonce()) {
        result = result && (getNonce()
            == other.getNonce());
      }
      result = result && (hasOrigSequenceNumber() == other.hasOrigSequenceNumber());
      if (hasOrigSequenceNumber()) {
        result = result && (getOrigSequenceNumber()
            == other.getOrigSequenceNumber());
      }
      result = result && getExtendedAttributesList()
          .equals(other.getExtendedAttributesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasLogSequenceNumber()) {
        hash = (37 * hash) + LOG_SEQUENCE_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLogSequenceNumber());
      }
      if (hasWriteTime()) {
        hash = (37 * hash) + WRITE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteTime());
      }
      if (hasClusterId()) {
        hash = (37 * hash) + CLUSTER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getClusterId().hashCode();
      }
      if (getScopesCount() > 0) {
        hash = (37 * hash) + SCOPES_FIELD_NUMBER;
        hash = (53 * hash) + getScopesList().hashCode();
      }
      if (hasFollowingKvCount()) {
        hash = (37 * hash) + FOLLOWING_KV_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + getFollowingKvCount();
      }
      if (getClusterIdsCount() > 0) {
        hash = (37 * hash) + CLUSTER_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getClusterIdsList().hashCode();
      }
      if (hasNonceGroup()) {
        hash = (37 * hash) + NONCEGROUP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNonceGroup());
      }
      if (hasNonce()) {
        hash = (37 * hash) + NONCE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNonce());
      }
      if (hasOrigSequenceNumber()) {
        hash = (37 * hash) + ORIG_SEQUENCE_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getOrigSequenceNumber());
      }
      if (getExtendedAttributesCount() > 0) {
        hash = (37 * hash) + EXTENDED_ATTRIBUTES_FIELD_NUMBER;
        hash = (53 * hash) + getExtendedAttributesList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.WALKey}
     *
     * <pre>
     *
     * Protocol buffer version of WALKey; see WALKey comment, not really a key but WALEdit header
     * for some KVs
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKeyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALKey_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALKey_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getClusterIdFieldBuilder();
          getScopesFieldBuilder();
          getClusterIdsFieldBuilder();
          getExtendedAttributesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        logSequenceNumber_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        writeTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (scopesBuilder_ == null) {
          scopes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          scopesBuilder_.clear();
        }
        followingKvCount_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (clusterIdsBuilder_ == null) {
          clusterIds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
        } else {
          clusterIdsBuilder_.clear();
        }
        nonceGroup_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        nonce_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        origSequenceNumber_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000400);
        if (extendedAttributesBuilder_ == null) {
          extendedAttributes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
        } else {
          extendedAttributesBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALKey_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.logSequenceNumber_ = logSequenceNumber_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.writeTime_ = writeTime_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        if (clusterIdBuilder_ == null) {
          result.clusterId_ = clusterId_;
        } else {
          result.clusterId_ = clusterIdBuilder_.build();
        }
        if (scopesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            scopes_ = java.util.Collections.unmodifiableList(scopes_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.scopes_ = scopes_;
        } else {
          result.scopes_ = scopesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        result.followingKvCount_ = followingKvCount_;
        if (clusterIdsBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080)) {
            clusterIds_ = java.util.Collections.unmodifiableList(clusterIds_);
            bitField0_ = (bitField0_ & ~0x00000080);
          }
          result.clusterIds_ = clusterIds_;
        } else {
          result.clusterIds_ = clusterIdsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000040;
        }
        result.nonceGroup_ = nonceGroup_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000080;
        }
        result.nonce_ = nonce_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000100;
        }
        result.origSequenceNumber_ = origSequenceNumber_;
        if (extendedAttributesBuilder_ == null) {
          if (((bitField0_ & 0x00000800) == 0x00000800)) {
            extendedAttributes_ = java.util.Collections.unmodifiableList(extendedAttributes_);
            bitField0_ = (bitField0_ & ~0x00000800);
          }
          result.extendedAttributes_ = extendedAttributes_;
        } else {
          result.extendedAttributes_ = extendedAttributesBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.getDefaultInstance()) return this;
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasLogSequenceNumber()) {
          setLogSequenceNumber(other.getLogSequenceNumber());
        }
        if (other.hasWriteTime()) {
          setWriteTime(other.getWriteTime());
        }
        if (other.hasClusterId()) {
          mergeClusterId(other.getClusterId());
        }
        if (scopesBuilder_ == null) {
          if (!other.scopes_.isEmpty()) {
            if (scopes_.isEmpty()) {
              scopes_ = other.scopes_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureScopesIsMutable();
              scopes_.addAll(other.scopes_);
            }
            onChanged();
          }
        } else {
          if (!other.scopes_.isEmpty()) {
            if (scopesBuilder_.isEmpty()) {
              scopesBuilder_.dispose();
              scopesBuilder_ = null;
              scopes_ = other.scopes_;
              bitField0_ = (bitField0_ & ~0x00000020);
              scopesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getScopesFieldBuilder() : null;
            } else {
              scopesBuilder_.addAllMessages(other.scopes_);
            }
          }
        }
        if (other.hasFollowingKvCount()) {
          setFollowingKvCount(other.getFollowingKvCount());
        }
        if (clusterIdsBuilder_ == null) {
          if (!other.clusterIds_.isEmpty()) {
            if (clusterIds_.isEmpty()) {
              clusterIds_ = other.clusterIds_;
              bitField0_ = (bitField0_ & ~0x00000080);
            } else {
              ensureClusterIdsIsMutable();
              clusterIds_.addAll(other.clusterIds_);
            }
            onChanged();
          }
        } else {
          if (!other.clusterIds_.isEmpty()) {
            if (clusterIdsBuilder_.isEmpty()) {
              clusterIdsBuilder_.dispose();
              clusterIdsBuilder_ = null;
              clusterIds_ = other.clusterIds_;
              bitField0_ = (bitField0_ & ~0x00000080);
              clusterIdsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getClusterIdsFieldBuilder() : null;
            } else {
              clusterIdsBuilder_.addAllMessages(other.clusterIds_);
            }
          }
        }
        if (other.hasNonceGroup()) {
          setNonceGroup(other.getNonceGroup());
        }
        if (other.hasNonce()) {
          setNonce(other.getNonce());
        }
        if (other.hasOrigSequenceNumber()) {
          setOrigSequenceNumber(other.getOrigSequenceNumber());
        }
        if (extendedAttributesBuilder_ == null) {
          if (!other.extendedAttributes_.isEmpty()) {
            if (extendedAttributes_.isEmpty()) {
              extendedAttributes_ = other.extendedAttributes_;
              bitField0_ = (bitField0_ & ~0x00000800);
            } else {
              ensureExtendedAttributesIsMutable();
              extendedAttributes_.addAll(other.extendedAttributes_);
            }
            onChanged();
          }
        } else {
          if (!other.extendedAttributes_.isEmpty()) {
            if (extendedAttributesBuilder_.isEmpty()) {
              extendedAttributesBuilder_.dispose();
              extendedAttributesBuilder_ = null;
              extendedAttributes_ = other.extendedAttributes_;
              bitField0_ = (bitField0_ & ~0x00000800);
              extendedAttributesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getExtendedAttributesFieldBuilder() : null;
            } else {
              extendedAttributesBuilder_.addAllMessages(other.extendedAttributes_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasLogSequenceNumber()) {
          
          return false;
        }
        if (!hasWriteTime()) {
          
          return false;
        }
        if (hasClusterId()) {
          if (!getClusterId().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getScopesCount(); i++) {
          if (!getScopes(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getClusterIdsCount(); i++) {
          if (!getClusterIds(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getExtendedAttributesCount(); i++) {
          if (!getExtendedAttributes(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes encoded_region_name = 1;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // required bytes table_name = 2;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required uint64 log_sequence_number = 3;
      private long logSequenceNumber_ ;
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public boolean hasLogSequenceNumber() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public long getLogSequenceNumber() {
        return logSequenceNumber_;
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public Builder setLogSequenceNumber(long value) {
        bitField0_ |= 0x00000004;
        logSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public Builder clearLogSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000004);
        logSequenceNumber_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 write_time = 4;
      private long writeTime_ ;
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public boolean hasWriteTime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public long getWriteTime() {
        return writeTime_;
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public Builder setWriteTime(long value) {
        bitField0_ |= 0x00000008;
        writeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public Builder clearWriteTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        writeTime_ = 0L;
        onChanged();
        return this;
      }

      // optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> clusterIdBuilder_;
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public boolean hasClusterId() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
        if (clusterIdBuilder_ == null) {
          return clusterId_;
        } else {
          return clusterIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder setClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          clusterId_ = value;
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder setClusterId(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdBuilder_ == null) {
          clusterId_ = builderForValue.build();
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder mergeClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              clusterId_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance()) {
            clusterId_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.newBuilder(clusterId_).mergeFrom(value).buildPartial();
          } else {
            clusterId_ = value;
          }
          onChanged();
        } else {
          clusterIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder clearClusterId() {
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
          onChanged();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder getClusterIdBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getClusterIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
        if (clusterIdBuilder_ != null) {
          return clusterIdBuilder_.getMessageOrBuilder();
        } else {
          return clusterId_;
        }
      }
      /**
       * <code>optional .hbase.pb.UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94)
       *can be read by the newer releases.
       * </pre>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
          getClusterIdFieldBuilder() {
        if (clusterIdBuilder_ == null) {
          clusterIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder>(
                  clusterId_,
                  getParentForChildren(),
                  isClean());
          clusterId_ = null;
        }
        return clusterIdBuilder_;
      }

      // repeated .hbase.pb.FamilyScope scopes = 6;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> scopes_ =
        java.util.Collections.emptyList();
      private void ensureScopesIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          scopes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope>(scopes_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> scopesBuilder_;

      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> getScopesList() {
        if (scopesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(scopes_);
        } else {
          return scopesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public int getScopesCount() {
        if (scopesBuilder_ == null) {
          return scopes_.size();
        } else {
          return scopesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index) {
        if (scopesBuilder_ == null) {
          return scopes_.get(index);
        } else {
          return scopesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder setScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.set(index, value);
          onChanged();
        } else {
          scopesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder setScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.set(index, builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.add(value);
          onChanged();
        } else {
          scopesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.add(index, value);
          onChanged();
        } else {
          scopesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.add(builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.add(index, builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder addAllScopes(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> values) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          super.addAll(values, scopes_);
          onChanged();
        } else {
          scopesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder clearScopes() {
        if (scopesBuilder_ == null) {
          scopes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          scopesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public Builder removeScopes(int index) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.remove(index);
          onChanged();
        } else {
          scopesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder getScopesBuilder(
          int index) {
        return getScopesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
          int index) {
        if (scopesBuilder_ == null) {
          return scopes_.get(index);  } else {
          return scopesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
           getScopesOrBuilderList() {
        if (scopesBuilder_ != null) {
          return scopesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(scopes_);
        }
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder addScopesBuilder() {
        return getScopesFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder addScopesBuilder(
          int index) {
        return getScopesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.FamilyScope scopes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder> 
           getScopesBuilderList() {
        return getScopesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
          getScopesFieldBuilder() {
        if (scopesBuilder_ == null) {
          scopesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder>(
                  scopes_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          scopes_ = null;
        }
        return scopesBuilder_;
      }

      // optional uint32 following_kv_count = 7;
      private int followingKvCount_ ;
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public boolean hasFollowingKvCount() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public int getFollowingKvCount() {
        return followingKvCount_;
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public Builder setFollowingKvCount(int value) {
        bitField0_ |= 0x00000040;
        followingKvCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public Builder clearFollowingKvCount() {
        bitField0_ = (bitField0_ & ~0x00000040);
        followingKvCount_ = 0;
        onChanged();
        return this;
      }

      // repeated .hbase.pb.UUID cluster_ids = 8;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> clusterIds_ =
        java.util.Collections.emptyList();
      private void ensureClusterIdsIsMutable() {
        if (!((bitField0_ & 0x00000080) == 0x00000080)) {
          clusterIds_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID>(clusterIds_);
          bitField0_ |= 0x00000080;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> clusterIdsBuilder_;

      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> getClusterIdsList() {
        if (clusterIdsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(clusterIds_);
        } else {
          return clusterIdsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public int getClusterIdsCount() {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.size();
        } else {
          return clusterIdsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index) {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.get(index);
        } else {
          return clusterIdsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder setClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.set(index, value);
          onChanged();
        } else {
          clusterIdsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder setClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.set(index, builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.add(value);
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.add(index, value);
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.add(builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.add(index, builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addAllClusterIds(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> values) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          super.addAll(values, clusterIds_);
          onChanged();
        } else {
          clusterIdsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder clearClusterIds() {
        if (clusterIdsBuilder_ == null) {
          clusterIds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          onChanged();
        } else {
          clusterIdsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder removeClusterIds(int index) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.remove(index);
          onChanged();
        } else {
          clusterIdsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder getClusterIdsBuilder(
          int index) {
        return getClusterIdsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
          int index) {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.get(index);  } else {
          return clusterIdsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
           getClusterIdsOrBuilderList() {
        if (clusterIdsBuilder_ != null) {
          return clusterIdsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(clusterIds_);
        }
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder addClusterIdsBuilder() {
        return getClusterIdsFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder addClusterIdsBuilder(
          int index) {
        return getClusterIdsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder> 
           getClusterIdsBuilderList() {
        return getClusterIdsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
          getClusterIdsFieldBuilder() {
        if (clusterIdsBuilder_ == null) {
          clusterIdsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder>(
                  clusterIds_,
                  ((bitField0_ & 0x00000080) == 0x00000080),
                  getParentForChildren(),
                  isClean());
          clusterIds_ = null;
        }
        return clusterIdsBuilder_;
      }

      // optional uint64 nonceGroup = 9;
      private long nonceGroup_ ;
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public boolean hasNonceGroup() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public long getNonceGroup() {
        return nonceGroup_;
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public Builder setNonceGroup(long value) {
        bitField0_ |= 0x00000100;
        nonceGroup_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public Builder clearNonceGroup() {
        bitField0_ = (bitField0_ & ~0x00000100);
        nonceGroup_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 nonce = 10;
      private long nonce_ ;
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public boolean hasNonce() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public long getNonce() {
        return nonce_;
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public Builder setNonce(long value) {
        bitField0_ |= 0x00000200;
        nonce_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public Builder clearNonce() {
        bitField0_ = (bitField0_ & ~0x00000200);
        nonce_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 orig_sequence_number = 11;
      private long origSequenceNumber_ ;
      /**
       * <code>optional uint64 orig_sequence_number = 11;</code>
       */
      public boolean hasOrigSequenceNumber() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional uint64 orig_sequence_number = 11;</code>
       */
      public long getOrigSequenceNumber() {
        return origSequenceNumber_;
      }
      /**
       * <code>optional uint64 orig_sequence_number = 11;</code>
       */
      public Builder setOrigSequenceNumber(long value) {
        bitField0_ |= 0x00000400;
        origSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 orig_sequence_number = 11;</code>
       */
      public Builder clearOrigSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000400);
        origSequenceNumber_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hbase.pb.Attribute extended_attributes = 12;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> extendedAttributes_ =
        java.util.Collections.emptyList();
      private void ensureExtendedAttributesIsMutable() {
        if (!((bitField0_ & 0x00000800) == 0x00000800)) {
          extendedAttributes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute>(extendedAttributes_);
          bitField0_ |= 0x00000800;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder> extendedAttributesBuilder_;

      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> getExtendedAttributesList() {
        if (extendedAttributesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(extendedAttributes_);
        } else {
          return extendedAttributesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public int getExtendedAttributesCount() {
        if (extendedAttributesBuilder_ == null) {
          return extendedAttributes_.size();
        } else {
          return extendedAttributesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute getExtendedAttributes(int index) {
        if (extendedAttributesBuilder_ == null) {
          return extendedAttributes_.get(index);
        } else {
          return extendedAttributesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder setExtendedAttributes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute value) {
        if (extendedAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.set(index, value);
          onChanged();
        } else {
          extendedAttributesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder setExtendedAttributes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder builderForValue) {
        if (extendedAttributesBuilder_ == null) {
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.set(index, builderForValue.build());
          onChanged();
        } else {
          extendedAttributesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder addExtendedAttributes(org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute value) {
        if (extendedAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.add(value);
          onChanged();
        } else {
          extendedAttributesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder addExtendedAttributes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute value) {
        if (extendedAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.add(index, value);
          onChanged();
        } else {
          extendedAttributesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder addExtendedAttributes(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder builderForValue) {
        if (extendedAttributesBuilder_ == null) {
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.add(builderForValue.build());
          onChanged();
        } else {
          extendedAttributesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder addExtendedAttributes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder builderForValue) {
        if (extendedAttributesBuilder_ == null) {
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.add(index, builderForValue.build());
          onChanged();
        } else {
          extendedAttributesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder addAllExtendedAttributes(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute> values) {
        if (extendedAttributesBuilder_ == null) {
          ensureExtendedAttributesIsMutable();
          super.addAll(values, extendedAttributes_);
          onChanged();
        } else {
          extendedAttributesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder clearExtendedAttributes() {
        if (extendedAttributesBuilder_ == null) {
          extendedAttributes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
          onChanged();
        } else {
          extendedAttributesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public Builder removeExtendedAttributes(int index) {
        if (extendedAttributesBuilder_ == null) {
          ensureExtendedAttributesIsMutable();
          extendedAttributes_.remove(index);
          onChanged();
        } else {
          extendedAttributesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder getExtendedAttributesBuilder(
          int index) {
        return getExtendedAttributesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder getExtendedAttributesOrBuilder(
          int index) {
        if (extendedAttributesBuilder_ == null) {
          return extendedAttributes_.get(index);  } else {
          return extendedAttributesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder> 
           getExtendedAttributesOrBuilderList() {
        if (extendedAttributesBuilder_ != null) {
          return extendedAttributesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(extendedAttributes_);
        }
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder addExtendedAttributesBuilder() {
        return getExtendedAttributesFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder addExtendedAttributesBuilder(
          int index) {
        return getExtendedAttributesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.Attribute extended_attributes = 12;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder> 
           getExtendedAttributesBuilderList() {
        return getExtendedAttributesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder> 
          getExtendedAttributesFieldBuilder() {
        if (extendedAttributesBuilder_ == null) {
          extendedAttributesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder>(
                  extendedAttributes_,
                  ((bitField0_ & 0x00000800) == 0x00000800),
                  getParentForChildren(),
                  isClean());
          extendedAttributes_ = null;
        }
        return extendedAttributesBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.WALKey)
    }

    static {
      defaultInstance = new WALKey(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.WALKey)
  }

  public interface AttributeOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string key = 1;
    /**
     * <code>required string key = 1;</code>
     */
    boolean hasKey();
    /**
     * <code>required string key = 1;</code>
     */
    java.lang.String getKey();
    /**
     * <code>required string key = 1;</code>
     */
    com.google.protobuf.ByteString
        getKeyBytes();

    // required bytes value = 2;
    /**
     * <code>required bytes value = 2;</code>
     */
    boolean hasValue();
    /**
     * <code>required bytes value = 2;</code>
     */
    com.google.protobuf.ByteString getValue();
  }
  /**
   * Protobuf type {@code hbase.pb.Attribute}
   */
  public static final class Attribute extends
      com.google.protobuf.GeneratedMessage
      implements AttributeOrBuilder {
    // Use Attribute.newBuilder() to construct.
    private Attribute(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private Attribute(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final Attribute defaultInstance;
    public static Attribute getDefaultInstance() {
      return defaultInstance;
    }

    public Attribute getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private Attribute(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              key_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              value_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_Attribute_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_Attribute_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder.class);
    }

    public static com.google.protobuf.Parser<Attribute> PARSER =
        new com.google.protobuf.AbstractParser<Attribute>() {
      public Attribute parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Attribute(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<Attribute> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string key = 1;
    public static final int KEY_FIELD_NUMBER = 1;
    private java.lang.Object key_;
    /**
     * <code>required string key = 1;</code>
     */
    public boolean hasKey() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string key = 1;</code>
     */
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          key_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string key = 1;</code>
     */
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required bytes value = 2;
    public static final int VALUE_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString value_;
    /**
     * <code>required bytes value = 2;</code>
     */
    public boolean hasValue() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes value = 2;</code>
     */
    public com.google.protobuf.ByteString getValue() {
      return value_;
    }

    private void initFields() {
      key_ = "";
      value_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasKey()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasValue()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getKeyBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, value_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getKeyBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, value_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute) obj;

      boolean result = true;
      result = result && (hasKey() == other.hasKey());
      if (hasKey()) {
        result = result && getKey()
            .equals(other.getKey());
      }
      result = result && (hasValue() == other.hasValue());
      if (hasValue()) {
        result = result && getValue()
            .equals(other.getValue());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasKey()) {
        hash = (37 * hash) + KEY_FIELD_NUMBER;
        hash = (53 * hash) + getKey().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + getValue().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.Attribute}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.AttributeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_Attribute_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_Attribute_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        key_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        value_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_Attribute_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.key_ = key_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.value_ = value_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute.getDefaultInstance()) return this;
        if (other.hasKey()) {
          bitField0_ |= 0x00000001;
          key_ = other.key_;
          onChanged();
        }
        if (other.hasValue()) {
          setValue(other.getValue());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasKey()) {
          
          return false;
        }
        if (!hasValue()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.Attribute) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string key = 1;
      private java.lang.Object key_ = "";
      /**
       * <code>required string key = 1;</code>
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string key = 1;</code>
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string key = 1;</code>
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string key = 1;</code>
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string key = 1;</code>
       */
      public Builder clearKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>required string key = 1;</code>
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }

      // required bytes value = 2;
      private com.google.protobuf.ByteString value_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes value = 2;</code>
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes value = 2;</code>
       */
      public com.google.protobuf.ByteString getValue() {
        return value_;
      }
      /**
       * <code>required bytes value = 2;</code>
       */
      public Builder setValue(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes value = 2;</code>
       */
      public Builder clearValue() {
        bitField0_ = (bitField0_ & ~0x00000002);
        value_ = getDefaultInstance().getValue();
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.Attribute)
    }

    static {
      defaultInstance = new Attribute(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.Attribute)
  }

  public interface FamilyScopeOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes family = 1;
    /**
     * <code>required bytes family = 1;</code>
     */
    boolean hasFamily();
    /**
     * <code>required bytes family = 1;</code>
     */
    com.google.protobuf.ByteString getFamily();

    // required .hbase.pb.ScopeType scope_type = 2;
    /**
     * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
     */
    boolean hasScopeType();
    /**
     * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType();
  }
  /**
   * Protobuf type {@code hbase.pb.FamilyScope}
   */
  public static final class FamilyScope extends
      com.google.protobuf.GeneratedMessage
      implements FamilyScopeOrBuilder {
    // Use FamilyScope.newBuilder() to construct.
    private FamilyScope(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FamilyScope(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FamilyScope defaultInstance;
    public static FamilyScope getDefaultInstance() {
      return defaultInstance;
    }

    public FamilyScope getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FamilyScope(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              family_ = input.readBytes();
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType value = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                scopeType_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FamilyScope_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FamilyScope_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder.class);
    }

    public static com.google.protobuf.Parser<FamilyScope> PARSER =
        new com.google.protobuf.AbstractParser<FamilyScope>() {
      public FamilyScope parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FamilyScope(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FamilyScope> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes family = 1;
    public static final int FAMILY_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString family_;
    /**
     * <code>required bytes family = 1;</code>
     */
    public boolean hasFamily() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes family = 1;</code>
     */
    public com.google.protobuf.ByteString getFamily() {
      return family_;
    }

    // required .hbase.pb.ScopeType scope_type = 2;
    public static final int SCOPE_TYPE_FIELD_NUMBER = 2;
    private org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType scopeType_;
    /**
     * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
     */
    public boolean hasScopeType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType() {
      return scopeType_;
    }

    private void initFields() {
      family_ = com.google.protobuf.ByteString.EMPTY;
      scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasFamily()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasScopeType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, family_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, scopeType_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, family_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, scopeType_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) obj;

      boolean result = true;
      result = result && (hasFamily() == other.hasFamily());
      if (hasFamily()) {
        result = result && getFamily()
            .equals(other.getFamily());
      }
      result = result && (hasScopeType() == other.hasScopeType());
      if (hasScopeType()) {
        result = result &&
            (getScopeType() == other.getScopeType());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasFamily()) {
        hash = (37 * hash) + FAMILY_FIELD_NUMBER;
        hash = (53 * hash) + getFamily().hashCode();
      }
      if (hasScopeType()) {
        hash = (37 * hash) + SCOPE_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getScopeType());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.FamilyScope}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FamilyScope_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FamilyScope_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        family_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FamilyScope_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.family_ = family_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.scopeType_ = scopeType_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance()) return this;
        if (other.hasFamily()) {
          setFamily(other.getFamily());
        }
        if (other.hasScopeType()) {
          setScopeType(other.getScopeType());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasFamily()) {
          
          return false;
        }
        if (!hasScopeType()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes family = 1;
      private com.google.protobuf.ByteString family_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes family = 1;</code>
       */
      public boolean hasFamily() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public com.google.protobuf.ByteString getFamily() {
        return family_;
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public Builder setFamily(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        family_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public Builder clearFamily() {
        bitField0_ = (bitField0_ & ~0x00000001);
        family_ = getDefaultInstance().getFamily();
        onChanged();
        return this;
      }

      // required .hbase.pb.ScopeType scope_type = 2;
      private org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
      /**
       * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
       */
      public boolean hasScopeType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType() {
        return scopeType_;
      }
      /**
       * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
       */
      public Builder setScopeType(org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        scopeType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hbase.pb.ScopeType scope_type = 2;</code>
       */
      public Builder clearScopeType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.FamilyScope)
    }

    static {
      defaultInstance = new FamilyScope(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.FamilyScope)
  }

  public interface CompactionDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes table_name = 1;
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    com.google.protobuf.ByteString getTableName();

    // required bytes encoded_region_name = 2;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // required bytes family_name = 3;
    /**
     * <code>required bytes family_name = 3;</code>
     */
    boolean hasFamilyName();
    /**
     * <code>required bytes family_name = 3;</code>
     */
    com.google.protobuf.ByteString getFamilyName();

    // repeated string compaction_input = 4;
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    java.util.List<java.lang.String>
    getCompactionInputList();
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    int getCompactionInputCount();
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    java.lang.String getCompactionInput(int index);
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    com.google.protobuf.ByteString
        getCompactionInputBytes(int index);

    // repeated string compaction_output = 5;
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    java.util.List<java.lang.String>
    getCompactionOutputList();
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    int getCompactionOutputCount();
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    java.lang.String getCompactionOutput(int index);
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    com.google.protobuf.ByteString
        getCompactionOutputBytes(int index);

    // required string store_home_dir = 6;
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    boolean hasStoreHomeDir();
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    java.lang.String getStoreHomeDir();
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    com.google.protobuf.ByteString
        getStoreHomeDirBytes();

    // optional bytes region_name = 7;
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    boolean hasRegionName();
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    com.google.protobuf.ByteString getRegionName();
  }
  /**
   * Protobuf type {@code hbase.pb.CompactionDescriptor}
   *
   * <pre>
   **
   * Special WAL entry to hold all related to a compaction.
   * Written to WAL before completing compaction.  There is
   * sufficient info in the below message to complete later
   * the * compaction should we fail the WAL write.
   * </pre>
   */
  public static final class CompactionDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements CompactionDescriptorOrBuilder {
    // Use CompactionDescriptor.newBuilder() to construct.
    private CompactionDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CompactionDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CompactionDescriptor defaultInstance;
    public static CompactionDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public CompactionDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CompactionDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              tableName_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              familyName_ = input.readBytes();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                compactionInput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000008;
              }
              compactionInput_.add(input.readBytes());
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                compactionOutput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000010;
              }
              compactionOutput_.add(input.readBytes());
              break;
            }
            case 50: {
              bitField0_ |= 0x00000008;
              storeHomeDir_ = input.readBytes();
              break;
            }
            case 58: {
              bitField0_ |= 0x00000010;
              regionName_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.UnmodifiableLazyStringList(compactionInput_);
        }
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(compactionOutput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_CompactionDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_CompactionDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<CompactionDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<CompactionDescriptor>() {
      public CompactionDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CompactionDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CompactionDescriptor> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes table_name = 1;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required bytes encoded_region_name = 2;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // required bytes family_name = 3;
    public static final int FAMILY_NAME_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString familyName_;
    /**
     * <code>required bytes family_name = 3;</code>
     */
    public boolean hasFamilyName() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required bytes family_name = 3;</code>
     */
    public com.google.protobuf.ByteString getFamilyName() {
      return familyName_;
    }

    // repeated string compaction_input = 4;
    public static final int COMPACTION_INPUT_FIELD_NUMBER = 4;
    private com.google.protobuf.LazyStringList compactionInput_;
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public java.util.List<java.lang.String>
        getCompactionInputList() {
      return compactionInput_;
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public int getCompactionInputCount() {
      return compactionInput_.size();
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public java.lang.String getCompactionInput(int index) {
      return compactionInput_.get(index);
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public com.google.protobuf.ByteString
        getCompactionInputBytes(int index) {
      return compactionInput_.getByteString(index);
    }

    // repeated string compaction_output = 5;
    public static final int COMPACTION_OUTPUT_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList compactionOutput_;
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public java.util.List<java.lang.String>
        getCompactionOutputList() {
      return compactionOutput_;
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public int getCompactionOutputCount() {
      return compactionOutput_.size();
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public java.lang.String getCompactionOutput(int index) {
      return compactionOutput_.get(index);
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public com.google.protobuf.ByteString
        getCompactionOutputBytes(int index) {
      return compactionOutput_.getByteString(index);
    }

    // required string store_home_dir = 6;
    public static final int STORE_HOME_DIR_FIELD_NUMBER = 6;
    private java.lang.Object storeHomeDir_;
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    public boolean hasStoreHomeDir() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    public java.lang.String getStoreHomeDir() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          storeHomeDir_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string store_home_dir = 6;</code>
     *
     * <pre>
     * relative to region dir
     * </pre>
     */
    public com.google.protobuf.ByteString
        getStoreHomeDirBytes() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storeHomeDir_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bytes region_name = 7;
    public static final int REGION_NAME_FIELD_NUMBER = 7;
    private com.google.protobuf.ByteString regionName_;
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public boolean hasRegionName() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public com.google.protobuf.ByteString getRegionName() {
      return regionName_;
    }

    private void initFields() {
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      familyName_ = com.google.protobuf.ByteString.EMPTY;
      compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      storeHomeDir_ = "";
      regionName_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasFamilyName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStoreHomeDir()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, familyName_);
      }
      for (int i = 0; i < compactionInput_.size(); i++) {
        output.writeBytes(4, compactionInput_.getByteString(i));
      }
      for (int i = 0; i < compactionOutput_.size(); i++) {
        output.writeBytes(5, compactionOutput_.getByteString(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(6, getStoreHomeDirBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(7, regionName_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, familyName_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < compactionInput_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(compactionInput_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getCompactionInputList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < compactionOutput_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(compactionOutput_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getCompactionOutputList().size();
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, getStoreHomeDirBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, regionName_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) obj;

      boolean result = true;
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasFamilyName() == other.hasFamilyName());
      if (hasFamilyName()) {
        result = result && getFamilyName()
            .equals(other.getFamilyName());
      }
      result = result && getCompactionInputList()
          .equals(other.getCompactionInputList());
      result = result && getCompactionOutputList()
          .equals(other.getCompactionOutputList());
      result = result && (hasStoreHomeDir() == other.hasStoreHomeDir());
      if (hasStoreHomeDir()) {
        result = result && getStoreHomeDir()
            .equals(other.getStoreHomeDir());
      }
      result = result && (hasRegionName() == other.hasRegionName());
      if (hasRegionName()) {
        result = result && getRegionName()
            .equals(other.getRegionName());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasFamilyName()) {
        hash = (37 * hash) + FAMILY_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getFamilyName().hashCode();
      }
      if (getCompactionInputCount() > 0) {
        hash = (37 * hash) + COMPACTION_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getCompactionInputList().hashCode();
      }
      if (getCompactionOutputCount() > 0) {
        hash = (37 * hash) + COMPACTION_OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getCompactionOutputList().hashCode();
      }
      if (hasStoreHomeDir()) {
        hash = (37 * hash) + STORE_HOME_DIR_FIELD_NUMBER;
        hash = (53 * hash) + getStoreHomeDir().hashCode();
      }
      if (hasRegionName()) {
        hash = (37 * hash) + REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getRegionName().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.CompactionDescriptor}
     *
     * <pre>
     **
     * Special WAL entry to hold all related to a compaction.
     * Written to WAL before completing compaction.  There is
     * sufficient info in the below message to complete later
     * the * compaction should we fail the WAL write.
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_CompactionDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_CompactionDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        familyName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        storeHomeDir_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        regionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_CompactionDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.familyName_ = familyName_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.UnmodifiableLazyStringList(
              compactionInput_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.compactionInput_ = compactionInput_;
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(
              compactionOutput_);
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.compactionOutput_ = compactionOutput_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000008;
        }
        result.storeHomeDir_ = storeHomeDir_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000010;
        }
        result.regionName_ = regionName_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasFamilyName()) {
          setFamilyName(other.getFamilyName());
        }
        if (!other.compactionInput_.isEmpty()) {
          if (compactionInput_.isEmpty()) {
            compactionInput_ = other.compactionInput_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureCompactionInputIsMutable();
            compactionInput_.addAll(other.compactionInput_);
          }
          onChanged();
        }
        if (!other.compactionOutput_.isEmpty()) {
          if (compactionOutput_.isEmpty()) {
            compactionOutput_ = other.compactionOutput_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureCompactionOutputIsMutable();
            compactionOutput_.addAll(other.compactionOutput_);
          }
          onChanged();
        }
        if (other.hasStoreHomeDir()) {
          bitField0_ |= 0x00000020;
          storeHomeDir_ = other.storeHomeDir_;
          onChanged();
        }
        if (other.hasRegionName()) {
          setRegionName(other.getRegionName());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        if (!hasFamilyName()) {
          
          return false;
        }
        if (!hasStoreHomeDir()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes table_name = 1;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required bytes encoded_region_name = 2;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // required bytes family_name = 3;
      private com.google.protobuf.ByteString familyName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public boolean hasFamilyName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public com.google.protobuf.ByteString getFamilyName() {
        return familyName_;
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public Builder setFamilyName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        familyName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public Builder clearFamilyName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        familyName_ = getDefaultInstance().getFamilyName();
        onChanged();
        return this;
      }

      // repeated string compaction_input = 4;
      private com.google.protobuf.LazyStringList compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureCompactionInputIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.LazyStringArrayList(compactionInput_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public java.util.List<java.lang.String>
          getCompactionInputList() {
        return java.util.Collections.unmodifiableList(compactionInput_);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public int getCompactionInputCount() {
        return compactionInput_.size();
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public java.lang.String getCompactionInput(int index) {
        return compactionInput_.get(index);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public com.google.protobuf.ByteString
          getCompactionInputBytes(int index) {
        return compactionInput_.getByteString(index);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder setCompactionInput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addCompactionInput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addAllCompactionInput(
          java.lang.Iterable<java.lang.String> values) {
        ensureCompactionInputIsMutable();
        super.addAll(values, compactionInput_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder clearCompactionInput() {
        compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addCompactionInputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.add(value);
        onChanged();
        return this;
      }

      // repeated string compaction_output = 5;
      private com.google.protobuf.LazyStringList compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureCompactionOutputIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.LazyStringArrayList(compactionOutput_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public java.util.List<java.lang.String>
          getCompactionOutputList() {
        return java.util.Collections.unmodifiableList(compactionOutput_);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public int getCompactionOutputCount() {
        return compactionOutput_.size();
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public java.lang.String getCompactionOutput(int index) {
        return compactionOutput_.get(index);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public com.google.protobuf.ByteString
          getCompactionOutputBytes(int index) {
        return compactionOutput_.getByteString(index);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder setCompactionOutput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addCompactionOutput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addAllCompactionOutput(
          java.lang.Iterable<java.lang.String> values) {
        ensureCompactionOutputIsMutable();
        super.addAll(values, compactionOutput_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder clearCompactionOutput() {
        compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addCompactionOutputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.add(value);
        onChanged();
        return this;
      }

      // required string store_home_dir = 6;
      private java.lang.Object storeHomeDir_ = "";
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public boolean hasStoreHomeDir() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public java.lang.String getStoreHomeDir() {
        java.lang.Object ref = storeHomeDir_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          storeHomeDir_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public com.google.protobuf.ByteString
          getStoreHomeDirBytes() {
        java.lang.Object ref = storeHomeDir_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storeHomeDir_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public Builder setStoreHomeDir(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public Builder clearStoreHomeDir() {
        bitField0_ = (bitField0_ & ~0x00000020);
        storeHomeDir_ = getDefaultInstance().getStoreHomeDir();
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       *
       * <pre>
       * relative to region dir
       * </pre>
       */
      public Builder setStoreHomeDirBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }

      // optional bytes region_name = 7;
      private com.google.protobuf.ByteString regionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public boolean hasRegionName() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public com.google.protobuf.ByteString getRegionName() {
        return regionName_;
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder setRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        regionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder clearRegionName() {
        bitField0_ = (bitField0_ & ~0x00000040);
        regionName_ = getDefaultInstance().getRegionName();
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.CompactionDescriptor)
    }

    static {
      defaultInstance = new CompactionDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.CompactionDescriptor)
  }

  public interface FlushDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hbase.pb.FlushDescriptor.FlushAction action = 1;
    /**
     * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
     */
    boolean hasAction();
    /**
     * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction getAction();

    // required bytes table_name = 2;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 2;</code>
     */
    com.google.protobuf.ByteString getTableName();

    // required bytes encoded_region_name = 3;
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // optional uint64 flush_sequence_number = 4;
    /**
     * <code>optional uint64 flush_sequence_number = 4;</code>
     */
    boolean hasFlushSequenceNumber();
    /**
     * <code>optional uint64 flush_sequence_number = 4;</code>
     */
    long getFlushSequenceNumber();

    // repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> 
        getStoreFlushesList();
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor getStoreFlushes(int index);
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    int getStoreFlushesCount();
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder> 
        getStoreFlushesOrBuilderList();
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder getStoreFlushesOrBuilder(
        int index);

    // optional bytes region_name = 6;
    /**
     * <code>optional bytes region_name = 6;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    boolean hasRegionName();
    /**
     * <code>optional bytes region_name = 6;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    com.google.protobuf.ByteString getRegionName();
  }
  /**
   * Protobuf type {@code hbase.pb.FlushDescriptor}
   *
   * <pre>
   **
   * Special WAL entry to hold all related to a flush.
   * </pre>
   */
  public static final class FlushDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements FlushDescriptorOrBuilder {
    // Use FlushDescriptor.newBuilder() to construct.
    private FlushDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FlushDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FlushDescriptor defaultInstance;
    public static FlushDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public FlushDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FlushDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction value = org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                action_ = value;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              tableName_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              flushSequenceNumber_ = input.readUInt64();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                storeFlushes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor>();
                mutable_bitField0_ |= 0x00000010;
              }
              storeFlushes_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              bitField0_ |= 0x00000010;
              regionName_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          storeFlushes_ = java.util.Collections.unmodifiableList(storeFlushes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<FlushDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<FlushDescriptor>() {
      public FlushDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FlushDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FlushDescriptor> getParserForType() {
      return PARSER;
    }

    /**
     * Protobuf enum {@code hbase.pb.FlushDescriptor.FlushAction}
     */
    public enum FlushAction
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>START_FLUSH = 0;</code>
       */
      START_FLUSH(0, 0),
      /**
       * <code>COMMIT_FLUSH = 1;</code>
       */
      COMMIT_FLUSH(1, 1),
      /**
       * <code>ABORT_FLUSH = 2;</code>
       */
      ABORT_FLUSH(2, 2),
      /**
       * <code>CANNOT_FLUSH = 3;</code>
       *
       * <pre>
       * marker for indicating that a flush has been requested but cannot complete
       * </pre>
       */
      CANNOT_FLUSH(3, 3),
      ;

      /**
       * <code>START_FLUSH = 0;</code>
       */
      public static final int START_FLUSH_VALUE = 0;
      /**
       * <code>COMMIT_FLUSH = 1;</code>
       */
      public static final int COMMIT_FLUSH_VALUE = 1;
      /**
       * <code>ABORT_FLUSH = 2;</code>
       */
      public static final int ABORT_FLUSH_VALUE = 2;
      /**
       * <code>CANNOT_FLUSH = 3;</code>
       *
       * <pre>
       * marker for indicating that a flush has been requested but cannot complete
       * </pre>
       */
      public static final int CANNOT_FLUSH_VALUE = 3;


      public final int getNumber() { return value; }

      public static FlushAction valueOf(int value) {
        switch (value) {
          case 0: return START_FLUSH;
          case 1: return COMMIT_FLUSH;
          case 2: return ABORT_FLUSH;
          case 3: return CANNOT_FLUSH;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<FlushAction>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<FlushAction>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<FlushAction>() {
              public FlushAction findValueByNumber(int number) {
                return FlushAction.valueOf(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.getDescriptor().getEnumTypes().get(0);
      }

      private static final FlushAction[] VALUES = values();

      public static FlushAction valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int index;
      private final int value;

      private FlushAction(int index, int value) {
        this.index = index;
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:hbase.pb.FlushDescriptor.FlushAction)
    }

    public interface StoreFlushDescriptorOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // required bytes family_name = 1;
      /**
       * <code>required bytes family_name = 1;</code>
       */
      boolean hasFamilyName();
      /**
       * <code>required bytes family_name = 1;</code>
       */
      com.google.protobuf.ByteString getFamilyName();

      // required string store_home_dir = 2;
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      boolean hasStoreHomeDir();
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      java.lang.String getStoreHomeDir();
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      com.google.protobuf.ByteString
          getStoreHomeDirBytes();

      // repeated string flush_output = 3;
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      java.util.List<java.lang.String>
      getFlushOutputList();
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      int getFlushOutputCount();
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      java.lang.String getFlushOutput(int index);
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      com.google.protobuf.ByteString
          getFlushOutputBytes(int index);
    }
    /**
     * Protobuf type {@code hbase.pb.FlushDescriptor.StoreFlushDescriptor}
     */
    public static final class StoreFlushDescriptor extends
        com.google.protobuf.GeneratedMessage
        implements StoreFlushDescriptorOrBuilder {
      // Use StoreFlushDescriptor.newBuilder() to construct.
      private StoreFlushDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private StoreFlushDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final StoreFlushDescriptor defaultInstance;
      public static StoreFlushDescriptor getDefaultInstance() {
        return defaultInstance;
      }

      public StoreFlushDescriptor getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private StoreFlushDescriptor(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 10: {
                bitField0_ |= 0x00000001;
                familyName_ = input.readBytes();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                storeHomeDir_ = input.readBytes();
                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                  flushOutput_ = new com.google.protobuf.LazyStringArrayList();
                  mutable_bitField0_ |= 0x00000004;
                }
                flushOutput_.add(input.readBytes());
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
            flushOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(flushOutput_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder.class);
      }

      public static com.google.protobuf.Parser<StoreFlushDescriptor> PARSER =
          new com.google.protobuf.AbstractParser<StoreFlushDescriptor>() {
        public StoreFlushDescriptor parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new StoreFlushDescriptor(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<StoreFlushDescriptor> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // required bytes family_name = 1;
      public static final int FAMILY_NAME_FIELD_NUMBER = 1;
      private com.google.protobuf.ByteString familyName_;
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public boolean hasFamilyName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public com.google.protobuf.ByteString getFamilyName() {
        return familyName_;
      }

      // required string store_home_dir = 2;
      public static final int STORE_HOME_DIR_FIELD_NUMBER = 2;
      private java.lang.Object storeHomeDir_;
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public boolean hasStoreHomeDir() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public java.lang.String getStoreHomeDir() {
        java.lang.Object ref = storeHomeDir_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            storeHomeDir_ = s;
          }
          return s;
        }
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public com.google.protobuf.ByteString
          getStoreHomeDirBytes() {
        java.lang.Object ref = storeHomeDir_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storeHomeDir_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // repeated string flush_output = 3;
      public static final int FLUSH_OUTPUT_FIELD_NUMBER = 3;
      private com.google.protobuf.LazyStringList flushOutput_;
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      public java.util.List<java.lang.String>
          getFlushOutputList() {
        return flushOutput_;
      }
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      public int getFlushOutputCount() {
        return flushOutput_.size();
      }
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      public java.lang.String getFlushOutput(int index) {
        return flushOutput_.get(index);
      }
      /**
       * <code>repeated string flush_output = 3;</code>
       *
       * <pre>
       * relative to store dir (if this is a COMMIT_FLUSH)
       * </pre>
       */
      public com.google.protobuf.ByteString
          getFlushOutputBytes(int index) {
        return flushOutput_.getByteString(index);
      }

      private void initFields() {
        familyName_ = com.google.protobuf.ByteString.EMPTY;
        storeHomeDir_ = "";
        flushOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        if (!hasFamilyName()) {
          memoizedIsInitialized = 0;
          return false;
        }
        if (!hasStoreHomeDir()) {
          memoizedIsInitialized = 0;
          return false;
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeBytes(1, familyName_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, getStoreHomeDirBytes());
        }
        for (int i = 0; i < flushOutput_.size(); i++) {
          output.writeBytes(3, flushOutput_.getByteString(i));
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(1, familyName_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, getStoreHomeDirBytes());
        }
        {
          int dataSize = 0;
          for (int i = 0; i < flushOutput_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeBytesSizeNoTag(flushOutput_.getByteString(i));
          }
          size += dataSize;
          size += 1 * getFlushOutputList().size();
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor)) {
          return super.equals(obj);
        }
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor) obj;

        boolean result = true;
        result = result && (hasFamilyName() == other.hasFamilyName());
        if (hasFamilyName()) {
          result = result && getFamilyName()
              .equals(other.getFamilyName());
        }
        result = result && (hasStoreHomeDir() == other.hasStoreHomeDir());
        if (hasStoreHomeDir()) {
          result = result && getStoreHomeDir()
              .equals(other.getStoreHomeDir());
        }
        result = result && getFlushOutputList()
            .equals(other.getFlushOutputList());
        result = result &&
            getUnknownFields().equals(other.getUnknownFields());
        return result;
      }

      private int memoizedHashCode = 0;
      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptorForType().hashCode();
        if (hasFamilyName()) {
          hash = (37 * hash) + FAMILY_NAME_FIELD_NUMBER;
          hash = (53 * hash) + getFamilyName().hashCode();
        }
        if (hasStoreHomeDir()) {
          hash = (37 * hash) + STORE_HOME_DIR_FIELD_NUMBER;
          hash = (53 * hash) + getStoreHomeDir().hashCode();
        }
        if (getFlushOutputCount() > 0) {
          hash = (37 * hash) + FLUSH_OUTPUT_FIELD_NUMBER;
          hash = (53 * hash) + getFlushOutputList().hashCode();
        }
        hash = (29 * hash) + getUnknownFields().hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hbase.pb.FlushDescriptor.StoreFlushDescriptor}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder.class);
        }

        // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          familyName_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000001);
          storeHomeDir_ = "";
          bitField0_ = (bitField0_ & ~0x00000002);
          flushOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor;
        }

        public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor getDefaultInstanceForType() {
          return org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.getDefaultInstance();
        }

        public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor build() {
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor buildPartial() {
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.familyName_ = familyName_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.storeHomeDir_ = storeHomeDir_;
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            flushOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(
                flushOutput_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.flushOutput_ = flushOutput_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor) {
            return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor other) {
          if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.getDefaultInstance()) return this;
          if (other.hasFamilyName()) {
            setFamilyName(other.getFamilyName());
          }
          if (other.hasStoreHomeDir()) {
            bitField0_ |= 0x00000002;
            storeHomeDir_ = other.storeHomeDir_;
            onChanged();
          }
          if (!other.flushOutput_.isEmpty()) {
            if (flushOutput_.isEmpty()) {
              flushOutput_ = other.flushOutput_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureFlushOutputIsMutable();
              flushOutput_.addAll(other.flushOutput_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          if (!hasFamilyName()) {
            
            return false;
          }
          if (!hasStoreHomeDir()) {
            
            return false;
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // required bytes family_name = 1;
        private com.google.protobuf.ByteString familyName_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>required bytes family_name = 1;</code>
         */
        public boolean hasFamilyName() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>required bytes family_name = 1;</code>
         */
        public com.google.protobuf.ByteString getFamilyName() {
          return familyName_;
        }
        /**
         * <code>required bytes family_name = 1;</code>
         */
        public Builder setFamilyName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          familyName_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required bytes family_name = 1;</code>
         */
        public Builder clearFamilyName() {
          bitField0_ = (bitField0_ & ~0x00000001);
          familyName_ = getDefaultInstance().getFamilyName();
          onChanged();
          return this;
        }

        // required string store_home_dir = 2;
        private java.lang.Object storeHomeDir_ = "";
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public boolean hasStoreHomeDir() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public java.lang.String getStoreHomeDir() {
          java.lang.Object ref = storeHomeDir_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            storeHomeDir_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public com.google.protobuf.ByteString
            getStoreHomeDirBytes() {
          java.lang.Object ref = storeHomeDir_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            storeHomeDir_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public Builder setStoreHomeDir(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          storeHomeDir_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public Builder clearStoreHomeDir() {
          bitField0_ = (bitField0_ & ~0x00000002);
          storeHomeDir_ = getDefaultInstance().getStoreHomeDir();
          onChanged();
          return this;
        }
        /**
         * <code>required string store_home_dir = 2;</code>
         *
         * <pre>
         *relative to region dir
         * </pre>
         */
        public Builder setStoreHomeDirBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          storeHomeDir_ = value;
          onChanged();
          return this;
        }

        // repeated string flush_output = 3;
        private com.google.protobuf.LazyStringList flushOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        private void ensureFlushOutputIsMutable() {
          if (!((bitField0_ & 0x00000004) == 0x00000004)) {
            flushOutput_ = new com.google.protobuf.LazyStringArrayList(flushOutput_);
            bitField0_ |= 0x00000004;
           }
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public java.util.List<java.lang.String>
            getFlushOutputList() {
          return java.util.Collections.unmodifiableList(flushOutput_);
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public int getFlushOutputCount() {
          return flushOutput_.size();
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public java.lang.String getFlushOutput(int index) {
          return flushOutput_.get(index);
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public com.google.protobuf.ByteString
            getFlushOutputBytes(int index) {
          return flushOutput_.getByteString(index);
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public Builder setFlushOutput(
            int index, java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  ensureFlushOutputIsMutable();
          flushOutput_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public Builder addFlushOutput(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  ensureFlushOutputIsMutable();
          flushOutput_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public Builder addAllFlushOutput(
            java.lang.Iterable<java.lang.String> values) {
          ensureFlushOutputIsMutable();
          super.addAll(values, flushOutput_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public Builder clearFlushOutput() {
          flushOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }
        /**
         * <code>repeated string flush_output = 3;</code>
         *
         * <pre>
         * relative to store dir (if this is a COMMIT_FLUSH)
         * </pre>
         */
        public Builder addFlushOutputBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  ensureFlushOutputIsMutable();
          flushOutput_.add(value);
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hbase.pb.FlushDescriptor.StoreFlushDescriptor)
      }

      static {
        defaultInstance = new StoreFlushDescriptor(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hbase.pb.FlushDescriptor.StoreFlushDescriptor)
    }

    private int bitField0_;
    // required .hbase.pb.FlushDescriptor.FlushAction action = 1;
    public static final int ACTION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction action_;
    /**
     * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
     */
    public boolean hasAction() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction getAction() {
      return action_;
    }

    // required bytes table_name = 2;
    public static final int TABLE_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required bytes encoded_region_name = 3;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // optional uint64 flush_sequence_number = 4;
    public static final int FLUSH_SEQUENCE_NUMBER_FIELD_NUMBER = 4;
    private long flushSequenceNumber_;
    /**
     * <code>optional uint64 flush_sequence_number = 4;</code>
     */
    public boolean hasFlushSequenceNumber() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint64 flush_sequence_number = 4;</code>
     */
    public long getFlushSequenceNumber() {
      return flushSequenceNumber_;
    }

    // repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;
    public static final int STORE_FLUSHES_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> storeFlushes_;
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> getStoreFlushesList() {
      return storeFlushes_;
    }
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder> 
        getStoreFlushesOrBuilderList() {
      return storeFlushes_;
    }
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    public int getStoreFlushesCount() {
      return storeFlushes_.size();
    }
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor getStoreFlushes(int index) {
      return storeFlushes_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder getStoreFlushesOrBuilder(
        int index) {
      return storeFlushes_.get(index);
    }

    // optional bytes region_name = 6;
    public static final int REGION_NAME_FIELD_NUMBER = 6;
    private com.google.protobuf.ByteString regionName_;
    /**
     * <code>optional bytes region_name = 6;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public boolean hasRegionName() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional bytes region_name = 6;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public com.google.protobuf.ByteString getRegionName() {
      return regionName_;
    }

    private void initFields() {
      action_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction.START_FLUSH;
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      flushSequenceNumber_ = 0L;
      storeFlushes_ = java.util.Collections.emptyList();
      regionName_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasAction()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getStoreFlushesCount(); i++) {
        if (!getStoreFlushes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, action_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, flushSequenceNumber_);
      }
      for (int i = 0; i < storeFlushes_.size(); i++) {
        output.writeMessage(5, storeFlushes_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(6, regionName_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, action_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, flushSequenceNumber_);
      }
      for (int i = 0; i < storeFlushes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, storeFlushes_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, regionName_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor) obj;

      boolean result = true;
      result = result && (hasAction() == other.hasAction());
      if (hasAction()) {
        result = result &&
            (getAction() == other.getAction());
      }
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasFlushSequenceNumber() == other.hasFlushSequenceNumber());
      if (hasFlushSequenceNumber()) {
        result = result && (getFlushSequenceNumber()
            == other.getFlushSequenceNumber());
      }
      result = result && getStoreFlushesList()
          .equals(other.getStoreFlushesList());
      result = result && (hasRegionName() == other.hasRegionName());
      if (hasRegionName()) {
        result = result && getRegionName()
            .equals(other.getRegionName());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasAction()) {
        hash = (37 * hash) + ACTION_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getAction());
      }
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasFlushSequenceNumber()) {
        hash = (37 * hash) + FLUSH_SEQUENCE_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFlushSequenceNumber());
      }
      if (getStoreFlushesCount() > 0) {
        hash = (37 * hash) + STORE_FLUSHES_FIELD_NUMBER;
        hash = (53 * hash) + getStoreFlushesList().hashCode();
      }
      if (hasRegionName()) {
        hash = (37 * hash) + REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getRegionName().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.FlushDescriptor}
     *
     * <pre>
     **
     * Special WAL entry to hold all related to a flush.
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStoreFlushesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        action_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction.START_FLUSH;
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        flushSequenceNumber_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (storeFlushesBuilder_ == null) {
          storeFlushes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          storeFlushesBuilder_.clear();
        }
        regionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_FlushDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.action_ = action_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.flushSequenceNumber_ = flushSequenceNumber_;
        if (storeFlushesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010)) {
            storeFlushes_ = java.util.Collections.unmodifiableList(storeFlushes_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.storeFlushes_ = storeFlushes_;
        } else {
          result.storeFlushes_ = storeFlushesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000010;
        }
        result.regionName_ = regionName_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.getDefaultInstance()) return this;
        if (other.hasAction()) {
          setAction(other.getAction());
        }
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasFlushSequenceNumber()) {
          setFlushSequenceNumber(other.getFlushSequenceNumber());
        }
        if (storeFlushesBuilder_ == null) {
          if (!other.storeFlushes_.isEmpty()) {
            if (storeFlushes_.isEmpty()) {
              storeFlushes_ = other.storeFlushes_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureStoreFlushesIsMutable();
              storeFlushes_.addAll(other.storeFlushes_);
            }
            onChanged();
          }
        } else {
          if (!other.storeFlushes_.isEmpty()) {
            if (storeFlushesBuilder_.isEmpty()) {
              storeFlushesBuilder_.dispose();
              storeFlushesBuilder_ = null;
              storeFlushes_ = other.storeFlushes_;
              bitField0_ = (bitField0_ & ~0x00000010);
              storeFlushesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStoreFlushesFieldBuilder() : null;
            } else {
              storeFlushesBuilder_.addAllMessages(other.storeFlushes_);
            }
          }
        }
        if (other.hasRegionName()) {
          setRegionName(other.getRegionName());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasAction()) {
          
          return false;
        }
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        for (int i = 0; i < getStoreFlushesCount(); i++) {
          if (!getStoreFlushes(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hbase.pb.FlushDescriptor.FlushAction action = 1;
      private org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction action_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction.START_FLUSH;
      /**
       * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
       */
      public boolean hasAction() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction getAction() {
        return action_;
      }
      /**
       * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
       */
      public Builder setAction(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        action_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hbase.pb.FlushDescriptor.FlushAction action = 1;</code>
       */
      public Builder clearAction() {
        bitField0_ = (bitField0_ & ~0x00000001);
        action_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.FlushAction.START_FLUSH;
        onChanged();
        return this;
      }

      // required bytes table_name = 2;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required bytes encoded_region_name = 3;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // optional uint64 flush_sequence_number = 4;
      private long flushSequenceNumber_ ;
      /**
       * <code>optional uint64 flush_sequence_number = 4;</code>
       */
      public boolean hasFlushSequenceNumber() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 flush_sequence_number = 4;</code>
       */
      public long getFlushSequenceNumber() {
        return flushSequenceNumber_;
      }
      /**
       * <code>optional uint64 flush_sequence_number = 4;</code>
       */
      public Builder setFlushSequenceNumber(long value) {
        bitField0_ |= 0x00000008;
        flushSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 flush_sequence_number = 4;</code>
       */
      public Builder clearFlushSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000008);
        flushSequenceNumber_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> storeFlushes_ =
        java.util.Collections.emptyList();
      private void ensureStoreFlushesIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          storeFlushes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor>(storeFlushes_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder> storeFlushesBuilder_;

      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> getStoreFlushesList() {
        if (storeFlushesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(storeFlushes_);
        } else {
          return storeFlushesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public int getStoreFlushesCount() {
        if (storeFlushesBuilder_ == null) {
          return storeFlushes_.size();
        } else {
          return storeFlushesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor getStoreFlushes(int index) {
        if (storeFlushesBuilder_ == null) {
          return storeFlushes_.get(index);
        } else {
          return storeFlushesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder setStoreFlushes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor value) {
        if (storeFlushesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoreFlushesIsMutable();
          storeFlushes_.set(index, value);
          onChanged();
        } else {
          storeFlushesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder setStoreFlushes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder builderForValue) {
        if (storeFlushesBuilder_ == null) {
          ensureStoreFlushesIsMutable();
          storeFlushes_.set(index, builderForValue.build());
          onChanged();
        } else {
          storeFlushesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder addStoreFlushes(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor value) {
        if (storeFlushesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoreFlushesIsMutable();
          storeFlushes_.add(value);
          onChanged();
        } else {
          storeFlushesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder addStoreFlushes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor value) {
        if (storeFlushesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoreFlushesIsMutable();
          storeFlushes_.add(index, value);
          onChanged();
        } else {
          storeFlushesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder addStoreFlushes(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder builderForValue) {
        if (storeFlushesBuilder_ == null) {
          ensureStoreFlushesIsMutable();
          storeFlushes_.add(builderForValue.build());
          onChanged();
        } else {
          storeFlushesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder addStoreFlushes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder builderForValue) {
        if (storeFlushesBuilder_ == null) {
          ensureStoreFlushesIsMutable();
          storeFlushes_.add(index, builderForValue.build());
          onChanged();
        } else {
          storeFlushesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder addAllStoreFlushes(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor> values) {
        if (storeFlushesBuilder_ == null) {
          ensureStoreFlushesIsMutable();
          super.addAll(values, storeFlushes_);
          onChanged();
        } else {
          storeFlushesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder clearStoreFlushes() {
        if (storeFlushesBuilder_ == null) {
          storeFlushes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          storeFlushesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public Builder removeStoreFlushes(int index) {
        if (storeFlushesBuilder_ == null) {
          ensureStoreFlushesIsMutable();
          storeFlushes_.remove(index);
          onChanged();
        } else {
          storeFlushesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder getStoreFlushesBuilder(
          int index) {
        return getStoreFlushesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder getStoreFlushesOrBuilder(
          int index) {
        if (storeFlushesBuilder_ == null) {
          return storeFlushes_.get(index);  } else {
          return storeFlushesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder> 
           getStoreFlushesOrBuilderList() {
        if (storeFlushesBuilder_ != null) {
          return storeFlushesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(storeFlushes_);
        }
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder addStoreFlushesBuilder() {
        return getStoreFlushesFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder addStoreFlushesBuilder(
          int index) {
        return getStoreFlushesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.FlushDescriptor.StoreFlushDescriptor store_flushes = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder> 
           getStoreFlushesBuilderList() {
        return getStoreFlushesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder> 
          getStoreFlushesFieldBuilder() {
        if (storeFlushesBuilder_ == null) {
          storeFlushesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FlushDescriptor.StoreFlushDescriptorOrBuilder>(
                  storeFlushes_,
                  ((bitField0_ & 0x00000010) == 0x00000010),
                  getParentForChildren(),
                  isClean());
          storeFlushes_ = null;
        }
        return storeFlushesBuilder_;
      }

      // optional bytes region_name = 6;
      private com.google.protobuf.ByteString regionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes region_name = 6;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public boolean hasRegionName() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional bytes region_name = 6;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public com.google.protobuf.ByteString getRegionName() {
        return regionName_;
      }
      /**
       * <code>optional bytes region_name = 6;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder setRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        regionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes region_name = 6;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder clearRegionName() {
        bitField0_ = (bitField0_ & ~0x00000020);
        regionName_ = getDefaultInstance().getRegionName();
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.FlushDescriptor)
    }

    static {
      defaultInstance = new FlushDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.FlushDescriptor)
  }

  public interface StoreDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes family_name = 1;
    /**
     * <code>required bytes family_name = 1;</code>
     */
    boolean hasFamilyName();
    /**
     * <code>required bytes family_name = 1;</code>
     */
    com.google.protobuf.ByteString getFamilyName();

    // required string store_home_dir = 2;
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    boolean hasStoreHomeDir();
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    java.lang.String getStoreHomeDir();
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    com.google.protobuf.ByteString
        getStoreHomeDirBytes();

    // repeated string store_file = 3;
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    java.util.List<java.lang.String>
    getStoreFileList();
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    int getStoreFileCount();
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    java.lang.String getStoreFile(int index);
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    com.google.protobuf.ByteString
        getStoreFileBytes(int index);

    // optional uint64 store_file_size_bytes = 4;
    /**
     * <code>optional uint64 store_file_size_bytes = 4;</code>
     *
     * <pre>
     * size of store file
     * </pre>
     */
    boolean hasStoreFileSizeBytes();
    /**
     * <code>optional uint64 store_file_size_bytes = 4;</code>
     *
     * <pre>
     * size of store file
     * </pre>
     */
    long getStoreFileSizeBytes();
  }
  /**
   * Protobuf type {@code hbase.pb.StoreDescriptor}
   */
  public static final class StoreDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements StoreDescriptorOrBuilder {
    // Use StoreDescriptor.newBuilder() to construct.
    private StoreDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StoreDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StoreDescriptor defaultInstance;
    public static StoreDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public StoreDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StoreDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              familyName_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              storeHomeDir_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                storeFile_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000004;
              }
              storeFile_.add(input.readBytes());
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              storeFileSizeBytes_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          storeFile_ = new com.google.protobuf.UnmodifiableLazyStringList(storeFile_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_StoreDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_StoreDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<StoreDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<StoreDescriptor>() {
      public StoreDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StoreDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StoreDescriptor> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes family_name = 1;
    public static final int FAMILY_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString familyName_;
    /**
     * <code>required bytes family_name = 1;</code>
     */
    public boolean hasFamilyName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes family_name = 1;</code>
     */
    public com.google.protobuf.ByteString getFamilyName() {
      return familyName_;
    }

    // required string store_home_dir = 2;
    public static final int STORE_HOME_DIR_FIELD_NUMBER = 2;
    private java.lang.Object storeHomeDir_;
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    public boolean hasStoreHomeDir() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    public java.lang.String getStoreHomeDir() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          storeHomeDir_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string store_home_dir = 2;</code>
     *
     * <pre>
     *relative to region dir
     * </pre>
     */
    public com.google.protobuf.ByteString
        getStoreHomeDirBytes() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storeHomeDir_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated string store_file = 3;
    public static final int STORE_FILE_FIELD_NUMBER = 3;
    private com.google.protobuf.LazyStringList storeFile_;
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public java.util.List<java.lang.String>
        getStoreFileList() {
      return storeFile_;
    }
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public int getStoreFileCount() {
      return storeFile_.size();
    }
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public java.lang.String getStoreFile(int index) {
      return storeFile_.get(index);
    }
    /**
     * <code>repeated string store_file = 3;</code>
     *
     * <pre>
     * relative to store dir
     * </pre>
     */
    public com.google.protobuf.ByteString
        getStoreFileBytes(int index) {
      return storeFile_.getByteString(index);
    }

    // optional uint64 store_file_size_bytes = 4;
    public static final int STORE_FILE_SIZE_BYTES_FIELD_NUMBER = 4;
    private long storeFileSizeBytes_;
    /**
     * <code>optional uint64 store_file_size_bytes = 4;</code>
     *
     * <pre>
     * size of store file
     * </pre>
     */
    public boolean hasStoreFileSizeBytes() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 store_file_size_bytes = 4;</code>
     *
     * <pre>
     * size of store file
     * </pre>
     */
    public long getStoreFileSizeBytes() {
      return storeFileSizeBytes_;
    }

    private void initFields() {
      familyName_ = com.google.protobuf.ByteString.EMPTY;
      storeHomeDir_ = "";
      storeFile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      storeFileSizeBytes_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasFamilyName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStoreHomeDir()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, familyName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getStoreHomeDirBytes());
      }
      for (int i = 0; i < storeFile_.size(); i++) {
        output.writeBytes(3, storeFile_.getByteString(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(4, storeFileSizeBytes_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, familyName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getStoreHomeDirBytes());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < storeFile_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(storeFile_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getStoreFileList().size();
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, storeFileSizeBytes_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor) obj;

      boolean result = true;
      result = result && (hasFamilyName() == other.hasFamilyName());
      if (hasFamilyName()) {
        result = result && getFamilyName()
            .equals(other.getFamilyName());
      }
      result = result && (hasStoreHomeDir() == other.hasStoreHomeDir());
      if (hasStoreHomeDir()) {
        result = result && getStoreHomeDir()
            .equals(other.getStoreHomeDir());
      }
      result = result && getStoreFileList()
          .equals(other.getStoreFileList());
      result = result && (hasStoreFileSizeBytes() == other.hasStoreFileSizeBytes());
      if (hasStoreFileSizeBytes()) {
        result = result && (getStoreFileSizeBytes()
            == other.getStoreFileSizeBytes());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasFamilyName()) {
        hash = (37 * hash) + FAMILY_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getFamilyName().hashCode();
      }
      if (hasStoreHomeDir()) {
        hash = (37 * hash) + STORE_HOME_DIR_FIELD_NUMBER;
        hash = (53 * hash) + getStoreHomeDir().hashCode();
      }
      if (getStoreFileCount() > 0) {
        hash = (37 * hash) + STORE_FILE_FIELD_NUMBER;
        hash = (53 * hash) + getStoreFileList().hashCode();
      }
      if (hasStoreFileSizeBytes()) {
        hash = (37 * hash) + STORE_FILE_SIZE_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStoreFileSizeBytes());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.StoreDescriptor}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_StoreDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_StoreDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        familyName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        storeHomeDir_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        storeFile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        storeFileSizeBytes_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_StoreDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.familyName_ = familyName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.storeHomeDir_ = storeHomeDir_;
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          storeFile_ = new com.google.protobuf.UnmodifiableLazyStringList(
              storeFile_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.storeFile_ = storeFile_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.storeFileSizeBytes_ = storeFileSizeBytes_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance()) return this;
        if (other.hasFamilyName()) {
          setFamilyName(other.getFamilyName());
        }
        if (other.hasStoreHomeDir()) {
          bitField0_ |= 0x00000002;
          storeHomeDir_ = other.storeHomeDir_;
          onChanged();
        }
        if (!other.storeFile_.isEmpty()) {
          if (storeFile_.isEmpty()) {
            storeFile_ = other.storeFile_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureStoreFileIsMutable();
            storeFile_.addAll(other.storeFile_);
          }
          onChanged();
        }
        if (other.hasStoreFileSizeBytes()) {
          setStoreFileSizeBytes(other.getStoreFileSizeBytes());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasFamilyName()) {
          
          return false;
        }
        if (!hasStoreHomeDir()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes family_name = 1;
      private com.google.protobuf.ByteString familyName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public boolean hasFamilyName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public com.google.protobuf.ByteString getFamilyName() {
        return familyName_;
      }
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public Builder setFamilyName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        familyName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes family_name = 1;</code>
       */
      public Builder clearFamilyName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        familyName_ = getDefaultInstance().getFamilyName();
        onChanged();
        return this;
      }

      // required string store_home_dir = 2;
      private java.lang.Object storeHomeDir_ = "";
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public boolean hasStoreHomeDir() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public java.lang.String getStoreHomeDir() {
        java.lang.Object ref = storeHomeDir_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          storeHomeDir_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public com.google.protobuf.ByteString
          getStoreHomeDirBytes() {
        java.lang.Object ref = storeHomeDir_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storeHomeDir_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public Builder setStoreHomeDir(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public Builder clearStoreHomeDir() {
        bitField0_ = (bitField0_ & ~0x00000002);
        storeHomeDir_ = getDefaultInstance().getStoreHomeDir();
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 2;</code>
       *
       * <pre>
       *relative to region dir
       * </pre>
       */
      public Builder setStoreHomeDirBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }

      // repeated string store_file = 3;
      private com.google.protobuf.LazyStringList storeFile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureStoreFileIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          storeFile_ = new com.google.protobuf.LazyStringArrayList(storeFile_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public java.util.List<java.lang.String>
          getStoreFileList() {
        return java.util.Collections.unmodifiableList(storeFile_);
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public int getStoreFileCount() {
        return storeFile_.size();
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public java.lang.String getStoreFile(int index) {
        return storeFile_.get(index);
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public com.google.protobuf.ByteString
          getStoreFileBytes(int index) {
        return storeFile_.getByteString(index);
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder setStoreFile(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureStoreFileIsMutable();
        storeFile_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addStoreFile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureStoreFileIsMutable();
        storeFile_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addAllStoreFile(
          java.lang.Iterable<java.lang.String> values) {
        ensureStoreFileIsMutable();
        super.addAll(values, storeFile_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder clearStoreFile() {
        storeFile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string store_file = 3;</code>
       *
       * <pre>
       * relative to store dir
       * </pre>
       */
      public Builder addStoreFileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureStoreFileIsMutable();
        storeFile_.add(value);
        onChanged();
        return this;
      }

      // optional uint64 store_file_size_bytes = 4;
      private long storeFileSizeBytes_ ;
      /**
       * <code>optional uint64 store_file_size_bytes = 4;</code>
       *
       * <pre>
       * size of store file
       * </pre>
       */
      public boolean hasStoreFileSizeBytes() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 store_file_size_bytes = 4;</code>
       *
       * <pre>
       * size of store file
       * </pre>
       */
      public long getStoreFileSizeBytes() {
        return storeFileSizeBytes_;
      }
      /**
       * <code>optional uint64 store_file_size_bytes = 4;</code>
       *
       * <pre>
       * size of store file
       * </pre>
       */
      public Builder setStoreFileSizeBytes(long value) {
        bitField0_ |= 0x00000008;
        storeFileSizeBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 store_file_size_bytes = 4;</code>
       *
       * <pre>
       * size of store file
       * </pre>
       */
      public Builder clearStoreFileSizeBytes() {
        bitField0_ = (bitField0_ & ~0x00000008);
        storeFileSizeBytes_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.StoreDescriptor)
    }

    static {
      defaultInstance = new StoreDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.StoreDescriptor)
  }

  public interface BulkLoadDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hbase.pb.TableName table_name = 1;
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    boolean hasTableName();
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName getTableName();
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder getTableNameOrBuilder();

    // required bytes encoded_region_name = 2;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // repeated .hbase.pb.StoreDescriptor stores = 3;
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> 
        getStoresList();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index);
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    int getStoresCount();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
        getStoresOrBuilderList();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
        int index);

    // required int64 bulkload_seq_num = 4;
    /**
     * <code>required int64 bulkload_seq_num = 4;</code>
     */
    boolean hasBulkloadSeqNum();
    /**
     * <code>required int64 bulkload_seq_num = 4;</code>
     */
    long getBulkloadSeqNum();

    // repeated string cluster_ids = 5;
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    java.util.List<java.lang.String>
    getClusterIdsList();
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    int getClusterIdsCount();
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    java.lang.String getClusterIds(int index);
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    com.google.protobuf.ByteString
        getClusterIdsBytes(int index);
  }
  /**
   * Protobuf type {@code hbase.pb.BulkLoadDescriptor}
   *
   * <pre>
   **
   * Special WAL entry used for writing bulk load events to WAL
   * </pre>
   */
  public static final class BulkLoadDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements BulkLoadDescriptorOrBuilder {
    // Use BulkLoadDescriptor.newBuilder() to construct.
    private BulkLoadDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private BulkLoadDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final BulkLoadDescriptor defaultInstance;
    public static BulkLoadDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public BulkLoadDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private BulkLoadDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = tableName_.toBuilder();
              }
              tableName_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(tableName_);
                tableName_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                stores_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor>();
                mutable_bitField0_ |= 0x00000004;
              }
              stores_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.PARSER, extensionRegistry));
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              bulkloadSeqNum_ = input.readInt64();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                clusterIds_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000010;
              }
              clusterIds_.add(input.readBytes());
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          stores_ = java.util.Collections.unmodifiableList(stores_);
        }
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          clusterIds_ = new com.google.protobuf.UnmodifiableLazyStringList(clusterIds_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_BulkLoadDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_BulkLoadDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<BulkLoadDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<BulkLoadDescriptor>() {
      public BulkLoadDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BulkLoadDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<BulkLoadDescriptor> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hbase.pb.TableName table_name = 1;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName tableName_;
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName getTableName() {
      return tableName_;
    }
    /**
     * <code>required .hbase.pb.TableName table_name = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder getTableNameOrBuilder() {
      return tableName_;
    }

    // required bytes encoded_region_name = 2;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // repeated .hbase.pb.StoreDescriptor stores = 3;
    public static final int STORES_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> stores_;
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> getStoresList() {
      return stores_;
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
        getStoresOrBuilderList() {
      return stores_;
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    public int getStoresCount() {
      return stores_.size();
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index) {
      return stores_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
        int index) {
      return stores_.get(index);
    }

    // required int64 bulkload_seq_num = 4;
    public static final int BULKLOAD_SEQ_NUM_FIELD_NUMBER = 4;
    private long bulkloadSeqNum_;
    /**
     * <code>required int64 bulkload_seq_num = 4;</code>
     */
    public boolean hasBulkloadSeqNum() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 bulkload_seq_num = 4;</code>
     */
    public long getBulkloadSeqNum() {
      return bulkloadSeqNum_;
    }

    // repeated string cluster_ids = 5;
    public static final int CLUSTER_IDS_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList clusterIds_;
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    public java.util.List<java.lang.String>
        getClusterIdsList() {
      return clusterIds_;
    }
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    public int getClusterIdsCount() {
      return clusterIds_.size();
    }
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    public java.lang.String getClusterIds(int index) {
      return clusterIds_.get(index);
    }
    /**
     * <code>repeated string cluster_ids = 5;</code>
     */
    public com.google.protobuf.ByteString
        getClusterIdsBytes(int index) {
      return clusterIds_.getByteString(index);
    }

    private void initFields() {
      tableName_ = org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.getDefaultInstance();
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      stores_ = java.util.Collections.emptyList();
      bulkloadSeqNum_ = 0L;
      clusterIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasBulkloadSeqNum()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getTableName().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getStoresCount(); i++) {
        if (!getStores(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, encodedRegionName_);
      }
      for (int i = 0; i < stores_.size(); i++) {
        output.writeMessage(3, stores_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(4, bulkloadSeqNum_);
      }
      for (int i = 0; i < clusterIds_.size(); i++) {
        output.writeBytes(5, clusterIds_.getByteString(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, encodedRegionName_);
      }
      for (int i = 0; i < stores_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, stores_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, bulkloadSeqNum_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < clusterIds_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(clusterIds_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getClusterIdsList().size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor) obj;

      boolean result = true;
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && getStoresList()
          .equals(other.getStoresList());
      result = result && (hasBulkloadSeqNum() == other.hasBulkloadSeqNum());
      if (hasBulkloadSeqNum()) {
        result = result && (getBulkloadSeqNum()
            == other.getBulkloadSeqNum());
      }
      result = result && getClusterIdsList()
          .equals(other.getClusterIdsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (getStoresCount() > 0) {
        hash = (37 * hash) + STORES_FIELD_NUMBER;
        hash = (53 * hash) + getStoresList().hashCode();
      }
      if (hasBulkloadSeqNum()) {
        hash = (37 * hash) + BULKLOAD_SEQ_NUM_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getBulkloadSeqNum());
      }
      if (getClusterIdsCount() > 0) {
        hash = (37 * hash) + CLUSTER_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getClusterIdsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.BulkLoadDescriptor}
     *
     * <pre>
     **
     * Special WAL entry used for writing bulk load events to WAL
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_BulkLoadDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_BulkLoadDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getTableNameFieldBuilder();
          getStoresFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (tableNameBuilder_ == null) {
          tableName_ = org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.getDefaultInstance();
        } else {
          tableNameBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (storesBuilder_ == null) {
          stores_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          storesBuilder_.clear();
        }
        bulkloadSeqNum_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        clusterIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_BulkLoadDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (tableNameBuilder_ == null) {
          result.tableName_ = tableName_;
        } else {
          result.tableName_ = tableNameBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (storesBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            stores_ = java.util.Collections.unmodifiableList(stores_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.stores_ = stores_;
        } else {
          result.stores_ = storesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.bulkloadSeqNum_ = bulkloadSeqNum_;
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          clusterIds_ = new com.google.protobuf.UnmodifiableLazyStringList(
              clusterIds_);
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.clusterIds_ = clusterIds_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          mergeTableName(other.getTableName());
        }
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (storesBuilder_ == null) {
          if (!other.stores_.isEmpty()) {
            if (stores_.isEmpty()) {
              stores_ = other.stores_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureStoresIsMutable();
              stores_.addAll(other.stores_);
            }
            onChanged();
          }
        } else {
          if (!other.stores_.isEmpty()) {
            if (storesBuilder_.isEmpty()) {
              storesBuilder_.dispose();
              storesBuilder_ = null;
              stores_ = other.stores_;
              bitField0_ = (bitField0_ & ~0x00000004);
              storesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStoresFieldBuilder() : null;
            } else {
              storesBuilder_.addAllMessages(other.stores_);
            }
          }
        }
        if (other.hasBulkloadSeqNum()) {
          setBulkloadSeqNum(other.getBulkloadSeqNum());
        }
        if (!other.clusterIds_.isEmpty()) {
          if (clusterIds_.isEmpty()) {
            clusterIds_ = other.clusterIds_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureClusterIdsIsMutable();
            clusterIds_.addAll(other.clusterIds_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        if (!hasBulkloadSeqNum()) {
          
          return false;
        }
        if (!getTableName().isInitialized()) {
          
          return false;
        }
        for (int i = 0; i < getStoresCount(); i++) {
          if (!getStores(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.BulkLoadDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hbase.pb.TableName table_name = 1;
      private org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName tableName_ = org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder> tableNameBuilder_;
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName getTableName() {
        if (tableNameBuilder_ == null) {
          return tableName_;
        } else {
          return tableNameBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public Builder setTableName(org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName value) {
        if (tableNameBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          tableName_ = value;
          onChanged();
        } else {
          tableNameBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public Builder setTableName(
          org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder builderForValue) {
        if (tableNameBuilder_ == null) {
          tableName_ = builderForValue.build();
          onChanged();
        } else {
          tableNameBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public Builder mergeTableName(org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName value) {
        if (tableNameBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              tableName_ != org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.getDefaultInstance()) {
            tableName_ =
              org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.newBuilder(tableName_).mergeFrom(value).buildPartial();
          } else {
            tableName_ = value;
          }
          onChanged();
        } else {
          tableNameBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public Builder clearTableName() {
        if (tableNameBuilder_ == null) {
          tableName_ = org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.getDefaultInstance();
          onChanged();
        } else {
          tableNameBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder getTableNameBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTableNameFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder getTableNameOrBuilder() {
        if (tableNameBuilder_ != null) {
          return tableNameBuilder_.getMessageOrBuilder();
        } else {
          return tableName_;
        }
      }
      /**
       * <code>required .hbase.pb.TableName table_name = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder> 
          getTableNameFieldBuilder() {
        if (tableNameBuilder_ == null) {
          tableNameBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableName.Builder, org.apache.hadoop.hbase.protobuf.generated.TableProtos.TableNameOrBuilder>(
                  tableName_,
                  getParentForChildren(),
                  isClean());
          tableName_ = null;
        }
        return tableNameBuilder_;
      }

      // required bytes encoded_region_name = 2;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // repeated .hbase.pb.StoreDescriptor stores = 3;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> stores_ =
        java.util.Collections.emptyList();
      private void ensureStoresIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          stores_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor>(stores_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> storesBuilder_;

      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> getStoresList() {
        if (storesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(stores_);
        } else {
          return storesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public int getStoresCount() {
        if (storesBuilder_ == null) {
          return stores_.size();
        } else {
          return storesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index) {
        if (storesBuilder_ == null) {
          return stores_.get(index);
        } else {
          return storesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder setStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.set(index, value);
          onChanged();
        } else {
          storesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder setStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.set(index, builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder addStores(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.add(value);
          onChanged();
        } else {
          storesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder addStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.add(index, value);
          onChanged();
        } else {
          storesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder addStores(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.add(builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder addStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.add(index, builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder addAllStores(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> values) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          super.addAll(values, stores_);
          onChanged();
        } else {
          storesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder clearStores() {
        if (storesBuilder_ == null) {
          stores_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          storesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public Builder removeStores(int index) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.remove(index);
          onChanged();
        } else {
          storesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder getStoresBuilder(
          int index) {
        return getStoresFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
          int index) {
        if (storesBuilder_ == null) {
          return stores_.get(index);  } else {
          return storesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
           getStoresOrBuilderList() {
        if (storesBuilder_ != null) {
          return storesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(stores_);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder addStoresBuilder() {
        return getStoresFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder addStoresBuilder(
          int index) {
        return getStoresFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder> 
           getStoresBuilderList() {
        return getStoresFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
          getStoresFieldBuilder() {
        if (storesBuilder_ == null) {
          storesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder>(
                  stores_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          stores_ = null;
        }
        return storesBuilder_;
      }

      // required int64 bulkload_seq_num = 4;
      private long bulkloadSeqNum_ ;
      /**
       * <code>required int64 bulkload_seq_num = 4;</code>
       */
      public boolean hasBulkloadSeqNum() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required int64 bulkload_seq_num = 4;</code>
       */
      public long getBulkloadSeqNum() {
        return bulkloadSeqNum_;
      }
      /**
       * <code>required int64 bulkload_seq_num = 4;</code>
       */
      public Builder setBulkloadSeqNum(long value) {
        bitField0_ |= 0x00000008;
        bulkloadSeqNum_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 bulkload_seq_num = 4;</code>
       */
      public Builder clearBulkloadSeqNum() {
        bitField0_ = (bitField0_ & ~0x00000008);
        bulkloadSeqNum_ = 0L;
        onChanged();
        return this;
      }

      // repeated string cluster_ids = 5;
      private com.google.protobuf.LazyStringList clusterIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureClusterIdsIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          clusterIds_ = new com.google.protobuf.LazyStringArrayList(clusterIds_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public java.util.List<java.lang.String>
          getClusterIdsList() {
        return java.util.Collections.unmodifiableList(clusterIds_);
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public int getClusterIdsCount() {
        return clusterIds_.size();
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public java.lang.String getClusterIds(int index) {
        return clusterIds_.get(index);
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public com.google.protobuf.ByteString
          getClusterIdsBytes(int index) {
        return clusterIds_.getByteString(index);
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public Builder setClusterIds(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureClusterIdsIsMutable();
        clusterIds_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public Builder addClusterIds(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureClusterIdsIsMutable();
        clusterIds_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public Builder addAllClusterIds(
          java.lang.Iterable<java.lang.String> values) {
        ensureClusterIdsIsMutable();
        super.addAll(values, clusterIds_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public Builder clearClusterIds() {
        clusterIds_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string cluster_ids = 5;</code>
       */
      public Builder addClusterIdsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureClusterIdsIsMutable();
        clusterIds_.add(value);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.BulkLoadDescriptor)
    }

    static {
      defaultInstance = new BulkLoadDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.BulkLoadDescriptor)
  }

  public interface RegionEventDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;
    /**
     * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
     */
    boolean hasEventType();
    /**
     * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType getEventType();

    // required bytes table_name = 2;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 2;</code>
     */
    com.google.protobuf.ByteString getTableName();

    // required bytes encoded_region_name = 3;
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // optional uint64 log_sequence_number = 4;
    /**
     * <code>optional uint64 log_sequence_number = 4;</code>
     */
    boolean hasLogSequenceNumber();
    /**
     * <code>optional uint64 log_sequence_number = 4;</code>
     */
    long getLogSequenceNumber();

    // repeated .hbase.pb.StoreDescriptor stores = 5;
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> 
        getStoresList();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index);
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    int getStoresCount();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
        getStoresOrBuilderList();
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
        int index);

    // optional .hbase.pb.ServerName server = 6;
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    boolean hasServer();
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer();
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder();

    // optional bytes region_name = 7;
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    boolean hasRegionName();
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    com.google.protobuf.ByteString getRegionName();
  }
  /**
   * Protobuf type {@code hbase.pb.RegionEventDescriptor}
   *
   * <pre>
   **
   * Special WAL entry to hold all related to a region event (open/close).
   * </pre>
   */
  public static final class RegionEventDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements RegionEventDescriptorOrBuilder {
    // Use RegionEventDescriptor.newBuilder() to construct.
    private RegionEventDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegionEventDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegionEventDescriptor defaultInstance;
    public static RegionEventDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public RegionEventDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegionEventDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType value = org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                eventType_ = value;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              tableName_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              logSequenceNumber_ = input.readUInt64();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                stores_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor>();
                mutable_bitField0_ |= 0x00000010;
              }
              stores_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = server_.toBuilder();
              }
              server_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(server_);
                server_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 58: {
              bitField0_ |= 0x00000020;
              regionName_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          stores_ = java.util.Collections.unmodifiableList(stores_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_RegionEventDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_RegionEventDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<RegionEventDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<RegionEventDescriptor>() {
      public RegionEventDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegionEventDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegionEventDescriptor> getParserForType() {
      return PARSER;
    }

    /**
     * Protobuf enum {@code hbase.pb.RegionEventDescriptor.EventType}
     */
    public enum EventType
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>REGION_OPEN = 0;</code>
       */
      REGION_OPEN(0, 0),
      /**
       * <code>REGION_CLOSE = 1;</code>
       */
      REGION_CLOSE(1, 1),
      ;

      /**
       * <code>REGION_OPEN = 0;</code>
       */
      public static final int REGION_OPEN_VALUE = 0;
      /**
       * <code>REGION_CLOSE = 1;</code>
       */
      public static final int REGION_CLOSE_VALUE = 1;


      public final int getNumber() { return value; }

      public static EventType valueOf(int value) {
        switch (value) {
          case 0: return REGION_OPEN;
          case 1: return REGION_CLOSE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<EventType>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<EventType>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<EventType>() {
              public EventType findValueByNumber(int number) {
                return EventType.valueOf(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.getDescriptor().getEnumTypes().get(0);
      }

      private static final EventType[] VALUES = values();

      public static EventType valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int index;
      private final int value;

      private EventType(int index, int value) {
        this.index = index;
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:hbase.pb.RegionEventDescriptor.EventType)
    }

    private int bitField0_;
    // required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;
    public static final int EVENT_TYPE_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType eventType_;
    /**
     * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
     */
    public boolean hasEventType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType getEventType() {
      return eventType_;
    }

    // required bytes table_name = 2;
    public static final int TABLE_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required bytes encoded_region_name = 3;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required bytes encoded_region_name = 3;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // optional uint64 log_sequence_number = 4;
    public static final int LOG_SEQUENCE_NUMBER_FIELD_NUMBER = 4;
    private long logSequenceNumber_;
    /**
     * <code>optional uint64 log_sequence_number = 4;</code>
     */
    public boolean hasLogSequenceNumber() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint64 log_sequence_number = 4;</code>
     */
    public long getLogSequenceNumber() {
      return logSequenceNumber_;
    }

    // repeated .hbase.pb.StoreDescriptor stores = 5;
    public static final int STORES_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> stores_;
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> getStoresList() {
      return stores_;
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
        getStoresOrBuilderList() {
      return stores_;
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    public int getStoresCount() {
      return stores_.size();
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index) {
      return stores_.get(index);
    }
    /**
     * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
        int index) {
      return stores_.get(index);
    }

    // optional .hbase.pb.ServerName server = 6;
    public static final int SERVER_FIELD_NUMBER = 6;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName server_;
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    public boolean hasServer() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer() {
      return server_;
    }
    /**
     * <code>optional .hbase.pb.ServerName server = 6;</code>
     *
     * <pre>
     * Server who opened the region
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder() {
      return server_;
    }

    // optional bytes region_name = 7;
    public static final int REGION_NAME_FIELD_NUMBER = 7;
    private com.google.protobuf.ByteString regionName_;
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public boolean hasRegionName() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional bytes region_name = 7;</code>
     *
     * <pre>
     * full region name
     * </pre>
     */
    public com.google.protobuf.ByteString getRegionName() {
      return regionName_;
    }

    private void initFields() {
      eventType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType.REGION_OPEN;
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      logSequenceNumber_ = 0L;
      stores_ = java.util.Collections.emptyList();
      server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      regionName_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasEventType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getStoresCount(); i++) {
        if (!getStores(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasServer()) {
        if (!getServer().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, eventType_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, logSequenceNumber_);
      }
      for (int i = 0; i < stores_.size(); i++) {
        output.writeMessage(5, stores_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(6, server_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(7, regionName_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, eventType_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, logSequenceNumber_);
      }
      for (int i = 0; i < stores_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, stores_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, server_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, regionName_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor) obj;

      boolean result = true;
      result = result && (hasEventType() == other.hasEventType());
      if (hasEventType()) {
        result = result &&
            (getEventType() == other.getEventType());
      }
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasLogSequenceNumber() == other.hasLogSequenceNumber());
      if (hasLogSequenceNumber()) {
        result = result && (getLogSequenceNumber()
            == other.getLogSequenceNumber());
      }
      result = result && getStoresList()
          .equals(other.getStoresList());
      result = result && (hasServer() == other.hasServer());
      if (hasServer()) {
        result = result && getServer()
            .equals(other.getServer());
      }
      result = result && (hasRegionName() == other.hasRegionName());
      if (hasRegionName()) {
        result = result && getRegionName()
            .equals(other.getRegionName());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasEventType()) {
        hash = (37 * hash) + EVENT_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getEventType());
      }
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasLogSequenceNumber()) {
        hash = (37 * hash) + LOG_SEQUENCE_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLogSequenceNumber());
      }
      if (getStoresCount() > 0) {
        hash = (37 * hash) + STORES_FIELD_NUMBER;
        hash = (53 * hash) + getStoresList().hashCode();
      }
      if (hasServer()) {
        hash = (37 * hash) + SERVER_FIELD_NUMBER;
        hash = (53 * hash) + getServer().hashCode();
      }
      if (hasRegionName()) {
        hash = (37 * hash) + REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getRegionName().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.RegionEventDescriptor}
     *
     * <pre>
     **
     * Special WAL entry to hold all related to a region event (open/close).
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_RegionEventDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_RegionEventDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStoresFieldBuilder();
          getServerFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        eventType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType.REGION_OPEN;
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        logSequenceNumber_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (storesBuilder_ == null) {
          stores_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          storesBuilder_.clear();
        }
        if (serverBuilder_ == null) {
          server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
        } else {
          serverBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        regionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_RegionEventDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.eventType_ = eventType_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.logSequenceNumber_ = logSequenceNumber_;
        if (storesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010)) {
            stores_ = java.util.Collections.unmodifiableList(stores_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.stores_ = stores_;
        } else {
          result.stores_ = storesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000010;
        }
        if (serverBuilder_ == null) {
          result.server_ = server_;
        } else {
          result.server_ = serverBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        result.regionName_ = regionName_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.getDefaultInstance()) return this;
        if (other.hasEventType()) {
          setEventType(other.getEventType());
        }
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasLogSequenceNumber()) {
          setLogSequenceNumber(other.getLogSequenceNumber());
        }
        if (storesBuilder_ == null) {
          if (!other.stores_.isEmpty()) {
            if (stores_.isEmpty()) {
              stores_ = other.stores_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureStoresIsMutable();
              stores_.addAll(other.stores_);
            }
            onChanged();
          }
        } else {
          if (!other.stores_.isEmpty()) {
            if (storesBuilder_.isEmpty()) {
              storesBuilder_.dispose();
              storesBuilder_ = null;
              stores_ = other.stores_;
              bitField0_ = (bitField0_ & ~0x00000010);
              storesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStoresFieldBuilder() : null;
            } else {
              storesBuilder_.addAllMessages(other.stores_);
            }
          }
        }
        if (other.hasServer()) {
          mergeServer(other.getServer());
        }
        if (other.hasRegionName()) {
          setRegionName(other.getRegionName());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasEventType()) {
          
          return false;
        }
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        for (int i = 0; i < getStoresCount(); i++) {
          if (!getStores(i).isInitialized()) {
            
            return false;
          }
        }
        if (hasServer()) {
          if (!getServer().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;
      private org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType eventType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType.REGION_OPEN;
      /**
       * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
       */
      public boolean hasEventType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType getEventType() {
        return eventType_;
      }
      /**
       * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
       */
      public Builder setEventType(org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        eventType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hbase.pb.RegionEventDescriptor.EventType event_type = 1;</code>
       */
      public Builder clearEventType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        eventType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescriptor.EventType.REGION_OPEN;
        onChanged();
        return this;
      }

      // required bytes table_name = 2;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required bytes encoded_region_name = 3;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 3;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // optional uint64 log_sequence_number = 4;
      private long logSequenceNumber_ ;
      /**
       * <code>optional uint64 log_sequence_number = 4;</code>
       */
      public boolean hasLogSequenceNumber() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 log_sequence_number = 4;</code>
       */
      public long getLogSequenceNumber() {
        return logSequenceNumber_;
      }
      /**
       * <code>optional uint64 log_sequence_number = 4;</code>
       */
      public Builder setLogSequenceNumber(long value) {
        bitField0_ |= 0x00000008;
        logSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 log_sequence_number = 4;</code>
       */
      public Builder clearLogSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000008);
        logSequenceNumber_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hbase.pb.StoreDescriptor stores = 5;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> stores_ =
        java.util.Collections.emptyList();
      private void ensureStoresIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          stores_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor>(stores_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> storesBuilder_;

      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> getStoresList() {
        if (storesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(stores_);
        } else {
          return storesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public int getStoresCount() {
        if (storesBuilder_ == null) {
          return stores_.size();
        } else {
          return storesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor getStores(int index) {
        if (storesBuilder_ == null) {
          return stores_.get(index);
        } else {
          return storesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder setStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.set(index, value);
          onChanged();
        } else {
          storesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder setStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.set(index, builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder addStores(org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.add(value);
          onChanged();
        } else {
          storesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder addStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor value) {
        if (storesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStoresIsMutable();
          stores_.add(index, value);
          onChanged();
        } else {
          storesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder addStores(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.add(builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder addStores(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder builderForValue) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.add(index, builderForValue.build());
          onChanged();
        } else {
          storesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder addAllStores(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor> values) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          super.addAll(values, stores_);
          onChanged();
        } else {
          storesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder clearStores() {
        if (storesBuilder_ == null) {
          stores_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          storesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public Builder removeStores(int index) {
        if (storesBuilder_ == null) {
          ensureStoresIsMutable();
          stores_.remove(index);
          onChanged();
        } else {
          storesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder getStoresBuilder(
          int index) {
        return getStoresFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder getStoresOrBuilder(
          int index) {
        if (storesBuilder_ == null) {
          return stores_.get(index);  } else {
          return storesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
           getStoresOrBuilderList() {
        if (storesBuilder_ != null) {
          return storesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(stores_);
        }
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder addStoresBuilder() {
        return getStoresFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder addStoresBuilder(
          int index) {
        return getStoresFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.getDefaultInstance());
      }
      /**
       * <code>repeated .hbase.pb.StoreDescriptor stores = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder> 
           getStoresBuilderList() {
        return getStoresFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder> 
          getStoresFieldBuilder() {
        if (storesBuilder_ == null) {
          storesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptor.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.StoreDescriptorOrBuilder>(
                  stores_,
                  ((bitField0_ & 0x00000010) == 0x00000010),
                  getParentForChildren(),
                  isClean());
          stores_ = null;
        }
        return storesBuilder_;
      }

      // optional .hbase.pb.ServerName server = 6;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> serverBuilder_;
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public boolean hasServer() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer() {
        if (serverBuilder_ == null) {
          return server_;
        } else {
          return serverBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public Builder setServer(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serverBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          server_ = value;
          onChanged();
        } else {
          serverBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public Builder setServer(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (serverBuilder_ == null) {
          server_ = builderForValue.build();
          onChanged();
        } else {
          serverBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public Builder mergeServer(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serverBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020) &&
              server_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance()) {
            server_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.newBuilder(server_).mergeFrom(value).buildPartial();
          } else {
            server_ = value;
          }
          onChanged();
        } else {
          serverBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public Builder clearServer() {
        if (serverBuilder_ == null) {
          server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
          onChanged();
        } else {
          serverBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getServerBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getServerFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder() {
        if (serverBuilder_ != null) {
          return serverBuilder_.getMessageOrBuilder();
        } else {
          return server_;
        }
      }
      /**
       * <code>optional .hbase.pb.ServerName server = 6;</code>
       *
       * <pre>
       * Server who opened the region
       * </pre>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getServerFieldBuilder() {
        if (serverBuilder_ == null) {
          serverBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  server_,
                  getParentForChildren(),
                  isClean());
          server_ = null;
        }
        return serverBuilder_;
      }

      // optional bytes region_name = 7;
      private com.google.protobuf.ByteString regionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public boolean hasRegionName() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public com.google.protobuf.ByteString getRegionName() {
        return regionName_;
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder setRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        regionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes region_name = 7;</code>
       *
       * <pre>
       * full region name
       * </pre>
       */
      public Builder clearRegionName() {
        bitField0_ = (bitField0_ & ~0x00000040);
        regionName_ = getDefaultInstance().getRegionName();
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.RegionEventDescriptor)
    }

    static {
      defaultInstance = new RegionEventDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.RegionEventDescriptor)
  }

  public interface WALTrailerOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hbase.pb.WALTrailer}
   *
   * <pre>
   **
   * A trailer that is appended to the end of a properly closed WAL file.
   * If missing, this is either a legacy or a corrupted WAL file.
   * N.B. This trailer currently doesn't contain any information and we
   * purposefully don't expose it in the WAL APIs. It's for future growth.
   * </pre>
   */
  public static final class WALTrailer extends
      com.google.protobuf.GeneratedMessage
      implements WALTrailerOrBuilder {
    // Use WALTrailer.newBuilder() to construct.
    private WALTrailer(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALTrailer(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALTrailer defaultInstance;
    public static WALTrailer getDefaultInstance() {
      return defaultInstance;
    }

    public WALTrailer getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALTrailer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALTrailer_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALTrailer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.Builder.class);
    }

    public static com.google.protobuf.Parser<WALTrailer> PARSER =
        new com.google.protobuf.AbstractParser<WALTrailer>() {
      public WALTrailer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALTrailer(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALTrailer> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hbase.pb.WALTrailer}
     *
     * <pre>
     **
     * A trailer that is appended to the end of a properly closed WAL file.
     * If missing, this is either a legacy or a corrupted WAL file.
     * N.B. This trailer currently doesn't contain any information and we
     * purposefully don't expose it in the WAL APIs. It's for future growth.
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALTrailer_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALTrailer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_hbase_pb_WALTrailer_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hbase.pb.WALTrailer)
    }

    static {
      defaultInstance = new WALTrailer(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hbase.pb.WALTrailer)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_WALHeader_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_WALHeader_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_WALKey_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_WALKey_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_Attribute_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_Attribute_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_FamilyScope_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_FamilyScope_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_CompactionDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_CompactionDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_FlushDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_FlushDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_StoreDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_StoreDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_BulkLoadDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_BulkLoadDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_RegionEventDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_RegionEventDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hbase_pb_WALTrailer_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hbase_pb_WALTrailer_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\tWAL.proto\022\010hbase.pb\032\013Table.proto\032\013HBas" +
      "e.proto\032\014Client.proto\"\217\001\n\tWALHeader\022\027\n\017h" +
      "as_compression\030\001 \001(\010\022\026\n\016encryption_key\030\002" +
      " \001(\014\022\033\n\023has_tag_compression\030\003 \001(\010\022\027\n\017wri" +
      "ter_cls_name\030\004 \001(\t\022\033\n\023cell_codec_cls_nam" +
      "e\030\005 \001(\t\"\355\002\n\006WALKey\022\033\n\023encoded_region_nam" +
      "e\030\001 \002(\014\022\022\n\ntable_name\030\002 \002(\014\022\033\n\023log_seque" +
      "nce_number\030\003 \002(\004\022\022\n\nwrite_time\030\004 \002(\004\022&\n\n" +
      "cluster_id\030\005 \001(\0132\016.hbase.pb.UUIDB\002\030\001\022%\n\006" +
      "scopes\030\006 \003(\0132\025.hbase.pb.FamilyScope\022\032\n\022f",
      "ollowing_kv_count\030\007 \001(\r\022#\n\013cluster_ids\030\010" +
      " \003(\0132\016.hbase.pb.UUID\022\022\n\nnonceGroup\030\t \001(\004" +
      "\022\r\n\005nonce\030\n \001(\004\022\034\n\024orig_sequence_number\030" +
      "\013 \001(\004\0220\n\023extended_attributes\030\014 \003(\0132\023.hba" +
      "se.pb.Attribute\"\'\n\tAttribute\022\013\n\003key\030\001 \002(" +
      "\t\022\r\n\005value\030\002 \002(\014\"F\n\013FamilyScope\022\016\n\006famil" +
      "y\030\001 \002(\014\022\'\n\nscope_type\030\002 \002(\0162\023.hbase.pb.S" +
      "copeType\"\276\001\n\024CompactionDescriptor\022\022\n\ntab" +
      "le_name\030\001 \002(\014\022\033\n\023encoded_region_name\030\002 \002" +
      "(\014\022\023\n\013family_name\030\003 \002(\014\022\030\n\020compaction_in",
      "put\030\004 \003(\t\022\031\n\021compaction_output\030\005 \003(\t\022\026\n\016" +
      "store_home_dir\030\006 \002(\t\022\023\n\013region_name\030\007 \001(" +
      "\014\"\244\003\n\017FlushDescriptor\0225\n\006action\030\001 \002(\0162%." +
      "hbase.pb.FlushDescriptor.FlushAction\022\022\n\n" +
      "table_name\030\002 \002(\014\022\033\n\023encoded_region_name\030" +
      "\003 \002(\014\022\035\n\025flush_sequence_number\030\004 \001(\004\022E\n\r" +
      "store_flushes\030\005 \003(\0132..hbase.pb.FlushDesc" +
      "riptor.StoreFlushDescriptor\022\023\n\013region_na" +
      "me\030\006 \001(\014\032Y\n\024StoreFlushDescriptor\022\023\n\013fami" +
      "ly_name\030\001 \002(\014\022\026\n\016store_home_dir\030\002 \002(\t\022\024\n",
      "\014flush_output\030\003 \003(\t\"S\n\013FlushAction\022\017\n\013ST" +
      "ART_FLUSH\020\000\022\020\n\014COMMIT_FLUSH\020\001\022\017\n\013ABORT_F" +
      "LUSH\020\002\022\020\n\014CANNOT_FLUSH\020\003\"q\n\017StoreDescrip" +
      "tor\022\023\n\013family_name\030\001 \002(\014\022\026\n\016store_home_d" +
      "ir\030\002 \002(\t\022\022\n\nstore_file\030\003 \003(\t\022\035\n\025store_fi" +
      "le_size_bytes\030\004 \001(\004\"\264\001\n\022BulkLoadDescript" +
      "or\022\'\n\ntable_name\030\001 \002(\0132\023.hbase.pb.TableN" +
      "ame\022\033\n\023encoded_region_name\030\002 \002(\014\022)\n\006stor" +
      "es\030\003 \003(\0132\031.hbase.pb.StoreDescriptor\022\030\n\020b" +
      "ulkload_seq_num\030\004 \002(\003\022\023\n\013cluster_ids\030\005 \003",
      "(\t\"\272\002\n\025RegionEventDescriptor\022=\n\nevent_ty" +
      "pe\030\001 \002(\0162).hbase.pb.RegionEventDescripto" +
      "r.EventType\022\022\n\ntable_name\030\002 \002(\014\022\033\n\023encod" +
      "ed_region_name\030\003 \002(\014\022\033\n\023log_sequence_num" +
      "ber\030\004 \001(\004\022)\n\006stores\030\005 \003(\0132\031.hbase.pb.Sto" +
      "reDescriptor\022$\n\006server\030\006 \001(\0132\024.hbase.pb." +
      "ServerName\022\023\n\013region_name\030\007 \001(\014\".\n\tEvent" +
      "Type\022\017\n\013REGION_OPEN\020\000\022\020\n\014REGION_CLOSE\020\001\"" +
      "\014\n\nWALTrailer*F\n\tScopeType\022\033\n\027REPLICATIO" +
      "N_SCOPE_LOCAL\020\000\022\034\n\030REPLICATION_SCOPE_GLO",
      "BAL\020\001B?\n*org.apache.hadoop.hbase.protobu" +
      "f.generatedB\tWALProtosH\001\210\001\000\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hbase_pb_WALHeader_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hbase_pb_WALHeader_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_WALHeader_descriptor,
              new java.lang.String[] { "HasCompression", "EncryptionKey", "HasTagCompression", "WriterClsName", "CellCodecClsName", });
          internal_static_hbase_pb_WALKey_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hbase_pb_WALKey_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_WALKey_descriptor,
              new java.lang.String[] { "EncodedRegionName", "TableName", "LogSequenceNumber", "WriteTime", "ClusterId", "Scopes", "FollowingKvCount", "ClusterIds", "NonceGroup", "Nonce", "OrigSequenceNumber", "ExtendedAttributes", });
          internal_static_hbase_pb_Attribute_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hbase_pb_Attribute_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_Attribute_descriptor,
              new java.lang.String[] { "Key", "Value", });
          internal_static_hbase_pb_FamilyScope_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hbase_pb_FamilyScope_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_FamilyScope_descriptor,
              new java.lang.String[] { "Family", "ScopeType", });
          internal_static_hbase_pb_CompactionDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hbase_pb_CompactionDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_CompactionDescriptor_descriptor,
              new java.lang.String[] { "TableName", "EncodedRegionName", "FamilyName", "CompactionInput", "CompactionOutput", "StoreHomeDir", "RegionName", });
          internal_static_hbase_pb_FlushDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hbase_pb_FlushDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_FlushDescriptor_descriptor,
              new java.lang.String[] { "Action", "TableName", "EncodedRegionName", "FlushSequenceNumber", "StoreFlushes", "RegionName", });
          internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor =
            internal_static_hbase_pb_FlushDescriptor_descriptor.getNestedTypes().get(0);
          internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_FlushDescriptor_StoreFlushDescriptor_descriptor,
              new java.lang.String[] { "FamilyName", "StoreHomeDir", "FlushOutput", });
          internal_static_hbase_pb_StoreDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hbase_pb_StoreDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_StoreDescriptor_descriptor,
              new java.lang.String[] { "FamilyName", "StoreHomeDir", "StoreFile", "StoreFileSizeBytes", });
          internal_static_hbase_pb_BulkLoadDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hbase_pb_BulkLoadDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_BulkLoadDescriptor_descriptor,
              new java.lang.String[] { "TableName", "EncodedRegionName", "Stores", "BulkloadSeqNum", "ClusterIds", });
          internal_static_hbase_pb_RegionEventDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hbase_pb_RegionEventDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_RegionEventDescriptor_descriptor,
              new java.lang.String[] { "EventType", "TableName", "EncodedRegionName", "LogSequenceNumber", "Stores", "Server", "RegionName", });
          internal_static_hbase_pb_WALTrailer_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hbase_pb_WALTrailer_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hbase_pb_WALTrailer_descriptor,
              new java.lang.String[] { });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hbase.protobuf.generated.TableProtos.getDescriptor(),
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.getDescriptor(),
          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
